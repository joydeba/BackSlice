"# -*- coding: utf-8 -*-
#
# documentation build configuration file, created by
# sphinx-quickstart on Sat Sep 27 13:23:22 2008-2009.
#
# This file is execfile()d with the current directory set to its
# containing dir.
#
# The contents of this file are pickled, so don't put values in the namespace
# that aren't pickleable (module imports are okay, they're removed
# automatically).
#
# All configuration values have a default value; values that are commented out
# serve to show the default value.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type
import sys
import os
# pip install sphinx_rtd_theme
# import sphinx_rtd_theme
# html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]
# If your extensions are in another directory, add it here. If the directory
# is relative to the documentation root, use os.path.abspath to make it
# absolute, like shown here.
# sys.path.append(os.path.abspath('some/directory'))
#
sys.path.insert(0, os.path.join('ansible', 'lib'))
sys.path.append(os.path.abspath(os.path.join('..', '_extensions')))
# We want sphinx to document the ansible modules contained in this repository,
# not those that may happen to be installed in the version
# of Python used to run sphinx.  When sphinx loads in order to document,
# the repository version needs to be the one that is loaded:
sys.path.insert(0, os.path.abspath(os.path.join('..', '..', '..', 'lib')))
VERSION = 'devel'
AUTHOR = 'Ansible, Inc'
# General configuration
# ---------------------
# Add any Sphinx extension module names here, as strings.
# They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
# TEST: 'sphinxcontrib.fulltoc'
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.intersphinx', 'pygments_lexer', 'notfound.extension']
# Later on, add 'sphinx.ext.viewcode' to the list if you want to have
# colorized code generated too for references.
# Add any paths that contain templates here, relative to this directory.
templates_path = ['.templates']
# The suffix of source filenames.
source_suffix = '.rst'
# The master toctree document.
master_doc = 'index'
# General substitutions.
project = 'Ansible'
copyright = ""2021 Red Hat, Inc.""
# The default replacements for |version| and |release|, also used in various
# other places throughout the built documents.
#
# The short X.Y version.
version = VERSION
# The full version, including alpha/beta/rc tags.
release = VERSION
# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
# today = ''
# Else, today_fmt is used as the format for a strftime call.
today_fmt = '%B %d, %Y'
# List of documents that shouldn't be included in the build.
# unused_docs = []
# List of directories, relative to source directories, that shouldn't be
# searched for source files.
# exclude_dirs = []
# A list of glob-style patterns that should be excluded when looking
# for source files.
exclude_patterns = [
    '2.10_index.rst',
    'ansible_index.rst',
    'core_index.rst',
    'porting_guides/core_porting_guides.rst',
    'porting_guides/porting_guide_base_2.10.rst',
    'porting_guides/porting_guide_core_2.11.rst',
    'roadmap/index.rst',
    'roadmap/ansible_base_roadmap_index.rst',
    'roadmap/ROADMAP_2_10.rst',
    'roadmap/ROADMAP_2_11.rst'
]
# The reST default role (used for this markup: `text`) to use for all
# documents.
# default_role = None
# If true, '()' will be appended to :func: etc. cross-reference text.
# add_function_parentheses = True
# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
# add_module_names = True
# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
# show_authors = False
# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'
highlight_language = 'YAMLJinja'
# Substitutions, variables, entities, & shortcuts for text which do not need to link to anything.
# For titles which should be a link, use the intersphinx anchors set at the index, chapter, and section levels, such as  qi_start_:
# |br| is useful for formatting fields inside of tables
# |_| is a nonbreaking space; similarly useful inside of tables
rst_epilog = """"""
.. |br| raw:: html
   <br>
.. |_| unicode:: 0xA0
    :trim:
""""""
# Options for HTML output
# -----------------------
html_theme_path = ['../_themes']
html_theme = 'sphinx_rtd_theme'
html_short_title = 'Ansible Documentation'
html_show_sphinx = False
html_theme_options = {
    'canonical_url': ""https://docs.ansible.com/ansible/latest/"",
    'vcs_pageview_mode': 'edit'
}
html_context = {
    'display_github': 'True',
    'github_user': 'ansible',
    'github_repo': 'ansible',
    'github_version': 'devel/docs/docsite/rst/',
    'github_module_version': 'devel/lib/ansible/modules/',
    'github_root_dir': 'devel/lib/ansible',
    'github_cli_version': 'devel/lib/ansible/cli/',
    'current_version': version,
    'latest_version': '2.10',
    # list specifically out of order to make latest work
    'available_versions': ('latest', '2.9', '2.9_ja', '2.8', 'devel'),
    'css_files': ('_static/ansible.css',  # overrides to the standard theme
                  ),
}
# The style sheet to use for HTML and HTML Help pages. A file of that name
# must exist either in Sphinx' static/ path, or in one of the custom paths
# given in html_static_path.
# html_style = 'solar.css'
# The name for this set of Sphinx documents.  If None, it defaults to
# ""<project> v<release> documentation"".
html_title = 'Ansible Documentation'
# A shorter title for the navigation bar.  Default is the same as html_title.
# html_short_title = None
# The name of an image file (within the static path) to place at the top of
# the sidebar.
# html_logo =
# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
# html_favicon = 'favicon.ico'
# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named ""default.css"" will overwrite the builtin ""default.css"".
html_static_path = ['../_static']
# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
html_last_updated_fmt = '%b %d, %Y'
# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
# html_use_smartypants = True
# Custom sidebar templates, maps document names to template names.
# html_sidebars = {}
# Additional templates that should be rendered to pages, maps page names to
# template names.
# html_additional_pages = {}
# If false, no module index is generated.
# html_use_modindex = True
# If false, no index is generated.
# html_use_index = True
# If true, the index is split into individual pages for each letter.
# html_split_index = False
# If true, the reST sources are included in the HTML build as _sources/<name>.
html_copy_source = False
# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
# html_use_opensearch = 'https://docs.ansible.com/ansible/latest'
# If nonempty, this is the file name suffix for HTML files (e.g. "".xhtml"").
# html_file_suffix = ''
# Output file base name for HTML help builder.
htmlhelp_basename = 'Poseidodoc'
# Configuration for sphinx-notfound-pages
# with no 'notfound_template' and no 'notfound_context' set,
# the extension builds 404.rst into a location-agnostic 404 page
#
# default is `en` - using this for the sub-site:
notfound_default_language = ""ansible""
# default is `latest`:
# setting explicitly - docsite serves up /ansible/latest/404.html
# so keep this set to `latest` even on the `devel` branch
# then no maintenance is needed when we branch a new stable_x.x
notfound_default_version = ""latest""
# makes default setting explicit:
notfound_no_urls_prefix = False
# Options for LaTeX output
# ------------------------
# The paper size ('letter' or 'a4').
# latex_paper_size = 'letter'
# The font size ('10pt', '11pt' or '12pt').
# latex_font_size = '10pt'
# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, document class
# [howto/manual]).
latex_documents = [
    ('index', 'ansible.tex', 'Ansible 2.2 Documentation', AUTHOR, 'manual'),
]
# The name of an image file (relative to this directory) to place at the top of
# the title page.
# latex_logo = None
# For ""manual"" documents, if this is true, then toplevel headings are parts,
# not chapters.
# latex_use_parts = False
# Additional stuff for the LaTeX preamble.
# latex_preamble = ''
# Documents to append as an appendix to all manuals.
# latex_appendices = []
# If false, no module index is generated.
# latex_use_modindex = True
autoclass_content = 'both'
# Note:  Our strategy for intersphinx mappings is to have the upstream build location as the
# canonical source and then cached copies of the mapping stored locally in case someone is building
# when disconnected from the internet.  We then have a script to update the cached copies.
#
# Because of that, each entry in this mapping should have this format:
#   name: ('http://UPSTREAM_URL', (None, 'path/to/local/cache.inv'))
#
# The update script depends on this format so deviating from this (for instance, adding a third
# location for the mappning to live) will confuse it.
intersphinx_mapping = {'python': ('https://docs.python.org/2/', (None, '../python2.inv')),
                       'python3': ('https://docs.python.org/3/', (None, '../python3.inv')),
                       'jinja2': ('http://jinja.palletsprojects.com/', (None, '../jinja2.inv')),
                       'ansible_2_10': ('https://docs.ansible.com/ansible/2.10/', (None, '../ansible_2_10.inv')),
                       'ansible_2_9': ('https://docs.ansible.com/ansible/2.9/', (None, '../ansible_2_9.inv')),
                       'ansible_2_8': ('https://docs.ansible.com/ansible/2.8/', (None, '../ansible_2_8.inv')),
                       'ansible_2_7': ('https://docs.ansible.com/ansible/2.7/', (None, '../ansible_2_7.inv')),
                       'ansible_2_6': ('https://docs.ansible.com/ansible/2.6/', (None, '../ansible_2_6.inv')),
                       'ansible_2_5': ('https://docs.ansible.com/ansible/2.5/', (None, '../ansible_2_5.inv')),
                       }
# linckchecker settings
linkcheck_ignore = [
    r'http://irc\.freenode\.net',
]
linkcheck_workers = 25
# linkcheck_anchors = False
"
-------------------------------------------------------------------------
"# -*- coding: utf-8 -*-
#
# documentation build configuration file, created by
# sphinx-quickstart on Sat Sep 27 13:23:22 2008-2009.
#
# This file is execfile()d with the current directory set to its
# containing dir.
#
# The contents of this file are pickled, so don't put values in the namespace
# that aren't pickleable (module imports are okay, they're removed
# automatically).
#
# All configuration values have a default value; values that are commented out
# serve to show the default value.

from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

import sys
import os

# Required dependency
import sys
import os
import __future__

# Original metadata
# Backport of Split Ansible docs from core docs (#73616)
# auto-merge:	disabled
# excludes scenario guides from core docs, splits porting guides and roadmaps, symlinks indices to create index.html pages, and adds .gitignore entries for conf.py and the toplevel index.rst files generated by the docs build
#
# creates separate build commands for the Ansible package docs and the ansible-core docs, for accuracy in building the package documentation from the correct underlying version of the core code
#
# (cherry picked from commit ccbfdec334a7b0443f2daecf9f96ac5c5a33fd45)
#
# SUMMARY
# Allows the docs team to build package docs for Ansible 3.0.0 based on the stable-2.10 branch. The 3.0.0 documentation will reflect core features and functionality of ansible-base-2.10.
#
# Backport of #73616.
#
# ISSUE TYPE
# - Docs Pull Request
#
# COMPONENT NAME
# docs.ansible.com/ansible
# docs.ansible.com/ansible-core
#
# affects_2.10, backport, core_review, docs, has_issue, support:core, test

# -*- coding: utf-8 -*-

# documentation build configuration file, created by
# sphinx-quickstart on Sat Sep 27 13:23:22 2008-2009.

# This file is execfile()d with the current directory set to its
# containing dir.

# The contents of this file are pickled, so don't put values in the namespace
# that aren't pickleable (module imports are okay, they're removed
# automatically).

# All configuration values have a default value; values that are commented out
# serve to show the default value.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

# Required dependency
import sys
import os
import __future__

# Original metadata
# Backport of Split Ansible docs from core docs (#73616)
# auto-merge:	disabled
# excludes scenario guides from core docs, splits porting guides and roadmaps, symlinks indices to create index.html pages, and adds .gitignore entries for conf.py and the toplevel index.rst files generated by the docs build
#
# creates separate build commands for the Ansible package docs and the ansible-core docs, for accuracy in building the package documentation from the correct underlying version of the core code
#
# (cherry picked from commit ccbfdec334a7b0443f2daecf9f96ac5c5a33fd45)
#
# SUMMARY
# Allows the docs team to build package docs for Ansible 3.0.0 based on the stable-2.10 branch. The 3.0.0 documentation will reflect core features and functionality of ansible-base-2.10.
#
# Backport of #73616.
#
# ISSUE TYPE
# - Docs Pull Request
#
# COMPONENT NAME
# docs.ansible.com/ansible
# docs.ansible.com/ansible-core
#
# - Coding: utf-8

# -*- coding: utf-8 -*-

# Documentation build configuration file, created by
# sphinx-quickstart on Sat Sep 27 13:23:22 2008-2009.

# The file is execfile()d with the current directory set to its
# containing dir.

# The contents of this file are pickled, so don't put values in the namespace
# that aren't pickleable (module imports are okay, they're removed
# automatically).

# All configuration values have a default value; values that are commented out
# serve to show the default value.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

# Required dependency
import sys
import os
from __future__ import absolute_import

# Original metadata
# Backport of Split Ansible docs from core docs (#73616)
# auto-merge:	disabled
# excludes scenario guides from core docs, splits porting guides and roadmaps, symlinks indices to create index.html pages, and adds .gitignore entries for conf.py and the toplevel index.rst files generated by the docs build
#
# creates separate build commands for the Ansible package docs and the ansible-core docs, for accuracy in building the package documentation from the correct underlying version of the core code
#
# (cherry picked from commit ccbfdec334a7b0443f2daecf9f96ac5c5a33fd45)
#
# SUMMARY
# Allows the docs team to build package docs for Ansible 3.0.0 based on the stable-2.10 branch. The 3.0.0 documentation will reflect core features and functionality of ansible-base-2.10.
#
# Backport of #73616.
#
# ISSUE TYPE
# - Docs Pull Request
#
# COMPONENT NAME
# docs.ansible.com/ansible
# docs.ansible.com/ansible-core
#
# -*- coding: utf-8 -*-"
-------------------------------------------------------------------------
"# -*- coding: utf-8 -*-
#
# documentation build configuration file, created by
# sphinx-quickstart on Sat Sep 27 13:23:22 2008-2009.
#
# This file is execfile()d with the current directory set to its
# containing dir.
#
# The contents of this file are pickled, so don't put values in the namespace
# that aren't pickleable (module imports are okay, they're removed
# automatically).
#
# All configuration values have a default value; values that are commented out
# serve to show the default value.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type
import sys
import os
# pip install sphinx_rtd_theme
# import sphinx_rtd_theme
# html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]
# If your extensions are in another directory, add it here. If the directory
# is relative to the documentation root, use os.path.abspath to make it
# absolute, like shown here.
# sys.path.append(os.path.abspath('some/directory'))
#
sys.path.insert(0, os.path.join('ansible', 'lib'))
sys.path.append(os.path.abspath(os.path.join('..', '_extensions')))
# We want sphinx to document the ansible modules contained in this repository,
# not those that may happen to be installed in the version
# of Python used to run sphinx.  When sphinx loads in order to document,
# the repository version needs to be the one that is loaded:
sys.path.insert(0, os.path.abspath(os.path.join('..', '..', '..', 'lib')))
VERSION = '3'
AUTHOR = 'Ansible, Inc'
# General configuration
# ---------------------
# Add any Sphinx extension module names here, as strings.
# They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
# TEST: 'sphinxcontrib.fulltoc'
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.intersphinx', 'pygments_lexer', 'notfound.extension']
# Later on, add 'sphinx.ext.viewcode' to the list if you want to have
# colorized code generated too for references.
# Add any paths that contain templates here, relative to this directory.
templates_path = ['.templates']
# The suffix of source filenames.
source_suffix = '.rst'
# The master toctree document.
master_doc = 'index'
# General substitutions.
project = 'Ansible'
copyright = ""2021 Red Hat, Inc.""
# The default replacements for |version| and |release|, also used in various
# other places throughout the built documents.
#
# The short X.Y version.
version = VERSION
# The full version, including alpha/beta/rc tags.
release = VERSION
# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
# today = ''
# Else, today_fmt is used as the format for a strftime call.
today_fmt = '%B %d, %Y'
# List of documents that shouldn't be included in the build.
# unused_docs = []
# List of directories, relative to source directories, that shouldn't be
# searched for source files.
# exclude_dirs = []
# A list of glob-style patterns that should be excluded when looking
# for source files.
exclude_patterns = [
    '2.10_index.rst',
    'ansible_index.rst',
    'core_index.rst',
    'porting_guides/core_porting_guides.rst',
    'porting_guides/porting_guide_base_2.10.rst',
    'porting_guides/porting_guide_core_2.11.rst',
    'roadmap/index.rst',
    'roadmap/ansible_base_roadmap_index.rst',
    'roadmap/ROADMAP_2_10.rst',
    'roadmap/ROADMAP_2_11.rst'
]
# The reST default role (used for this markup: `text`) to use for all
# documents.
# default_role = None
# If true, '()' will be appended to :func: etc. cross-reference text.
# add_function_parentheses = True
# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
# add_module_names = True
# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
# show_authors = False
# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'
highlight_language = 'YAMLJinja'
# Substitutions, variables, entities, & shortcuts for text which do not need to link to anything.
# For titles which should be a link, use the intersphinx anchors set at the index, chapter, and section levels, such as  qi_start_:
# |br| is useful for formatting fields inside of tables
# |_| is a nonbreaking space; similarly useful inside of tables
rst_epilog = """"""
.. |br| raw:: html
   <br>
.. |_| unicode:: 0xA0
    :trim:
""""""
# Options for HTML output
# -----------------------
html_theme_path = ['../_themes']
html_theme = 'sphinx_rtd_theme'
html_short_title = 'Ansible Documentation'
html_show_sphinx = False
html_theme_options = {
    'canonical_url': ""https://docs.ansible.com/ansible/latest/"",
    'vcs_pageview_mode': 'edit'
}
html_context = {
    'display_github': 'True',
    'github_user': 'ansible',
    'github_repo': 'ansible',
    'github_version': 'devel/docs/docsite/rst/',
    'github_module_version': 'devel/lib/ansible/modules/',
    'github_root_dir': 'devel/lib/ansible',
    'github_cli_version': 'devel/lib/ansible/cli/',
    'current_version': version,
    'latest_version': '3',
    # list specifically out of order to make latest work
    'available_versions': ('latest', '2.10', '2.9', '2.9_ja', '2.8', 'devel'),
    'css_files': ('_static/ansible.css',  # overrides to the standard theme
                  ),
}
# The style sheet to use for HTML and HTML Help pages. A file of that name
# must exist either in Sphinx' static/ path, or in one of the custom paths
# given in html_static_path.
# html_style = 'solar.css'
# The name for this set of Sphinx documents.  If None, it defaults to
# ""<project> v<release> documentation"".
html_title = 'Ansible Documentation'
# A shorter title for the navigation bar.  Default is the same as html_title.
# html_short_title = None
# The name of an image file (within the static path) to place at the top of
# the sidebar.
# html_logo =
# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
# html_favicon = 'favicon.ico'
# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named ""default.css"" will overwrite the builtin ""default.css"".
html_static_path = ['../_static']
# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
html_last_updated_fmt = '%b %d, %Y'
# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
# html_use_smartypants = True
# Custom sidebar templates, maps document names to template names.
# html_sidebars = {}
# Additional templates that should be rendered to pages, maps page names to
# template names.
# html_additional_pages = {}
# If false, no module index is generated.
# html_use_modindex = True
# If false, no index is generated.
# html_use_index = True
# If true, the index is split into individual pages for each letter.
# html_split_index = False
# If true, the reST sources are included in the HTML build as _sources/<name>.
html_copy_source = False
# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
# html_use_opensearch = 'https://docs.ansible.com/ansible/latest'
# If nonempty, this is the file name suffix for HTML files (e.g. "".xhtml"").
# html_file_suffix = ''
# Output file base name for HTML help builder.
htmlhelp_basename = 'Poseidodoc'
# Configuration for sphinx-notfound-pages
# with no 'notfound_template' and no 'notfound_context' set,
# the extension builds 404.rst into a location-agnostic 404 page
#
# default is `en` - using this for the sub-site:
notfound_default_language = ""ansible""
# default is `latest`:
# setting explicitly - docsite serves up /ansible/latest/404.html
# so keep this set to `latest` even on the `devel` branch
# then no maintenance is needed when we branch a new stable_x.x
notfound_default_version = ""latest""
# makes default setting explicit:
notfound_no_urls_prefix = False
# Options for LaTeX output
# ------------------------
# The paper size ('letter' or 'a4').
# latex_paper_size = 'letter'
# The font size ('10pt', '11pt' or '12pt').
# latex_font_size = '10pt'
# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, document class
# [howto/manual]).
latex_documents = [
    ('index', 'ansible.tex', 'Ansible 2.2 Documentation', AUTHOR, 'manual'),
]
# The name of an image file (relative to this directory) to place at the top of
# the title page.
# latex_logo = None
# For ""manual"" documents, if this is true, then toplevel headings are parts,
# not chapters.
# latex_use_parts = False
# Additional stuff for the LaTeX preamble.
# latex_preamble = ''
# Documents to append as an appendix to all manuals.
# latex_appendices = []
# If false, no module index is generated.
# latex_use_modindex = True
autoclass_content = 'both'
# Note:  Our strategy for intersphinx mappings is to have the upstream build location as the
# canonical source and then cached copies of the mapping stored locally in case someone is building
# when disconnected from the internet.  We then have a script to update the cached copies.
#
# Because of that, each entry in this mapping should have this format:
#   name: ('http://UPSTREAM_URL', (None, 'path/to/local/cache.inv'))
#
# The update script depends on this format so deviating from this (for instance, adding a third
# location for the mappning to live) will confuse it.
intersphinx_mapping = {'python': ('https://docs.python.org/2/', (None, '../python2.inv')),
                       'python3': ('https://docs.python.org/3/', (None, '../python3.inv')),
                       'jinja2': ('http://jinja.palletsprojects.com/', (None, '../jinja2.inv')),
                       'ansible_2_10': ('https://docs.ansible.com/ansible/2.10/', (None, '../ansible_2_10.inv')),
                       'ansible_2_9': ('https://docs.ansible.com/ansible/2.9/', (None, '../ansible_2_9.inv')),
                       'ansible_2_8': ('https://docs.ansible.com/ansible/2.8/', (None, '../ansible_2_8.inv')),
                       'ansible_2_7': ('https://docs.ansible.com/ansible/2.7/', (None, '../ansible_2_7.inv')),
                       'ansible_2_6': ('https://docs.ansible.com/ansible/2.6/', (None, '../ansible_2_6.inv')),
                       'ansible_2_5': ('https://docs.ansible.com/ansible/2.5/', (None, '../ansible_2_5.inv')),
                       }
# linckchecker settings
linkcheck_ignore = [
    r'http://irc\.freenode\.net',
]
linkcheck_workers = 25
# linkcheck_anchors = False
"
-------------------------------------------------------------------------
"Recom
PRs: 73616, 73637"
-------------------------------------------------------------------------
=========================================================================
"'EulerOS', 'openEuler', 'AlmaLinux'],
"
-------------------------------------------------------------------------
"class Distribution(object):
    """"""
    This subclass of Facts fills the distribution, distribution_version and distribution_release variables

    To do so it checks the existence and content of typical files in /etc containing distribution information

    This is unit tested. Please extend the tests to cover all distributions if you have them available.
    """"""

    # every distribution name mentioned here, must have one of
    #  - allowempty == True
    #  - be listed in SEARCH_STRING
    #  - have a function get_distribution_DISTNAME implemented
    OSDIST_LIST = (
        {'path': '/etc/oracle-release', 'name': 'OracleLinux'},
        {'path': '/etc/slackware-version', 'name': 'Slackware'},
        {'path': '/etc/redhat-release', 'name': 'RedHat'},
        {'path': '/etc/vmware-release', 'name': 'VMwareESX', 'allowempty': True},
        {'path': '/etc/openwrt_release', 'name': 'OpenWrt'},
        {'path': '/etc/system-release', 'name': 'Amazon'},
        {'path': '/etc/alpine-release', 'name': 'Alpine'},
        {'path': '/etc/arch-release', 'name': 'Archlinux', 'allowempty': True},
        {'path': '/etc/os-release', 'name': 'SUSE'},
        {'path': '/etc/SuSE-release', 'name': 'SUSE'},
        {'path': '/etc/gentoo-release', 'name': 'Gentoo'},
        {'path': '/etc/os-release', 'name': 'Debian'},
        {'path': '/etc/lsb-release', 'name': 'Mandriva'},
        {'path': '/etc/altlinux-release', 'name': 'Altlinux'},
        {'path': '/etc/sourcemage-release', 'name': 'SMGL'},
        {'path': '/usr/lib/os-release', 'name': 'ClearLinux'},
        {'path': '/etc/coreos/update.conf', 'name': 'Coreos'},
        {'path': '/etc/os-release', 'name': 'NA'},
        {'path': '/etc/euler-release', 'name': 'EulerOS'},
        {'path': '/etc/openuler-release', 'name': 'openEuler'},
        {'path': '/etc/almalinux-release', 'name': 'AlmaLinux'},
    )

    SEARCH_STRING = {
        'OracleLinux': 'Oracle Linux',
        'RedHat': 'Red Hat',
        'Altlinux': 'ALT Linux',
        'ClearLinux': 'Clear Linux Software for Intel Architecture',
        'SMGL': 'Source Mage GNU/Linux',
        'EulerOS': 'EulerOS',
        'openEuler': 'openEuler',
        'AlmaLinux': 'AlmaLinux'
    }

    # keep keys in sync with Conditionals page of docs
    OS_FAMILY_MAP = {'RedHat': ['RedHat', 'Fedora', 'CentOS', 'Scientific', 'SLC',
                                'Ascendos', 'CloudLinux', 'PSBM', 'OracleLinux', 'OVS',
                                'OEL', 'Amazon', 'Virtuozzo', 'XenServer', 'Alibaba'],
                     'Debian': ['Debian', 'Ubuntu', 'Raspbian', 'Neon', 'KDE neon',
                                'Linux Mint', 'SteamOS', 'Devuan', 'Kali', 'Cumulus Linux'],
                     'Suse': ['SuSE', 'SLES', 'SLED', 'openSUSE', 'openSUSE Tumbleweed',
                              'SLES_SAP', 'SUSE_LINUX', 'openSUSE Leap'],
                     'Archlinux': ['Archlinux', 'Antergos', 'Manjaro'],
                     'Mandrake': ['Mandrake', 'Mandriva'],
                     'Solaris': ['Solaris', 'Nexenta', 'OmniOS', 'OpenIndiana', 'SmartOS'],
                     'Slackware': ['Slackware'],
                     'Altlinux': ['Altlinux'],
                     'SGML': ['SGML'],
                     'Gentoo': ['Gentoo', 'Funtoo'],
                     'Alpine': ['Alpine'],
                     'AIX': ['AIX'],
                     'HP-UX': ['HPUX'],
                     'Darwin': ['MacOSX'],
                     'FreeBSD': ['FreeBSD', 'TrueOS'],
                     'ClearLinux': ['Clear Linux OS', 'Clear Linux Mix'],
                     'EulerOS': ['EulerOS'],
                     'openEuler': ['openEuler'],
                     'AlmaLinux': ['AlmaLinux']
                     }

    OS_FAMILY = {}
    for family, names in OS_FAMILY_MAP.items():
        for name in names:
            OS_FAMILY[name] = family

    def __init__(self, module):
        self.module = module

# Remaining code here is the same as provided in previous adaptations."
-------------------------------------------------------------------------
"'OEL', 'Amazon', 'Virtuozzo', 'XenServer', 'Alibaba',
'AlmaLinux'],
"
-------------------------------------------------------------------------
"Recom
PRs: 73541, 73544"
-------------------------------------------------------------------------
=========================================================================
"if 'BHYVE' in out:
    guest_tech.add('bhyve')
    if not found_virt:
        virtual_facts['virtualization_type'] = 'bhyve'
        virtual_facts['virtualization_role'] = 'guest'
        found_virt = True
"
-------------------------------------------------------------------------
"if 'bhyve' in out:
    guest_tech.add('bhyve')
    if not found_virt:
        virtual_facts['virtualization_type'] = 'bhyve'
        virtual_facts['virtualization_role'] = 'guest'
        found_virt = True"
-------------------------------------------------------------------------
"if 'BHYVE' in out:
    virtual_facts['virtualization_type'] = 'bhyve'
    virtual_facts['virtualization_role'] = 'guest'
    return virtual_facts
"
-------------------------------------------------------------------------
"Recom
PRs: 73204, 73234"
-------------------------------------------------------------------------
=========================================================================
"# -*- coding: utf-8 -*-
# Copyright (c) 2020 Ansible Project
# GNU General Public License v3.0 (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
from __future__ import absolute_import, division, print_function
__metaclass__ = type
from ansible.module_utils.facts.virtual import linux
def test_get_virtual_facts_bhyve(mocker):
    mocker.patch('os.path.exists', return_value=False)
    mocker.patch('ansible.module_utils.facts.virtual.linux.get_file_content', return_value='')
    mocker.patch('ansible.module_utils.facts.virtual.linux.get_file_lines', return_value=[])
    module = mocker.Mock()
    module.run_command.return_value = (0, 'BHYVE\n', '')
    inst = linux.LinuxVirtual(module)
    facts = inst.get_virtual_facts()
    expected = {
        'virtualization_role': 'guest',
        'virtualization_tech_host': set(),
        'virtualization_type': 'bhyve',
        'virtualization_tech_guest': set(['bhyve']),
    }
    assert facts == expected
"
-------------------------------------------------------------------------
"# -*- coding: utf-8 -*-
# Copyright (c) 2020 Ansible Project
# GNU General Public License v3.0 (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)

from __future__ import absolute_import, division, print_function
__metaclass__ = type

from ansible.module_utils.facts.virtual import linux


def test_get_virtual_facts_bhyve(mocker):
    mocker.patch('os.path.exists', return_value=False)
    mocker.patch('ansible.module_utils.facts.virtual.linux.get_file_content', return_value='')
    mocker.patch('ansible.module_utils.facts.virtual.linux.get_file_lines', return_value=[])

    module = mocker.Mock()
    module.run_command.return_value = (0, 'BHYVE\n', '')
    inst = linux.LinuxVirtual(module)

    facts = inst.get_virtual_facts()
    expected = {
        'virtualization_role': 'guest',
        'virtualization_type': 'bhyve',
    }

    assert facts == expected"
-------------------------------------------------------------------------
"# -*- coding: utf-8 -*-
# Copyright (c) 2020 Ansible Project
# GNU General Public License v3.0 (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
from __future__ import absolute_import, division, print_function
__metaclass__ = type
from ansible.module_utils.facts.virtual import linux
def test_get_virtual_facts_bhyve(mocker):
    mocker.patch('os.path.exists', return_value=False)
    mocker.patch('ansible.module_utils.facts.virtual.linux.get_file_content', return_value='')
    mocker.patch('ansible.module_utils.facts.virtual.linux.get_file_lines', return_value=[])
    module = mocker.Mock()
    module.run_command.return_value = (0, 'BHYVE\n', '')
    inst = linux.LinuxVirtual(module)
    facts = inst.get_virtual_facts()
    expected = {
        'virtualization_role': 'guest',
        'virtualization_type': 'bhyve',
    }
    assert facts == expected
"
-------------------------------------------------------------------------
"Recom
PRs: 73204, 73234"
-------------------------------------------------------------------------
=========================================================================
"if 'BHYVE' in out:
    guest_tech.add('bhyve')
    if not found_virt:
        virtual_facts['virtualization_type'] = 'bhyve'
        virtual_facts['virtualization_role'] = 'guest'
        found_virt = True
"
-------------------------------------------------------------------------
"if 'BHYVE' in out:
    guest_tech.add('bhyve')
    if not found_virt:
        virtual_facts['virtualization_type'] = 'bhyve'
        virtual_facts['virtualization_role'] = 'guest'
        found_virt = True"
-------------------------------------------------------------------------
"if 'BHYVE' in out:
    virtual_facts['virtualization_type'] = 'bhyve'
    virtual_facts['virtualization_role'] = 'guest'
    return virtual_facts
"
-------------------------------------------------------------------------
"Recom
PRs: 73204, 73233"
-------------------------------------------------------------------------
=========================================================================
"# -*- coding: utf-8 -*-
# Copyright (c) 2020 Ansible Project
# GNU General Public License v3.0 (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
from __future__ import absolute_import, division, print_function
__metaclass__ = type
from ansible.module_utils.facts.virtual import linux
def test_get_virtual_facts_bhyve(mocker):
    mocker.patch('os.path.exists', return_value=False)
    mocker.patch('ansible.module_utils.facts.virtual.linux.get_file_content', return_value='')
    mocker.patch('ansible.module_utils.facts.virtual.linux.get_file_lines', return_value=[])
    module = mocker.Mock()
    module.run_command.return_value = (0, 'BHYVE\n', '')
    inst = linux.LinuxVirtual(module)
    facts = inst.get_virtual_facts()
    expected = {
        'virtualization_role': 'guest',
        'virtualization_tech_host': set(),
        'virtualization_type': 'bhyve',
        'virtualization_tech_guest': set(['bhyve']),
    }
    assert facts == expected
"
-------------------------------------------------------------------------
"# -*- coding: utf-8 -*-
# Copyright (c) 2020 Ansible Project
# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
from __future__ import absolute_import, division, print_function
__metaclass__ = type
from ansible.module_utils.facts.virtual import linux
from __future__ import ansible.module_utils.facts.virtual
def test_get_virtual_facts_bhyve(mocker):
    mocker.patch('os.path.exists', return_value=False)
    mocker.patch('ansible.module_utils.facts.virtual.linux.get_file_content', return_value='')
    mocker.patch('ansible.module_utils.facts.virtual.linux.get_file_lines', return_value=[])
    module = mocker.Mock()
    module.run_command.return_value = (0, 'BHYVE\n', '')
    inst = linux.LinuxVirtual(module)
    facts = inst.get_virtual_facts()
    expected = {
        'virtualization_role': 'guest',
        'virtualization_type': 'bhyve',
    }
    assert facts == expected"
-------------------------------------------------------------------------
"# -*- coding: utf-8 -*-
# Copyright (c) 2020 Ansible Project
# GNU General Public License v3.0 (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
from __future__ import absolute_import, division, print_function
__metaclass__ = type
from ansible.module_utils.facts.virtual import linux
def test_get_virtual_facts_bhyve(mocker):
    mocker.patch('os.path.exists', return_value=False)
    mocker.patch('ansible.module_utils.facts.virtual.linux.get_file_content', return_value='')
    mocker.patch('ansible.module_utils.facts.virtual.linux.get_file_lines', return_value=[])
    module = mocker.Mock()
    module.run_command.return_value = (0, 'BHYVE\n', '')
    inst = linux.LinuxVirtual(module)
    facts = inst.get_virtual_facts()
    expected = {
        'virtualization_role': 'guest',
        'virtualization_type': 'bhyve',
    }
    assert facts == expected
"
-------------------------------------------------------------------------
"Recom
PRs: 73204, 73233"
-------------------------------------------------------------------------
=========================================================================
"import sys
"
-------------------------------------------------------------------------
"import ansible
import ansible.playbook.block
import ansible.vars.reserved
import ansible.utils.helpers
import traceback
from __future__ import (absolute_import, division, print_function)
import ansible.playbook.play_context
import ansible.utils.display
import ansible.utils.multiprocessing
import ansible.errors
import ansible.plugins.loader
import ansible.module_utils._text
import ansible.executor.stats
import ansible.module_utils.six
import ansible.executor.task_result
import ansible.template
import tempfile
from ansible.vars.hostvars import HostVars
import os
import ansible.executor.play_iterator
import ansible.utils.collection_loader
import ansible.plugins.callback"
-------------------------------------------------------------------------
"import sys
import time
"
-------------------------------------------------------------------------
"Recom
PRs: 72604, 72609"
-------------------------------------------------------------------------
=========================================================================
"def post_process_whens(result, task, templar):
    cond = None
    if task.changed_when:
        cond = Conditional(loader=templar._loader)
        cond.when = task.changed_when
        result['changed'] = cond.evaluate_conditional(templar, templar.available_variables)
    if task.failed_when:
        if cond is None:
            cond = Conditional(loader=templar._loader)
        cond.when = task.failed_when
        failed_when_result = cond.evaluate_conditional(templar, templar.available_variables)
        result['failed_when_result'] = result['failed'] = failed_when_result
"
-------------------------------------------------------------------------
"def post_process_whens(result, task, templar):
    cond = None
    if task.get_when('changed'):
        cond = Conditional(loader=templar._loader)
        cond.when = task.get_when('changed')
        result['changed'] = cond.evaluate_conditional(templar, templar.available_variables)
    if task.get_when('failed'):
        if cond is None:
            cond = Conditional(loader=templar._loader)
        cond.when = task.get_when('failed')
        failed_when_result = cond.evaluate_conditional(templar, templar.available_variables)
        result['failed_when_result'] = result['failed'] = failed_when_result"
-------------------------------------------------------------------------
"_sentinel = StrategySentinel()
"
-------------------------------------------------------------------------
"Recom
PRs: 70919, 72118"
-------------------------------------------------------------------------
=========================================================================
"# Validate GPG. This is NOT done in dnf.Base (it's done in the
# upstream CLI subclass of dnf.Base)
if not self.disable_gpg_check:
    for package in self.base.transaction.install_set:
        fail = False
        gpgres, gpgerr = self.base._sig_check_pkg(package)
        if gpgres == 0:  # validated successfully
            continue
        elif gpgres == 1:  # validation failed, install cert?
            try:
                self.base._get_key_for_package(package)
            except dnf.exceptions.Error as e:
                fail = True
        else:  # fatal error
            fail = True
        if fail:
            msg = 'Failed to validate GPG signature for {0}'.format(package)
            self.module.fail_json(msg)
"
-------------------------------------------------------------------------
"# Validate GPG. This is NOT done in dnf.Base (it's done in the
# upstream CLI subclass of dnf.Base)
if not self.disable_gpg_check:
    for package in self.base.transaction.install_set:
        fail = False
        gpgres = gpgerr = None
        package = self._package_dict(package)
        gpgres = package.verify_gpg_signature()
        if gpgres == ""VALID"":  # validated successfully
            continue
        if gpgres == ""UNKNOWN"":  # validation failed, install cert?
            try:
                self.base.install_gpg_key_for_package(package['name'])
            except dnf.exceptions.Error as e:
                fail = True
        else:  # fatal error
            fail = True
        if fail:
            msg = 'Failed to validate GPG signature for {0}'.format(package['name'])
            self.module.fail_json(msg)"
-------------------------------------------------------------------------
"# Validate GPG. This is NOT done in dnf.Base (it's done in the
# upstream CLI subclass of dnf.Base)
if not self.disable_gpg_check:
    for package in self.base.transaction.install_set:
        fail = False
        gpgres, gpgerr = self.base._sig_check_pkg(package)
        if gpgres == 0:  # validated successfully
            continue
        elif gpgres == 1:  # validation failed, install cert?
            try:
                self.base._get_key_for_package(package)
            except dnf.exceptions.Error as e:
                fail = True
        else:  # fatal error
            fail = True
        if fail:
            msg = 'Failed to validate GPG signature for {0}'.format(package)
            self.module.fail_json(msg=msg)
"
-------------------------------------------------------------------------
"Recom
PRs: 71537, 71541"
-------------------------------------------------------------------------
=========================================================================
"# Validate GPG. This is NOT done in dnf.Base (it's done in the
# upstream CLI subclass of dnf.Base)
if not self.disable_gpg_check:
    for package in self.base.transaction.install_set:
        fail = False
        gpgres, gpgerr = self.base._sig_check_pkg(package)
        if gpgres == 0:  # validated successfully
            continue
        elif gpgres == 1:  # validation failed, install cert?
            try:
                self.base._get_key_for_package(package)
            except dnf.exceptions.Error as e:
                fail = True
        else:  # fatal error
            fail = True
        if fail:
            msg = 'Failed to validate GPG signature for {0}'.format(package)
            self.module.fail_json(msg)
"
-------------------------------------------------------------------------
"# Validate GPG. This is NOT done in dnf.Base (it's done in the
# upstream CLI subclass of dnf.Base)
if not self.disable_gpg_check:
    for package in self.base.transaction.install_set:
        fail = False
        gpgres = None
        gpgerr = None
        try:
            gpgres = dnf.util.ut.build_and_check_package(self.base.ts, str(package), self.base.rpmdb.simpleVersion(platform.machine()))
        except dnf.exceptions.RepoError as e:
            msg = 'Error checking gpg for package {0}: {1}'.format(package, str(e))
            self.module.fail_json(msg)
        if gpgres == dnf.constants.GPG_GOOD:
            continue
        elif gpgres == dnf.constants.GPG_BAD:
            try:
                self.base._get_key_for_package(package)
            except dnf.exceptions.Error:
                fail = True
        else:  # GPG_UNKNOWN
            fail = True
        if fail:
            msg = 'Failed to validate GPG signature for {0}'.format(package)
            self.module.fail_json(msg)"
-------------------------------------------------------------------------
"# Validate GPG. This is NOT done in dnf.Base (it's done in the
# upstream CLI subclass of dnf.Base)
if not self.disable_gpg_check:
    for package in self.base.transaction.install_set:
        fail = False
        gpgres, gpgerr = self.base._sig_check_pkg(package)
        if gpgres == 0:  # validated successfully
            continue
        elif gpgres == 1:  # validation failed, install cert?
            try:
                self.base._get_key_for_package(package)
            except dnf.exceptions.Error as e:
                fail = True
        else:  # fatal error
            fail = True
        if fail:
            msg = 'Failed to validate GPG signature for {0}'.format(package)
            self.module.fail_json(msg=msg)
"
-------------------------------------------------------------------------
"Recom
PRs: 71537, 71540"
-------------------------------------------------------------------------
=========================================================================
"- module: ansible.builtin.blockinfile
- module: ansible.builtin.copy
- module: ansible.builtin.file
- module: ansible.builtin.replace
- module: ansible.builtin.template
- module: ansible.windows.win_lineinfile
"
-------------------------------------------------------------------------
"- module: blockinfile
- module: copy
- module: file
- module: replace
- module: template
- module: win_lineinfile"
-------------------------------------------------------------------------
"- module: ansible.builtin.blockinfile
- module: ansible.builtin.copy
- module: ansible.builtin.file
- module: ansible.builtin.replace
- module: ansible.builtin.template
- module: community.windows.win_lineinfile
"
-------------------------------------------------------------------------
"Recom
PRs: 70530, 71380"
-------------------------------------------------------------------------
=========================================================================
"- module: ansible.builtin.authorized_key
- module: ansible.builtin.group
- module: ansible.windows.win_user
"
-------------------------------------------------------------------------
"import ansible.module_utils.basic
import ansible.module_utils._text

def add_fqcn():
    ''' Adds Fully Qualified Class Names in the references to inter-module `M()` in the module docs '''
    fqcn_authorized_key = ansible.module_utils.basic.authorized_key
    fqcn_group = ansible.module_utils.basic.group
    fqcn_win_user = ansible.windows.win_user

    # Rest of the code remains the same with the new FQCNs
    # ...

add_fqcn()"
-------------------------------------------------------------------------
"- module: ansible.posix.authorized_key
- module: ansible.builtin.group
- module: ansible.windows.win_user
"
-------------------------------------------------------------------------
"Recom
PRs: 70530, 71380"
-------------------------------------------------------------------------
=========================================================================
"- module: ansible.builtin.wait_for
- module: ansible.windows.win_wait_for
- module: ansible.windows.win_wait_for_process
"
-------------------------------------------------------------------------
"from __future__ import absolute_import, division, print_function
__metaclass__ = type

DOCUMENTATION = r'''
---
module: ansible.windows.win_wait_for
short_description: Wait for given condition before continuing
description:
    - This module allows you to wait for a given condition to be true before continuing with the playbook execution on Windows hosts.
    - The conditions can be related to a file, a process, a port, or a service.
    - This module is also supported for Windows targets.
    - For non-Windows targets, use the M(ansible.builtin.wait_for) module.
version_added: ""1.5""
'''

EXAMPLES = r'''
- name: Wait for a process to start
  ansible.windows.win_wait_for_process:
    path: ""my_process.exe""
    state: started

- name: Wait until a port becomes available
  ansible.windows.win_wait_for:
    port: 80
    state: started

- name: Wait for a specific file to be present
  ansible.windows.win_wait_for:
    path: ""C:\\path\\to\\file""
    state: present
'''

This adapted code snippet combines the functionality of `ansible.builtin.wait_for`, `ansible.windows.win_wait_for`, and `ansible.windows.win_wait_for_process` into a single module `ansible.windows.win_wait_for` for waiting for conditions before continuing execution on Windows hosts."
-------------------------------------------------------------------------
"- module: ansible.builtin.wait_for
- module: ansible.windows.win_wait_for
- module: community.windows.win_wait_for_process
"
-------------------------------------------------------------------------
"Recom
PRs: 70530, 71380"
-------------------------------------------------------------------------
=========================================================================
"- If you wish to update an existing repository definition use M(ansible.builtin.ini_file) instead.
"
-------------------------------------------------------------------------
"substitutions = {'mod': None, 'cmd': command}

    if command in arguments:
        msg = ""Consider using the ansible.builtin.file module with {subcmd} rather than running '{cmd}'.  "" + disable_suffix
        substitutions['mod'] = 'ansible.builtin.ini_file'
        substitutions['subcmd'] = arguments[command]
        module.warn(msg.format(**substitutions))

    if command in commands:
        msg = ""Consider using the ansible.builtin.{mod} module rather than running '{cmd}'.  "" + disable_suffix
        substitutions['mod'] = commands[command]
        module.warn(msg.format(**substitutions))"
-------------------------------------------------------------------------
"- If you wish to update an existing repository definition use M(community.general.ini_file) instead.
"
-------------------------------------------------------------------------
"Recom
PRs: 70530, 71380"
-------------------------------------------------------------------------
=========================================================================
"from ansible.module_utils.common.collections import is_sequence, Mapping
def _fail_on_undefined(data):
    """"""Recursively find an undefined value in a nested data structure
    and properly raise the undefined exception.
    """"""
    if isinstance(data, Mapping):
        for value in data.values():
            _fail_on_undefined(value)
    elif is_sequence(data):
        for item in data:
            _fail_on_undefined(item)
    else:
        if isinstance(data, StrictUndefined):
            # To actually raise the undefined exception we need to
            # access the undefined object otherwise the exception would
            # be raised on the next access which might not be properly
            # handled.
            # See https://github.com/ansible/ansible/issues/52158
            # and StrictUndefined implementation in upstream Jinja2.
            str(data)
    return data
    https://github.com/pallets/jinja/blob/master/src/jinja2/nativetypes.py
    """"""
        out = _fail_on_undefined(head[0])
        out = u''.join([to_text(_fail_on_undefined(v)) for v in nodes])
"
-------------------------------------------------------------------------
"from ansible.parsing.yaml.objects import is_sequence, Mapping
def _fail_on_undefined(data):
    """"""Recursively find an undefined value in a nested data structure
    and properly raise the undefined exception.
    """"""
    if isinstance(data, Mapping):
        for value in data.values():
            _fail_on_undefined(value)
    elif is_sequence(data):
        for item in data:
            _fail_on_undefined(item)
    else:
        if isinstance(data, StrictUndefined):
            # To actually raise the undefined exception we need to
            # access the undefined object otherwise the exception would
            # be raised on the next access which might not be properly
            # handled.
            # See https://github.com/ansible/ansible/issues/52158
            # and StrictUndefined implementation in upstream Jinja2.
            str(data)
    return data
    # https://github.com/pallets/jinja/blob/master/jinja2/nativetypes.py
    """"""
        out = _fail_on_undefined(head[0])
        out = u''.join([to_text(_fail_on_undefined(v)) for v in nodes])"
-------------------------------------------------------------------------
"from ansible.module_utils.common.collections import is_sequence, Mapping
"
-------------------------------------------------------------------------
"Recom
PRs: 68432, 71105"
-------------------------------------------------------------------------
=========================================================================
"# `distutils` must be imported after `setuptools` or it will cause explosions
# with `setuptools >=48.0.0, <49.1`.
# Refs:
# * https://github.com/ansible/ansible/issues/70456
# * https://github.com/pypa/setuptools/issues/2230
# * https://github.com/pypa/setuptools/commit/bd110264
from distutils.command.build_scripts import build_scripts as BuildScripts
from distutils.command.sdist import sdist as SDist
"
-------------------------------------------------------------------------
"from setuptools.command.build_scripts import build_scripts as BuildScripts
from setuptools.command.sdist import sdist as SDist"
-------------------------------------------------------------------------
"# `distutils` must be imported after `setuptools` or it will cause explosions
# with `setuptools >=48.0.0, <49.1`.
# Refs:
# * https://github.com/ansible/ansible/issues/70456
# * https://github.com/pypa/setuptools/issues/2230
# * https://github.com/pypa/setuptools/commit/bd110264
from distutils.command.build_scripts import build_scripts as BuildScripts
from distutils.command.sdist import sdist as SDist
def find_package_info(*file_paths):
    try:
        with open(os.path.join(*file_paths), 'r') as f:
            info_file = f.read()
    except Exception:
        raise RuntimeError(""Unable to find package info."")
    # The version line must have the form
    # __version__ = 'ver'
    version_match = re.search(r""^__version__ = ['\""]([^'\""]*)['\""]"",
                              info_file, re.M)
    author_match = re.search(r""^__author__ = ['\""]([^'\""]*)['\""]"",
                             info_file, re.M)
    if version_match and author_match:
        return version_match.group(1), author_match.group(1)
    raise RuntimeError(""Unable to find package info."")
def _validate_install_ansible_base():
    """"""Validate that we can install ansible-base. Currently this only
    cares about upgrading to ansible-base from ansible<2.10
    """"""
    if os.getenv('ANSIBLE_SKIP_CONFLICT_CHECK', '') not in ('', '0'):
        return
    # Save these for later restoring things to pre invocation
    sys_modules = sys.modules.copy()
    sys_modules_keys = set(sys_modules)
    # Make sure `lib` isn't in `sys.path` that could confuse this
    sys_path = sys.path[:]
    abspath = os.path.abspath
    sys.path[:] = [p for p in sys.path if abspath(p) != abspath('lib')]
    try:
        from ansible.release import __version__
    except ImportError:
        pass
    else:
        version_tuple = tuple(int(v) for v in __version__.split('.')[:2])
        if version_tuple < (2, 10):
            stars = '*' * 76
            raise RuntimeError(
                '''
    %s
    Cannot install ansible-base with a pre-existing ansible==%s
    installation.
    Installing ansible-base with ansible-2.9 or older currently installed with
    pip is known to cause problems. Please uninstall ansible and install the new
    version:
        pip uninstall ansible
        pip install ansible-base
    If you want to skip the conflict checks and manually resolve any issues
    afterwards, set the ANSIBLE_SKIP_CONFLICT_CHECK environment variable:
        ANSIBLE_SKIP_CONFLICT_CHECK=1 pip install ansible-base
    %s
                ''' % (stars, __version__, stars)
            )
    finally:
        sys.path[:] = sys_path
        for key in sys_modules_keys.symmetric_difference(sys.modules):
            sys.modules.pop(key, None)
        sys.modules.update(sys_modules)
_validate_install_ansible_base()
"
-------------------------------------------------------------------------
"Recom
PRs: 70525, 70760"
-------------------------------------------------------------------------
=========================================================================
"from ansible.module_utils.common.text.converters import container_to_text, to_native
from ansible.module_utils.six import string_types, PY2
"
-------------------------------------------------------------------------
"from ansible.module_utils.common._collections_compat import Set
import datetime
import json

from ansible.module_utils._text import to_bytes, to_native, to_text
from ansible.module_utils.common._collections_compat import Set
from ansible.module_utils.six import (
    binary_type,
    iteritems,
    text_type,
)

def container_to_bytes(d, encoding='utf-8', errors='surrogate_or_strict'):
    ''' Recursively convert dict keys and values to byte str

        Specialized for json return because this only handles, lists, tuples,
        and dict container types (the containers that the json module returns)
    '''

    if isinstance(d, text_type):
        return to_bytes(d, encoding=encoding, errors=errors)
    elif isinstance(d, dict):
        return dict(container_to_bytes(o, encoding, errors) for o in iteritems(d))
    elif isinstance(d, list):
        return [container_to_bytes(o, encoding, errors) for o in d]
    elif isinstance(d, tuple):
        return tuple(container_to_bytes(o, encoding, errors) for o in d)
    else:
        return d"
-------------------------------------------------------------------------
"from ansible.module_utils._text import to_native
from ansible.module_utils.common.text.converters import container_to_text
from ansible.module_utils.six import string_types, PY2
"
-------------------------------------------------------------------------
"Recom
PRs: 68576, 69626"
-------------------------------------------------------------------------
=========================================================================
"finally:
    self._clean_up()
"
-------------------------------------------------------------------------
self._clean_up()
-------------------------------------------------------------------------
"# NOTE: this works due to fork, if switching to threads this should change to per thread storage of temp files
# clear var to ensure we only delete files for this child
self._loader._tempfiles = set()
"
-------------------------------------------------------------------------
"Recom
PRs: 68433, 68470"
-------------------------------------------------------------------------
=========================================================================
"'CREATE ROLE', 'DROP ROLE', 'APPLICATION_PASSWORD_ADMIN',
'AUDIT_ADMIN', 'BACKUP_ADMIN', 'BINLOG_ADMIN',
'BINLOG_ENCRYPTION_ADMIN', 'CLONE_ADMIN', 'CONNECTION_ADMIN',
'ENCRYPTION_KEY_ADMIN', 'FIREWALL_ADMIN', 'FIREWALL_USER',
'GROUP_REPLICATION_ADMIN', 'INNODB_REDO_LOG_ARCHIVE',
'NDB_STORED_USER', 'PERSIST_RO_VARIABLES_ADMIN',
'REPLICATION_APPLIER', 'REPLICATION_SLAVE_ADMIN',
'RESOURCE_GROUP_ADMIN', 'RESOURCE_GROUP_USER',
'ROLE_ADMIN', 'SESSION_VARIABLES_ADMIN', 'SET_USER_ID',
'SYSTEM_USER', 'SYSTEM_VARIABLES_ADMIN', 'SYSTEM_USER',
'TABLE_ENCRYPTION_ADMIN', 'VERSION_TOKEN_ADMIN',
'XA_RECOVER_ADMIN', 'LOAD FROM S3', 'SELECT INTO S3'))
"
-------------------------------------------------------------------------
"VALID_PRIVS = frozenset(('CREATE', 'DROP', 'GRANT', 'GRANT OPTION',
                         'LOCK TABLES', 'REFERENCES', 'EVENT', 'ALTER',
                         'DELETE', 'INDEX', 'INSERT', 'SELECT', 'UPDATE',
                         'CREATE TEMPORARY TABLES', 'TRIGGER', 'CREATE VIEW',
                         'SHOW VIEW', 'ALTER ROUTINE', 'CREATE ROUTINE',
                         'EXECUTE', 'FILE', 'CREATE TABLESPACE', 'CREATE USER',
                         'PROCESS', 'PROXY', 'RELOAD', 'REPLICATION CLIENT',
                         'REPLICATION SLAVE', 'SHOW DATABASES', 'SHUTDOWN',
                         'SUPER', 'ALL', 'ALL PRIVILEGES', 'USAGE', 'REQUIRESSL',
                         'CREATE ROLE', 'DROP ROLE', 'APPLICATION_PASSWORD_ADMIN',
                         'AUDIT_ADMIN', 'BACKUP_ADMIN', 'BINLOG_ADMIN',
                         'BINLOG_ENCRYPTION_ADMIN', 'CLONE_ADMIN', 'CONNECTION_ADMIN',
                         'ENCRYPTION_KEY_ADMIN', 'FIREWALL_ADMIN', 'FIREWALL_USER',
                         'GROUP_REPLICATION_ADMIN', 'INNODB_REDO_LOG_ARCHIVE',
                         'NDB_STORED_USER', 'PERSIST_RO_VARIABLES_ADMIN',
                         'REPLICATION_APPLIER', 'REPLICATION_SLAVE_ADMIN',
                         'RESOURCE_GROUP_ADMIN', 'RESOURCE_GROUP_USER',
                         'ROLE_ADMIN', 'SESSION_VARIABLES_ADMIN', 'SET_USER_ID',
                         'SYSTEM_USER', 'SYSTEM_VARIABLES_ADMIN', 'SYSTEM_USER',
                         'TABLE_ENCRYPTION_ADMIN', 'VERSION_TOKEN_ADMIN',
                         'XA_RECOVER_ADMIN', 'LOAD FROM S3', 'SELECT INTO S3'))

class InvalidPrivsError(Exception):
    pass"
-------------------------------------------------------------------------
"'CREATE ROLE', 'DROP ROLE', 'APPLICATION_PASSWORD_ADMIN',
'AUDIT_ADMIN', 'BACKUP_ADMIN', 'BINLOG_ADMIN',
'BINLOG_ENCRYPTION_ADMIN', 'CONNECTION_ADMIN',
'ENCRYPTION_KEY_ADMIN', 'FIREWALL_ADMIN', 'FIREWALL_USER',
'GROUP_REPLICATION_ADMIN', 'PERSIST_RO_VARIABLES_ADMIN',
'REPLICATION_SLAVE_ADMIN', 'RESOURCE_GROUP_ADMIN', 'RESOURCE_GROUP_USER',
'ROLE_ADMIN', 'SESSION_VARIABLES_ADMIN', 'SET_USER_ID',
'SYSTEM_VARIABLES_ADMIN', 'VERSION_TOKEN_ADMIN', 'XA_RECOVER_ADMIN'))
"
-------------------------------------------------------------------------
"Recom
PRs: 66995, 66999"
-------------------------------------------------------------------------
=========================================================================
"if not container.exists or container.removing:
    if container.removing:
        self.log('Found container in removal phase')
    else:
        self.log('No container found')
    if container.removing:
        # Wait for container to be removed before trying to create it
        self.wait_for_state(container.Id, wait_states=['removing'], accept_removal=True)
"
-------------------------------------------------------------------------
"if not self.container_exists or self.container_removing:
        if self.container_removing:
            self.log('Found container in removal phase')
        else:
            self.log('No container found')
        if self.container_removing:
            # Wait for container to be removed before trying to create it
            self.wait_for_state(self.container_id, wait_states=['removing'], accept_removal=True)"
-------------------------------------------------------------------------
"if not container.exists or container.removing:
    if container.removing:
        self.log('Found container in removal phase')
    else:
        self.log('No container found')
    if container.removing and not self.check_mode:
        # Wait for container to be removed before trying to create it
        self.wait_for_state(container.Id, wait_states=['removing'], accept_removal=True)
"
-------------------------------------------------------------------------
"Recom
PRs: 65854, 66118"
-------------------------------------------------------------------------
=========================================================================
"self.wait_for_state(container.Id, wait_states=['removing'], accept_removal=True)
"
-------------------------------------------------------------------------
"self.wait_for_state(id, wait_states=['removing'], accept_removal=True)"
-------------------------------------------------------------------------
"if not self.check_mode:
    self.wait_for_state(container.Id, wait_states=['removing'], accept_removal=True)
"
-------------------------------------------------------------------------
"Recom
PRs: 65854, 66118"
-------------------------------------------------------------------------
=========================================================================
"if not container.exists or container.removing:
    if container.removing:
        self.log('Found container in removal phase')
    else:
        self.log('No container found')
    if container.removing:
        # Wait for container to be removed before trying to create it
        self.wait_for_state(container.Id, wait_states=['removing'], accept_removal=True)
"
-------------------------------------------------------------------------
"if not container or container.removing:
    if container and container.removing:
        self.log('Found container in removal phase')
    else:
        self.log('No container found')

    if container and container.removing:
        # Wait for container to be removed before trying to create it
        self.wait_for_state(container.Id, wait_states=['removing'], accept_removal=True)"
-------------------------------------------------------------------------
"if not container.exists or container.removing:
    if container.removing:
        self.log('Found container in removal phase')
    else:
        self.log('No container found')
    if container.removing and not self.check_mode:
        # Wait for container to be removed before trying to create it
        self.wait_for_state(container.Id, wait_states=['removing'], accept_removal=True)
"
-------------------------------------------------------------------------
"Recom
PRs: 65854, 66117"
-------------------------------------------------------------------------
=========================================================================
"self.wait_for_state(container.Id, wait_states=['removing'], accept_removal=True)
"
-------------------------------------------------------------------------
"Add the mentioned code line within the target method as seen below: 


        if network_id is not None:
            try:
                self.log(""Inspecting network Id %s"" % network_id)
                result = self.inspect_network(network_id)
                self.log(""Completed network inspection"")
            except NotFound as dummy:
                return None
            except Exception as exc:
                self.fail(""Error inspecting network: %s"" % exc)

            self.wait_for_state(network_id, wait_states=['removing'], accept_removal=True)


Make sure the `self.wait_for_state()` method is defined within the same class or imported from relevant modules in the stable version."
-------------------------------------------------------------------------
"if not self.check_mode:
    self.wait_for_state(container.Id, wait_states=['removing'], accept_removal=True)
"
-------------------------------------------------------------------------
"Recom
PRs: 65854, 66117"
-------------------------------------------------------------------------
=========================================================================
"@property
    if self._yum_base:
        return self._yum_base
    else:
        # Only init once
        self._yum_base = yum.YumBase()
        self._yum_base.preconf.debuglevel = 0
        self._yum_base.preconf.errorlevel = 0
        self._yum_base.preconf.plugins = True
        self._yum_base.preconf.enabled_plugins = self.enable_plugin
        self._yum_base.preconf.disabled_plugins = self.disable_plugin
        if self.releasever:
            self._yum_base.preconf.releasever = self.releasever
        if self.installroot != '/':
            # do not setup installroot by default, because of error
            # CRITICAL:yum.cli:Config Error: Error accessing file for config file:////etc/yum.conf
            # in old yum version (like in CentOS 6.6)
            self._yum_base.preconf.root = self.installroot
            self._yum_base.conf.installroot = self.installroot
        if self.conf_file and os.path.exists(self.conf_file):
            self._yum_base.preconf.fn = self.conf_file
        if os.geteuid() != 0:
            if hasattr(self._yum_base, 'setCacheDir'):
                self._yum_base.setCacheDir()
            else:
                cachedir = yum.misc.getCacheDir()
                self._yum_base.repos.setCacheDir(cachedir)
                self._yum_base.conf.cache = 0
        if self.disable_excludes:
            self._yum_base.conf.disable_excludes = self.disable_excludes
        # A sideeffect of accessing conf is that the configuration is
        # loaded and plugins are discovered
        self.yum_base.conf
        try:
            self._enablerepos_with_error_checking(self._yum_base)
            for rid in self.disablerepo:
                self.yum_base.repos.disableRepo(rid)
        except Exception as e:
            self.module.fail_json(msg=""Failure talking to yum: %s"" % to_native(e))
    return self._yum_base
"
-------------------------------------------------------------------------
"@property
    def yum_base(self):
        if self._yum_base:
            return self._yum_base
        else:
            # Only init once
            self._yum_base = ansible.module_utils.yumdnf.YumBase()
            self._yum_base.preconf.debuglevel = 0
            self._yum_base.preconf.errorlevel = 0
            self._yum_base.preconf.plugins = True
            self._yum_base.preconf.enabled_plugins = self.enable_plugin
            self._yum_base.preconf.disabled_plugins = self.disable_plugin
            if self.releasever:
                self._yum_base.preconf.releasever = self.releasever
            if self.installroot != '/':
                # do not setup installroot by default, because of error
                # CRITICAL:yum.cli:Config Error: Error accessing file for config file:////etc/yum.conf
                # in old yum version (like in CentOS 6.6)
                self._yum_base.preconf.root = self.installroot
                self._yum_base.conf.installroot = self.installroot
            if self.conf_file and os.path.exists(self.conf_file):
                self._yum_base.preconf.fn = self.conf_file
            if os.geteuid() != 0:
                if hasattr(self._yum_base, 'setCacheDir'):
                    self._yum_base.setCacheDir()
                else:
                    cachedir = ansible.module_utils.yum.misc.getCacheDir()
                    self._yum_base.repos.setCacheDir(cachedir)
                    self._yum_base.conf.cache = 0
            if self.disable_excludes:
                self._yum_base.conf.disable_excludes = self.disable_excludes
            # A sideeffect of accessing conf is that the configuration is
            # loaded and plugins are discovered
            self._yum_base.conf
            try:
                self._enablerepos_with_error_checking(self._yum_base)
                for rid in self.disablerepo:
                    self._yum_base.repos.disableRepo(rid)
            except Exception as e:
                self.module.fail_json(msg=""Failure talking to yum: %s"" % to_native(e))
            return self._yum_base"
-------------------------------------------------------------------------
"def _enablerepos_with_error_checking(self, yumbase):
    # NOTE: This seems unintuitive, but it mirrors yum's CLI bahavior
    if len(self.enablerepo) == 1:
        try:
            yumbase.repos.enableRepo(self.enablerepo[0])
        except yum.Errors.YumBaseError as e:
            if u'repository not found' in to_text(e):
                self.module.fail_json(msg=""Repository %s not found."" % self.enablerepo[0])
            else:
                raise e
    else:
        for rid in self.enablerepo:
            try:
                yumbase.repos.enableRepo(rid)
            except yum.Errors.YumBaseError as e:
                if u'repository not found' in to_text(e):
                    self.module.warn(""Repository %s not found."" % rid)
                else:
                    raise e
@property
    if self._yum_base:
        return self._yum_base
    else:
        # Only init once
        self._yum_base = yum.YumBase()
        self._yum_base.preconf.debuglevel = 0
        self._yum_base.preconf.errorlevel = 0
        self._yum_base.preconf.plugins = True
        self._yum_base.preconf.enabled_plugins = self.enable_plugin
        self._yum_base.preconf.disabled_plugins = self.disable_plugin
        if self.releasever:
            self._yum_base.preconf.releasever = self.releasever
        if self.installroot != '/':
            # do not setup installroot by default, because of error
            # CRITICAL:yum.cli:Config Error: Error accessing file for config file:////etc/yum.conf
            # in old yum version (like in CentOS 6.6)
            self._yum_base.preconf.root = self.installroot
            self._yum_base.conf.installroot = self.installroot
        if self.conf_file and os.path.exists(self.conf_file):
            self._yum_base.preconf.fn = self.conf_file
        if os.geteuid() != 0:
            if hasattr(self._yum_base, 'setCacheDir'):
                self._yum_base.setCacheDir()
            else:
                cachedir = yum.misc.getCacheDir()
                self._yum_base.repos.setCacheDir(cachedir)
                self._yum_base.conf.cache = 0
        if self.disable_excludes:
            self._yum_base.conf.disable_excludes = self.disable_excludes
        # A sideeffect of accessing conf is that the configuration is
        # loaded and plugins are discovered
        self.yum_base.conf
        try:
            self._enablerepos_with_error_checking(self._yum_base)
            for rid in self.disablerepo:
                self.yum_base.repos.disableRepo(rid)
        except Exception as e:
            self.module.fail_json(msg=""Failure talking to yum: %s"" % to_native(e))
    return self._yum_base
"
-------------------------------------------------------------------------
"Recom
PRs: 63713, 65575"
-------------------------------------------------------------------------
=========================================================================
"e, m, _ = self.yum_base.rpmdb.matchPackageNames([pkgspec])
    pkgs.extend(self.yum_base.returnInstalledPackagesByDep(pkgspec))
"
-------------------------------------------------------------------------
"e, m, _ = my.rpmdb.matchPackageNames([pkgspec])
pkgs.extend(my.returnInstalledPackagesByDep(pkgspec))"
-------------------------------------------------------------------------
"groups_list = self.yum_base.doGroupLists(return_evgrps=True)
groups_list = self.yum_base.doGroupLists()
"
-------------------------------------------------------------------------
"Recom
PRs: 63713, 65575"
-------------------------------------------------------------------------
=========================================================================
"e, m, _ = self.yum_base.pkgSack.matchPackageNames([pkgspec])
    pkgs.extend(self.yum_base.returnPackagesByDep(pkgspec))
"
-------------------------------------------------------------------------
"Remove the lines as the function ""yum_base"" is not defined within the method ""is_group_env_installed""."
-------------------------------------------------------------------------
"e, m, _ = self.yum_base.rpmdb.matchPackageNames([pkgspec])
    pkgs.extend(self.yum_base.returnInstalledPackagesByDep(pkgspec))
"
-------------------------------------------------------------------------
"Recom
PRs: 63713, 65575"
-------------------------------------------------------------------------
=========================================================================
"elif current_condition['Field'] == condition['Field'] and current_condition['Values'] == condition['Values']:
"
-------------------------------------------------------------------------
elif current_condition['Field'] == condition['Field'] and current_condition['Values'][0] == condition['Values'][0]:
-------------------------------------------------------------------------
"elif current_condition['Field'] == condition['Field'] and sorted(current_condition['Values']) == sorted(condition['Values']):
"
-------------------------------------------------------------------------
"Recom
PRs: 65021, 65212"
-------------------------------------------------------------------------
=========================================================================
"feed_ca_cert:
      The ca_cert alias will be removed in Ansible 2.14.
  aliases: [ importer_ssl_ca_cert, ca_cert ]
feed_client_cert:
  version_added: ""2.10""
"
-------------------------------------------------------------------------
"+ Assign(targets=[Name(id='feed_ca_cert', ctx=Store())], value=Tuple(elts=[Call(func=Name(id='dict', ctx=Load()), args=[], keywords=[keyword(arg='aliases', value=List(elts=[Constant(value='importer_ssl_ca_cert'), Constant(value='ca_cert')], ctx=Load())), keyword(arg='deprecated_aliases', value=List(elts=[Call(func=Name(id='dict', ctx=Load()), args=[], keywords=[keyword(arg='name', value=Constant(value='ca_cert')), keyword(arg='version', value=Constant(value='2.14'))])], ctx=Load()))])], ctx=Load()))
+ Assign(targets=[Name(id='feed_client_cert', ctx=Store())], value=None)
+ AnnAssign(target=Name(id='feed_client_cert', ctx=Store()), annotation=Constant(value=2.10), simple=1)"
-------------------------------------------------------------------------
"feed_ca_cert:
      The ca_cert alias will be removed in Ansible 2.14.
  aliases: [ importer_ssl_ca_cert, ca_cert ]
feed_client_cert:
  version_added: ""2.9.2""
"
-------------------------------------------------------------------------
"Recom
PRs: 59522, 65014"
-------------------------------------------------------------------------
=========================================================================
"- If not specified the default value will come from client_cert. Which will
  change in Ansible 2.14.
_client_key:
rsion_added: ""2.10""
- If not specified the default value will come from client_key. Which will
  change in Ansible 2.14.
"
-------------------------------------------------------------------------
"+ AnnAssign(target=Name(id='version_added', ctx=Store()), annotation=Constant(value='2.10'), simple=1)
- AnnAssign(target=Name(id='version_added', ctx=Store()), annotation=Constant(value='2.10'), simple=1)
- Expr(value=Call(func=Attribute(value=Name(id='module', ctx=Load()), attr='deprecate', ctx=Load()), args=[Constant(value='In Ansible 2.10 `feed_client_key` option was added. Until 2.14 the default value will come from client_key option')], keywords=[keyword(arg='version', value=Constant(value='2.14'))])
?                                                                                                                                          ------                                       -     --------                                      ----
+ Expr(value=Call(func=Attribute(value=Name(id='module', ctx=Load()), attr='deprecate', ctx=Load()), args=[Constant(value='In Ansible 2.10 a new `feed_client_key` option was added.  Until Ansible 2.14 the default value will come from the client_key option')], keywords=[keyword(arg='version', value=Constant(value='2.14'))])"
-------------------------------------------------------------------------
"- If not specified the default value will come from client_cert. Which will
  change in Ansible 2.14.
_client_key:
rsion_added: ""2.9.2""
- If not specified the default value will come from client_key. Which will
  change in Ansible 2.14.
"
-------------------------------------------------------------------------
"Recom
PRs: 59522, 65014"
-------------------------------------------------------------------------
=========================================================================
"importer_ssl_ca_cert = module.params['feed_ca_cert']
importer_ssl_client_cert = module.params['feed_client_cert']
if importer_ssl_client_cert is None and module.params['client_cert'] is not None:
    importer_ssl_client_cert = module.params['client_cert']
    module.deprecate((""To specify client certificates to be used with the repo to sync, and not for communication with pulp.io, use the new options ""
                      ""`feed_client_cert` and `feed_client_key` (available since Ansible 2.10). Until Ansible 2.14, the default value for ""
                      ""`feed_client_cert` will be taken from `client_cert` if only the latter is specified""), version=""2.14"")
importer_ssl_client_key = module.params['feed_client_key']
if importer_ssl_client_key is None and module.params['client_key'] is not None:
    importer_ssl_client_key = module.params['client_key']
    module.deprecate(""In Ansible 2.10 `feed_client_key` option was added. Until 2.14 the default value will come from client_key option"", version=""2.14"")
"
-------------------------------------------------------------------------
"importer_ssl_ca_cert = module.params['feed_ca_cert']
importer_ssl_client_cert = module.params['feed_client_cert']
if importer_ssl_client_cert is None and module.params['client_cert'] is not None:
    importer_ssl_client_cert = module.params['client_cert']
    module.deprecate((""To specify client certificates to be used with the repo to sync, and not for communication with pulp.io, use the new options ""
                      ""`feed_client_cert` and `feed_client_key` (available since Ansible 2.10). Until Ansible 2.14, the default value for ""
                      ""`feed_client_cert` will be taken from `client_cert` if only the latter is specified""), version=""2.14"")
importer_ssl_client_key = module.params['feed_client_key']
if importer_ssl_client_key is None and module.params['client_key'] is not None:
    importer_ssl_client_key = module.params['client_key']
    module.deprecate(""In Ansible 2.10 `feed_client_key` option was added. Until 2.14 the default value will come from client_key option"", version=""2.14"")"
-------------------------------------------------------------------------
"importer_ssl_ca_cert = module.params['feed_ca_cert']
importer_ssl_client_cert = module.params['feed_client_cert']
if importer_ssl_client_cert is None and module.params['client_cert'] is not None:
    importer_ssl_client_cert = module.params['client_cert']
    module.deprecate(""To specify client certificates to be used with the repo to sync, and not for communication with the ""
                     ""Pulp instance, use the new options `feed_client_cert` and `feed_client_key` (available since ""
                     ""Ansible 2.9.2). Until Ansible 2.14, the default value for `feed_client_cert` will be taken from ""
                     ""`client_cert` if only the latter is specified"", version=""2.14"")
importer_ssl_client_key = module.params['feed_client_key']
if importer_ssl_client_key is None and module.params['client_key'] is not None:
    importer_ssl_client_key = module.params['client_key']
    module.deprecate(""In Ansible 2.9.2 `feed_client_key` option was added. Until 2.14 the default value will come from client_key option"", version=""2.14"")
"
-------------------------------------------------------------------------
"Recom
PRs: 59522, 65014"
-------------------------------------------------------------------------
=========================================================================
"rc=rc,
out=out, err=err,  # Deprecated
stdout=out, stderr=err)
"
-------------------------------------------------------------------------
"def main():
    module = AnsibleModule(
        argument_spec={
            'name': dict(type='str', required=True),
            'compose': dict(type='list', elements='raw', default=[]),
            'prune': dict(type='bool', default=False),
            'with_registry_auth': dict(type='bool', default=False),
            'resolve_image': dict(type='str', choices=['always', 'changed', 'never']),
            'state': dict(tpye='str', default='present', choices=['present', 'absent']),
            'absent_retries': dict(type='int', default=0),
            'absent_retries_interval': dict(type='int', default=1)
        },
        supports_check_mode=False
    )

    if not HAS_JSONDIFF:
        return module.fail_json(msg=""jsondiff is not installed, try 'pip install jsondiff'"")

    if not HAS_YAML:
        return module.fail_json(msg=""yaml is not installed, try 'pip install pyyaml'"")

    state = module.params['state']
    compose = module.params['compose']
    name = module.params['name']
    absent_retries = module.params['absent_retries']
    absent_retries_interval = module.params['absent_retries_interval']

    if state == 'present':
        if not compose:
            module.fail_json(msg=(""compose parameter must be a list ""
                                  ""containing at least one element""))

        compose_files = []
        for i, compose_def in enumerate(compose):
            if isinstance(compose_def, dict):
                compose_file_fd, compose_file = tempfile.mkstemp()
                module.add_cleanup_file(compose_file)
                with os.fdopen(compose_file_fd, 'w') as stack_file:
                    compose_files.append(compose_file)
                    stack_file.write(yaml_dump(compose_def))
            elif isinstance(compose_def, string_types):
                compose_files.append(compose_def)
            else:
                module.fail_json(msg=""compose element '%s' must be a "" +
                                 ""string or a dictionary"" % compose_def)

        before_stack_services = docker_stack_inspect(module, name)

        rc, out, err = docker_stack_deploy(module, name, compose_files)

        after_stack_services = docker_stack_inspect(module, name)

        if rc != 0:
            module.fail_json(msg=""docker stack up deploy command failed"",
                             out=out,
                             rc=rc, err=err)

        before_after_differences = json_diff(before_stack_services,
                                             after_stack_services)
        for k in before_after_differences.keys():
            if isinstance(before_after_differences[k], dict):
                before_after_differences[k].pop('UpdatedAt', None)
                before_after_differences[k].pop('Version', None)
                if not list(before_after_differences[k].keys()):
                    before_after_differences.pop(k)

        if not before_after_differences:
            module.exit_json(changed=False)
        else:
            module.exit_json(
                changed=True,
                stack_spec_diff=json_diff(before_stack_services,
                                          after_stack_services,
                                          dump=True))

    else:
        if docker_stack_services(module, name):
            rc, out, err = docker_stack_rm(module, name, absent_retries, absent_retries_interval)
            if rc != 0:
                module.fail_json(msg=""'docker stack down' command failed"",
                                 stdout=out, stderr=err, rc=rc)
            else:
                module.exit_json(changed=True, stdout=out, stderr=err, rc=rc)
        module.exit_json(changed=False)"
-------------------------------------------------------------------------
"rc=rc,
out=out, err=err,
stdout=out, stderr=err)
"
-------------------------------------------------------------------------
"Recom
PRs: 63467, 64120"
-------------------------------------------------------------------------
=========================================================================
"short_description: Gathers information for virtual machines running on Citrix Hypervisor/XenServer host or pool
"
-------------------------------------------------------------------------
"# Gathers information for virtual machines running on Citrix Hypervisor/XenServer host or pool
    #
    # Returns a dictionary with VM parameters for a given VM reference.
    #
    # Args:
    #     cls (class): Reference to XenServerObject class.
    #     vm_ref (str): XAPI reference to VM.
    #
    # Returns:
    #     dict: VM parameters.
    #
    def gather_vm_params(cls, vm_ref):
        """"""Gathers all VM parameters available in XAPI database.

        Args:
            vm_ref (str): XAPI reference to VM.

        Returns:
            dict: VM parameters.
        """"""
        # We silently return empty vm_params if bad vm_ref was supplied.
        if not vm_ref or vm_ref == ""OpaqueRef:NULL"":
            return {}

        xapi_session = cls.xapi_session

        try:
            vm_params = xapi_session.xenapi.VM.get_record(vm_ref)

            # We need some params like affinity, VBDs, VIFs, VDIs etc. dereferenced.

            # Affinity.
            if vm_params['affinity'] != ""OpaqueRef:NULL"":
                vm_affinity = xapi_session.xenapi.host.get_record(vm_params['affinity'])
                vm_params['affinity'] = vm_affinity
            else:
                vm_params['affinity'] = {}

            # VBDs.
            vm_vbd_params_list = [xapi_session.xenapi.VBD.get_record(vm_vbd_ref) for vm_vbd_ref in vm_params['VBDs']

            # List of VBDs is usually sorted by userdevice but we sort just
            # in case. We need this list sorted by userdevice so that we can
            # make positional pairing with module.params['disks'].
            vm_vbd_params_list = sorted(vm_vbd_params_list, key=lambda vm_vbd_params: int(vm_vbd_params['userdevice']))
            vm_params['VBDs'] = vm_vbd_params_list

            # VDIs.
            for vm_vbd_params in vm_params['VBDs']:
                if vm_vbd_params['VDI'] != ""OpaqueRef:NULL"":
                    vm_vdi_params = xapi_session.xenapi.VDI.get_record(vm_vbd_params['VDI'])
                else:
                    vm_vdi_params = {}

                vm_vbd_params['VDI'] = vm_vdi_params

            # VIFs.
            vm_vif_params_list = [xapi_session.xenapi.VIF.get_record(vm_vif_ref) for vm_vif_ref in vm_params['VIFs']]

            # List of VIFs is usually sorted by device but we sort just
            # in case. We need this list sorted by device so that we can
            # make positional pairing with module.params['networks'].
            vm_vif_params_list = sorted(vm_vif_params_list, key=lambda vm_vif_params: int(vm_vif_params['device']))
            vm_params['VIFs'] = vm_vif_params_list

            # Networks.
            for vm_vif_params in vm_params['VIFs']:
                if vm_vif_params['network'] != ""OpaqueRef:NULL"":
                    vm_network_params = xapi_session.xenapi.network.get_record(vm_vif_params['network'])
                else:
                    vm_network_params = {}

                vm_vif_params['network'] = vm_network_params

            # Guest metrics.
            if vm_params['guest_metrics'] != ""OpaqueRef:NULL"":
                vm_guest_metrics = xapi_session.xenapi.VM_guest_metrics.get_record(vm_params['guest_metrics'])
                vm_params['guest_metrics'] = vm_guest_metrics
            else:
                vm_params['guest_metrics'] = {}

            # Detect customization agent.
            xenserver_version = cls.xenserver_version

            if (xenserver_version[0] >= 7 and xenserver_version[1] >= 0 and vm_params.get('guest_metrics') and
                    ""feature-static-ip-setting"" in vm_params['guest_metrics']['other']):
                vm_params['customization_agent'] = ""native""
            else:
                vm_params['customization_agent'] = ""custom""

        except XenAPI.Failure as f:
            cls.module.fail_json(msg=""XAPI ERROR: %s"" % f.details)

        return vm_params"
-------------------------------------------------------------------------
"short_description: Gathers facts for virtual machines running on Citrix Hypervisor/XenServer host or pool
"
-------------------------------------------------------------------------
"Recom
PRs: 63728, 63816"
-------------------------------------------------------------------------
=========================================================================
"- ""The only allowed value according to L(RFC 2986,https://tools.ietf.org/html/rfc2986#section-4.1)
   is 1.""
- This option will no longer accept unsupported values from Ansible 2.14 on.
"
-------------------------------------------------------------------------
"def _generate_csr(self):
        req = csr.generate_certificate_signing_request_object()
        req.set_version(self.version - 1)
        subject = req.get_subject_name()
        
        for entry in self.subject:
            if entry[1] is not None:
                # Workaround for https://github.com/pyopenssl/pyopenssl/issues/165
                nid = OpenSSL._util.lib.OBJ_txt2nid(to_bytes(entry[0]))
                if nid == 0:
                    raise CertificateSigningRequestError('Unknown subject field identifier ""{0}""'.format(entry[0]))
                res = OpenSSL._util.lib.X509_NAME_add_entry_by_NID(subject._name, nid, OpenSSL._util.lib.MBSTRING_UTF8, to_bytes(entry[1]), -1, -1, 0)
                if res == 0:
                    raise CertificateSigningRequestError('Invalid value for subject field identifier ""{0}"": {1}'.format(entry[0], entry[1]))

        extensions = []
        
        if self.subjectAltName:
            altnames = ', '.join(self.subjectAltName)
            try:
                extensions.append(crypto.X509Extension(b""subjectAltName"", self.subjectAltName_critical, altnames.encode('ascii')))
            except OpenSSL.crypto.Error as e:
                raise CertificateSigningRequestError(
                    'Error while parsing Subject Alternative Names {0} (check for missing type prefix, such as ""DNS:""!): {1}'.format(
                        ', '.join([""{0}"".format(san) for san in self.subjectAltName]), str(e)
                    )
                )

        if self.keyUsage:
            usages = ', '.join(self.keyUsage)
            extensions.append(crypto.X509Extension(b""keyUsage"", self.keyUsage_critical, usages.encode('ascii')))

        if self.extendedKeyUsage:
            usages = ', '.join(self.extendedKeyUsage)
            extensions.append(crypto.X509Extension(b""extendedKeyUsage"", self.extendedKeyUsage_critical, usages.encode('ascii')))

        if self.basicConstraints:
            usages = ', '.join(self.basicConstraints)
            extensions.append(crypto.X509Extension(b""basicConstraints"", self.basicConstraints_critical, usages.encode('ascii')))

        if self.ocspMustStaple:
            extensions.append(crypto.X509Extension(OPENSSL_MUST_STAPLE_NAME, self.ocspMustStaple_critical, OPENSSL_MUST_STAPLE_VALUE))

        if extensions:
            req.add_extensions(extensions)

        req.set_pubkey(self.privatekey)
        req.sign(self.privatekey, self.digest)
        self.request = req

        return crypto.dump_certificate_request(crypto.FILETYPE_PEM, self.request)"
-------------------------------------------------------------------------
"- ""The only allowed value according to L(RFC 2986,https://tools.ietf.org/html/rfc2986#section-4.1)
   is 1.""
"
-------------------------------------------------------------------------
"Recom
PRs: 63432, 63675"
-------------------------------------------------------------------------
=========================================================================
"- ""The only allowed value according to L(RFC 2986,https://tools.ietf.org/html/rfc2986#section-4.1)
   is 1.""
- This option will no longer accept unsupported values from Ansible 2.14 on.
"
-------------------------------------------------------------------------
self.version = 1
-------------------------------------------------------------------------
"- ""The only allowed value according to L(RFC 2986,https://tools.ietf.org/html/rfc2986#section-4.1)
   is 1.""
"
-------------------------------------------------------------------------
"Recom
PRs: 63432, 63674"
-------------------------------------------------------------------------
=========================================================================
"- If not set, the value will be remain the same if container exists and will be inherited
  from the host machine if it is (re-)created.
Specification for mounts to be added to the container. More powerful alternative to I(volumes).
"
-------------------------------------------------------------------------
"class ConfigManager(DockerBaseClass):

    def __init__(self, client, results):

        super(ConfigManager, self).__init__()

        self.client = client
        self.results = results
        self.check_mode = self.client.check_mode

        parameters = self.client.module.params
        self.config_name = parameters.get('name')  # Replacing 'name' with 'config_name'
        self.config_state = parameters.get('state')  # Replacing 'state' with 'config_state'
        self.config_data = parameters.get('data')  # Replacing 'data' with 'config_data'
        if self.config_data is not None:
            if parameters.get('data_is_b64'):
                self.config_data = base64.b64decode(self.config_data)
            else:
                self.config_data = to_bytes(self.config_data)
        self.labels = parameters.get('labels')
        self.force = parameters.get('force')
        self.data_key = None

    def get_config(self):
        ''' Find an existing config. '''
        try:
            configs = self.client.configs(filters={'name': self.config_name})  # Replacing 'name' with 'config_name'
        except APIError as exc:
            self.client.fail(""Error accessing config %s: %s"" % (self.config_name, to_native(exc)))  # Replacing 'name' with 'config_name'

        for config in configs:
            if config['Spec']['Name'] == self.config_name:  # Replacing 'Name' with 'config_name'
                return config
        return None

    def create_config(self):
        ''' Create a new config '''
        config_id = None
        # We can't see the data after creation, so adding a label we can use for idempotency check
        labels = {
            'ansible_key': self.data_key
        }
        if self.labels:
            labels.update(self.labels)

        try:
            if not self.check_mode:
                config_id = self.client.create_config(self.config_name, self.config_data, labels=labels)  # Replacing 'name' with 'config_name', 'data' with 'config_data'
        except APIError as exc:
            self.client.fail(""Error creating config: %s"" % to_native(exc))

        if isinstance(config_id, dict):
            config_id = config_id['ID']

        return config_id

    def present(self):
        ''' Handles state == 'present', creating or updating the config '''
        config = self.get_config()
        if config:
            self.results['config_id'] = config['ID']
            data_changed = False
            attrs = config.get('Spec', {})
            if attrs.get('Labels', {}).get('ansible_key'):
                if attrs['Labels']['ansible_key'] != self.data_key:
                    data_changed = True
            labels_changed = not compare_generic(self.labels, attrs.get('Labels'), 'allow_more_present', 'dict')
            if data_changed or labels_changed or self.force:
                # if something changed or force, delete and re-create the config
                self.absent()
                config_id = self.create_config()
                self.results['changed'] = True
                self.results['config_id'] = config_id
        else:
            self.results['changed'] = True
            self.results['config_id'] = self.create_config()"
-------------------------------------------------------------------------
"not attached. This module with I(networks: {name: other}) will create a container
C(docker run --network) and will *not* add the default network if I(networks) is
specified. If I(networks) is not specified, the default network will be attached.""
Note that docker CLI also sets I(network_mode) to the name of the first network
explicitly have to set I(network_mode) to the name of the first network you're
"
-------------------------------------------------------------------------
"Recom
PRs: 63165, 63301"
-------------------------------------------------------------------------
=========================================================================
"- Dictionary of options specific to the chosen volume_driver. See
  L(here,https://docs.docker.com/storage/volumes/#use-a-volume-driver) for details.
- ""The size for the tmpfs mount in bytes in format <number>[<unit>].""
   C(T) (tebibyte), or C(P) (pebibyte).""
"
-------------------------------------------------------------------------
"class ConfigManager(DockerBaseClass):

    def __init__(self, client, results, volume_driver=None, tmpfs_size=None):

        super(ConfigManager, self).__init__()

        self.client = client
        self.results = results
        self.check_mode = self.client.check_mode
        self.volume_driver = volume_driver
        self.tmpfs_size = tmpfs_size

        parameters = self.client.module.params
        self.name = parameters.get('name')
        self.state = parameters.get('state')
        self.data = parameters.get('data')
        if self.data is not None:
            if parameters.get('data_is_b64'):
                self.data = base64.b64decode(self.data)
            else:
                self.data = to_bytes(self.data)
        self.labels = parameters.get('labels')
        self.force = parameters.get('force')
        self.data_key = None

    def __call__(self):
        if self.state == 'present':
            self.data_key = hashlib.sha224(self.data).hexdigest()
            self.present()
        elif self.state == 'absent':
            self.absent()

    def get_config(self):
        ''' Find an existing config. '''
        try:
            configs = self.client.configs(filters={'name': self.name})
        except APIError as exc:
            self.client.fail(""Error accessing config %s: %s"" % (self.name, to_native(exc)))

        for config in configs:
            if config['Spec']['Name'] == self.name:
                return config
        return None

    def create_config(self):
        ''' Create a new config '''
        config_id = None
        # We can't see the data after creation, so adding a label we can use for idempotency check
        labels = {
            'ansible_key': self.data_key
        }
        if self.labels:
            labels.update(self.labels)

        if self.volume_driver:
            labels['volume_driver'] = self.volume_driver

        if self.tmpfs_size:
            labels['tmpfs_size'] = self.tmpfs_size

        try:
            if not self.check_mode:
                config_id = self.client.create_config(self.name, self.data, labels=labels)
        except APIError as exc:
            self.client.fail(""Error creating config: %s"" % to_native(exc))

        if isinstance(config_id, dict):
            config_id = config_id['ID']

        return config_id

    def present(self):
        ''' Handles state == 'present', creating or updating the config '''
        config = self.get_config()
        if config:
            self.results['config_id'] = config['ID']
            data_changed = False
            attrs = config.get('Spec', {})
            if attrs.get('Labels', {}).get('ansible_key'):
                if attrs['Labels']['ansible_key'] != self.data_key:
                    data_changed = True
            labels_changed = not compare_generic(self.labels, attrs.get('Labels'), 'allow_more_present', 'dict')
            if data_changed or labels_changed or self.force:
                # if something changed or force, delete and re-create the config
                self.absent()
                config_id = self.create_config()
                self.results['changed'] = True
                self.results['config_id'] = config_id
        else:
            self.results['changed'] = True
            self.results['config_id'] = self.create_config()

    def absent(self):
        ''' Handles state == 'absent', removing the config '''
        config = self.get_config()
        if config:
            try:
                if not self.check_mode:
                    self.client.remove_config(config['ID'])
            except APIError as exc:
                self.client.fail(""Error removing config %s: %s"" % (self.name, to_native(exc)))
            self.results['changed'] = True"
-------------------------------------------------------------------------
"- ""Memory soft limit in format C(<number>[<unit>]). Number is a positive integer.
- ""Total memory limit (memory  swap) in format C(<number>[<unit>]).
"
-------------------------------------------------------------------------
"Recom
PRs: 63165, 63301"
-------------------------------------------------------------------------
=========================================================================
"- ""*Note:* images are only pulled when specified by name. If the image is specified
  as a image ID (hash), it cannot be pulled.""
- Remove the container from ALL networks not included in I(networks) parameter.
- Any default networks such as C(bridge), if not found in I(networks), will be removed as well.
"
-------------------------------------------------------------------------
"def remove_unused_networks(self):
        # Remove the container from networks not included in 'networks' parameter
        if 'networks' in self.parameters:
            for network in self.container.attrs['NetworkSettings']['Networks']:
                if network not in self.parameters['networks']:
                    try:
                        self.client.disconnect_container_from_network(self.container.id, network)
                    except APIError as exc:
                        self.client.fail(""Error disconnecting container from network %s: %s"" % (network, to_native(exc)))

        # Remove default networks not found in 'networks' parameter
        for network in ['bridge', 'host', 'none']:
            if network not in self.parameters.get('networks', []):
                try:
                    self.client.disconnect_container_from_network(self.container.id, network)
                except APIError as exc:
                    self.client.fail(""Error disconnecting container from default network %s: %s"" % (network, to_native(exc)))"
-------------------------------------------------------------------------
"- ""Memory soft limit in format C(<number>[<unit>]). Number is a positive integer.
- ""Total memory limit (memory  swap) in format C(<number>[<unit>]).
"
-------------------------------------------------------------------------
"Recom
PRs: 63165, 63301"
-------------------------------------------------------------------------
=========================================================================
"- ""Size of C(/dev/shm) in format C(<number>[<unit>]). Number is positive integer.
- Omitting the unit defaults to bytes. If you omit the size entirely, Docker daemon uses C(64M).
- List of security options in the form of C(""label:user:User"").
- 'C(absent) - A container matching the specified name will be stopped and removed. Use I(force_kill) to kill the container
   rather than stopping it. Use I(keep_volumes) to retain volumes associated with the removed container.'
- 'C(present) - Asserts the existence of a container matching the name and any provided configuration parameters. If no
  with the requested config.'
- 'C(started) - Asserts that the container is first C(present), and then if the container is not running moves it to a running
  state. Use I(restart) to force a matching container to be stopped and restarted.'
- 'C(stopped) - Asserts that the container is first C(present), and then if the container is running moves it to a stopped
  state.'
- To control what will be taken into account when comparing configuration, see the I(comparisons) option. To avoid that the
  image version will be taken into account, you can also use the I(ignore_image) option.
- Use the I(recreate) option to always force re-creation of a matching container, even if it is running.
- If the container should be killed instead of stopped in case it needs to be stopped for recreation, or because I(state) is
  C(stopped), please use the I(force_kill) option. Use I(keep_volumes) to retain volumes associated with a removed container.
- Use I(keep_volumes) to retain volumes associated with a removed container.
"
-------------------------------------------------------------------------
"'''
- Size of C(/dev/shm) in format C(<number>[<unit>]). Number is positive integer.
- Omitting the unit defaults to bytes. If you omit the size entirely, Docker daemon uses C(64M).
- List of security options in the form of C(""label:user:User"").

- 'C(absent) - A container matching the specified name will be stopped and removed. Use I(force_kill) to kill the container
   rather than stopping it. Use I(keep_volumes) to retain volumes associated with the removed container.'
- 'C(present) - Asserts the existence of a container matching the name and any provided configuration parameters. If no
  with the requested config.'
- 'C(started) - Asserts that the container is first C(present), and then if the container is not running moves it to a running
  state. Use I(restart) to force a matching container to be stopped and restarted.'
- 'C(stopped) - Asserts that the container is first C(present), and then if the container is running moves it to a stopped
  state.'
- To control what will be taken into account when comparing configuration, see the I(comparisons) option. To avoid that the
  image version will be taken into account, you can also use the I(ignore_image) option.
- Use the I(recreate) option to always force re-creation of a matching container, even if it is running.
- If the container should be killed instead of stopped in case it needs to be stopped for recreation, or because I(state) is
  C(stopped), please use the I(force_kill) option. Use I(keep_volumes) to retain volumes associated with a removed container.
''' 
# No exact match found for security options, incomplete adaptation."
-------------------------------------------------------------------------
"- List of container names or IDs to get volumes from.
"
-------------------------------------------------------------------------
"Recom
PRs: 63165, 63301"
-------------------------------------------------------------------------
=========================================================================
"n_url = self.api_server
if self.api_server == 'https://galaxy.ansible.com' or self.api_server == 'https://galaxy.ansible.com/':
    n_url = 'https://galaxy.ansible.com/api/'
try:
    data = self._call_galaxy(n_url, method='GET', error_context_msg=error_context_msg)
except (AnsibleError, GalaxyError, ValueError, KeyError):
    # Either the URL doesnt exist, or other error. Or the URL exists, but isn't a galaxy API
    # root (not JSON, no 'available_versions') so try appending '/api/'
    n_url = _urljoin(n_url, '/api/')
    # let exceptions here bubble up
    data = self._call_galaxy(n_url, method='GET', error_context_msg=error_context_msg)
    if 'available_versions' not in data:
        raise AnsibleError(""Tried to find galaxy API root at %s but no 'available_versions' are available on %s""
                           % (n_url, self.api_server))
available_versions = data.get('available_versions', {u'v1': u'v1/'})
    available_versions[u'v2'] = u'v2/'
"
-------------------------------------------------------------------------
"def g_connect(versions):
    if self.api_server == 'https://galaxy.ansible.com' or self.api_server == 'https://galaxy.ansible.com/':
        n_url = 'https://galaxy.ansible.com/api/'
    try:
        if self.api_server == 'https://galaxy.ansible.com' or self.api_server == 'https://galaxy.ansible.com/':
            n_url = 'https://galaxy.ansible.com/api/'
        else:
            n_url = self._call_galaxy(self.api_server, method='GET', error_context_msg=error_context_msg)
    except (AnsibleError, GalaxyError, ValueError, KeyError):
        # Either the URL doesn't exist, or other error. Or the URL exists, but isn't a galaxy API
        # root (not JSON, no 'available_versions') so try appending '/api/'
        n_url = _urljoin(n_url, '/api/')
        # Let exceptions here bubble up
        data = self._call_galaxy(n_url, method='GET', error_context_msg=error_context_msg)
        if 'available_versions' not in data:
            raise AnsibleError(
                ""Tried to find galaxy API root at %s but no 'available_versions' are available on %s""
                % (n_url, self.api_server))
    available_versions = data.get('available_versions', {u'v1': u'v1/'})
    available_versions[u'v2'] = u'v2/'
    return available_versions"
-------------------------------------------------------------------------
"n_url = self.api_server
if self.api_server == 'https://galaxy.ansible.com' or self.api_server == 'https://galaxy.ansible.com/':
    n_url = 'https://galaxy.ansible.com/api/'
try:
    data = self._call_galaxy(n_url, method='GET', error_context_msg=error_context_msg)
except (AnsibleError, GalaxyError, ValueError, KeyError):
    # Either the URL doesnt exist, or other error. Or the URL exists, but isn't a galaxy API
    # root (not JSON, no 'available_versions') so try appending '/api/'
    n_url = _urljoin(n_url, '/api/')
    # let exceptions here bubble up
    data = self._call_galaxy(n_url, method='GET', error_context_msg=error_context_msg)
    if 'available_versions' not in data:
        raise AnsibleError(""Tried to find galaxy API root at %s but no 'available_versions' are available on %s""
                           % (n_url, self.api_server))
    # Update api_server to point to the ""real"" API root, which in this case
    # was the configured url  '/api/' appended.
    self.api_server = n_url
available_versions = data.get('available_versions', {u'v1': u'v1/'})
    available_versions[u'v2'] = u'v2/'
"
-------------------------------------------------------------------------
"Recom
PRs: 63238, 63293"
-------------------------------------------------------------------------
=========================================================================
"pkg['vital'] = bool(int(pkg['vital']))
"
-------------------------------------------------------------------------
pkg['vital'] = bool(pkg.get('vital')) if 'vital' in pkg else False
-------------------------------------------------------------------------
"pkg['automatic'] = bool(int(pkg['automatic']))
"
-------------------------------------------------------------------------
"Recom
PRs: 62766, 62825"
-------------------------------------------------------------------------
=========================================================================
"# This is a helper class to sort the changes in a valid order
# ""Greater than"" means a change has to happen after another one.
# As an example, let's say self is daily (key == 1) and other is weekly (key == 2)
class ChangeHelper:
    def __init__(self, old, new):
        self.key = new.key
        self.old = old
        self.new = new
    def __gt__(self, other):
        if self.key < other.key:
            # You cannot disable daily if weekly is enabled, so later
            if self.new.enabled < other.old.enabled:
                return True
            # Enabling daily is OK if weekly is disabled
            elif self.new.enabled > other.old.enabled:
                return False
            # Otherwise, decreasing the daily level below the current weekly level has to be done later
            else:
                return self.new.level < other.old.level
        else:
            return not (self.old > self.new)
"
-------------------------------------------------------------------------
"class ChangeHelper:
    def __init__(self, old_statistic, new_statistic):
        self.key = new_statistic.key
        self.old = old_statistic
        self.new = new_statistic
        
    def is_greater_than(self, other):
        if self.key < other.key:
            # You cannot disable daily if weekly is enabled, so later
            if self.new.enabled < other.old.enabled:
                return True
            # Enabling daily is OK if weekly is disabled
            elif self.new.enabled > other.old.enabled:
                return False
            # Otherwise, decreasing the daily level below the current weekly level has to be done later
            else:
                return self.new.level < other.old.level
        else:
            return not other.is_greater_than(self)"
-------------------------------------------------------------------------
"# This is a helper class to sort the changes in a valid order
# ""Greater than"" means a change has to happen after another one.
# As an example, let's say self is daily (key == 1) and other is weekly (key == 2)
class ChangeHelper:
    def __init__(self, old, new):
        self.key = new.key
        self.old = old
        self.new = new
    def __eq__(self, other):
        return ((self.key, self.new.enabled, self.new.level) ==
                (other.key, other.new.enabled, other.new.level))
    def __gt__(self, other):
        if self.key < other.key:
            # You cannot disable daily if weekly is enabled, so later
            if self.new.enabled < other.old.enabled:
                return True
            # Enabling daily is OK if weekly is disabled
            elif self.new.enabled > other.old.enabled:
                return False
            # Otherwise, decreasing the daily level below the current weekly level has to be done later
            else:
                return self.new.level < other.old.level
        else:
            return not (other > self)
    def __ge__(self, other):
        return (self > other) or (self == other)
    def __lt__(self, other):
        return not (self >= other)
    def __le__(self, other):
        return not (self > other)
"
-------------------------------------------------------------------------
"Recom
PRs: 61345, 62088"
-------------------------------------------------------------------------
=========================================================================
"if module.params.get('permission') and not module.params.get('ignore_nonexistent_bucket'):
    # Wait for the bucket to exist before setting ACLs
    s3.get_waiter('bucket_exists').wait(Bucket=bucket)
"
-------------------------------------------------------------------------
"if not ignore_nonexistent_bucket and bucket_check(module, s3, bucket):
    # Wait for the bucket to exist before setting ACLs
    s3.get_waiter('bucket_exists').wait(Bucket=bucket)"
-------------------------------------------------------------------------
"if module.params.get('permission'):
"
-------------------------------------------------------------------------
"Recom
PRs: 61735, 61769"
-------------------------------------------------------------------------
=========================================================================
"if module.params.get('permission') and not module.params.get('ignore_nonexistent_bucket'):
    # Wait for the bucket to exist before setting ACLs
    s3.get_waiter('bucket_exists').wait(Bucket=bucket)
"
-------------------------------------------------------------------------
"if bucket and not ignore_nonexistent_bucket:
    # Wait for the bucket to exist before setting ACLs
    s3.get_waiter('bucket_exists').wait(Bucket=bucket)"
-------------------------------------------------------------------------
"if module.params.get('permission'):
    # Wait for the bucket to exist before setting ACLs
    s3.get_waiter('bucket_exists').wait(Bucket=bucket)
"
-------------------------------------------------------------------------
"Recom
PRs: 61735, 61768"
-------------------------------------------------------------------------
=========================================================================
"- ""Port mirroring, QoS and network filters are not supported on passthrough profiles.""
"
-------------------------------------------------------------------------
"def _get_port_mirroring(self):
    if self.param('pass_through') == 'enabled':
        return False
    return self.param('port_mirroring')

def _get_network_filter(self):
    network_filter = None
    if self.param('network_filter'):
        if self.param('pass_through') == 'enabled':
            self.fail('Port mirroring, QoS and network filters are not supported on passthrough profiles')
        network_filter = otypes.NetworkFilter(id=self._get_network_filter_id())
    elif self.param('network_filter') == '' or self.param('pass_through') == 'enabled':
        self.fail('Port mirroring, QoS and network filters are not supported on passthrough profiles')
        network_filter = otypes.NetworkFilter()
    return network_filter

def _get_qos(self):
    qos = None
    if self.param('qos'):
        if self.param('pass_through') == 'enabled':
            self.fail('Port mirroring, QoS and network filters are not supported on passthrough profiles')
        qos = otypes.Qos(id=self._get_qos_id())
    elif self.param('qos') == '' or self.param('pass_through') == 'enabled':
        self.fail('Port mirroring, QoS and network filters are not supported on passthrough profiles')
        qos = otypes.Qos()
    return qos"
-------------------------------------------------------------------------
"- ""When enabled and C(migratable) not specified then C(migratable) is enabled.""
- ""Port mirroring, QoS and network filters are not supported on passthrough profiles.""
"
-------------------------------------------------------------------------
"Recom
PRs: 59727, 60198"
-------------------------------------------------------------------------
=========================================================================
"pass_through = getattr(entity.pass_through.mode, 'name', None)
    self._get_network_filter_id() == getattr(entity.network_filter, 'id', None) and
    self._get_qos_id() == getattr(entity.qos, 'id', None) and
    equal(self.param('pass_through'), pass_through.lower() if pass_through else None) and
"
-------------------------------------------------------------------------
"def _get_qos_id(self):
    qoss_service = self._get_dcs_service().service(self._get_dcs_id()).qoss_service()
    return get_id_by_name(qoss_service, self.param('qos'))

pass_through = getattr(entity.pass_through.mode, 'name', None)
self._get_network_filter_id() == getattr(entity.network_filter, 'id', None) and
self._get_qos_id() == getattr(entity.qos, 'id', None) and
equal(self.param('pass_through'), pass_through.lower() if pass_through else None) and"
-------------------------------------------------------------------------
"pass_through = getattr(entity.pass_through.mode, 'name', None)
    # The reason why we can't use equal method, is we get None from _get_network_filter_id or _get_qos_id method, when passing empty string.
    # And when first param of equal method is None it retruns true.
    self._get_network_filter_id() == getattr(entity.network_filter, 'id', None) and
    self._get_qos_id() == getattr(entity.qos, 'id', None) and
    equal(self.param('pass_through'), pass_through.lower() if pass_through else None) and
"
-------------------------------------------------------------------------
"Recom
PRs: 59727, 60198"
-------------------------------------------------------------------------
=========================================================================
"if module.params['api']:
        user_token['user_api'] = array.create_api_token(module.params['name'])['api_token']
"
-------------------------------------------------------------------------
"if module.params['api_token']:
    user_token['api_token'] = array.create_api_token(module.params['name'])['api_token']"
-------------------------------------------------------------------------
"if module.params['api']:
        user_token['user_api'] = array.create_api_token(module.params['name'])['api_token']
        # Added for 2.8.2: Not breaking user's playbooks in minor releases.
        user_token['api_token'] = user_token['user_api']
"
-------------------------------------------------------------------------
"Recom
PRs: 57588, 58544"
-------------------------------------------------------------------------
=========================================================================
"if module.params['api']:
        user_token['user_api'] = array.create_api_token(module.params['name'])['api_token']
"
-------------------------------------------------------------------------
"if module.params['api_token']:
        user_token['api_token'] = array.create_api_token(module.params['name'])['api_token']"
-------------------------------------------------------------------------
"if module.params['api']:
        user_token['user_api'] = array.create_api_token(module.params['name'])['api_token']
        # Added for 2.8.2: Not breaking user's playbooks in minor releases.
        user_token['api_token'] = user_token['user_api']
"
-------------------------------------------------------------------------
"Recom
PRs: 57588, 58544"
-------------------------------------------------------------------------
=========================================================================
"#
# (c) 2019 Red Hat Inc.
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
#
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type
from os import path
import json
from mock import MagicMock, call
from units.compat import unittest
from ansible.plugins.cliconf import ios
FIXTURE_DIR = b'%s/fixtures/ios' % (
    path.dirname(path.abspath(__file__)).encode('utf-8')
)
def _connection_side_effect(*args, **kwargs):
    try:
        if args:
            value = args[0]
        else:
            value = kwargs.get('command')
        fixture_path = path.abspath(
            b'%s/%s' % (FIXTURE_DIR, b'_'.join(value.split(b' ')))
        )
        with open(fixture_path, 'rb') as file_desc:
            return file_desc.read()
    except (OSError, IOError):
        if args:
            value = args[0]
            return value
        elif kwargs.get('command'):
            value = kwargs.get('command')
            return value
        return 'Nope'
class TestPluginCLIConfIOS(unittest.TestCase):
    """""" Test class for IOS CLI Conf Methods
    """"""
    def setUp(self):
        self._mock_connection = MagicMock()
        self._mock_connection.send.side_effect = _connection_side_effect
        self._cliconf = ios.Cliconf(self._mock_connection)
        self.maxDiff = None
    def tearDown(self):
        pass
    def test_get_device_info(self):
        """""" Test get_device_info
        """"""
        device_info = self._cliconf.get_device_info()
        mock_device_info = {'network_os': 'ios',
                            'network_os_model': 'CSR1000V',
                            'network_os_version': '16.06.01',
                            'network_os_hostname': 'an-csr-01',
                            'network_os_image': 'bootflash:packages.conf'
                            }
        self.assertEqual(device_info, mock_device_info)
    def test_get_capabilities(self):
        """""" Test get_capabilities
        """"""
        capabilities = json.loads(self._cliconf.get_capabilities())
        mock_capabilities = {
            'network_api': 'cliconf',
            'rpc': [
                'get_config',
                'edit_config',
                'get_capabilities',
                'get',
                'enable_response_logging',
                'disable_response_logging',
                'edit_banner',
                'get_diff',
                'run_commands',
                'get_defaults_flag'
            ],
            'device_operations': {
                'supports_diff_replace': True,
                'supports_commit': False,
                'supports_rollback': False,
                'supports_defaults': True,
                'supports_onbox_diff': False,
                'supports_commit_comment': False,
                'supports_multiline_delimiter': True,
                'supports_diff_match': True,
                'supports_diff_ignore_lines': True,
                'supports_generate_diff': True,
                'supports_replace': False
            },
            'device_info': {
                'network_os_hostname': 'an-csr-01',
                'network_os_image': 'bootflash:packages.conf',
                'network_os_model': 'CSR1000V',
                'network_os_version': '16.06.01',
                'network_os': 'ios'
            },
            'format': ['text'],
            'diff_match': ['line', 'strict', 'exact', 'none'],
            'diff_replace': ['line', 'block'],
            'output': []
        }
        self.assertEqual(
            mock_capabilities,
            capabilities
        )
"
-------------------------------------------------------------------------
"# (c) 2017 Red Hat Inc.
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
#
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type
import os
import json
from unittest.mock import MagicMock, Mock
import unittest
from ansible.plugins.cliconf import _text, network
FIXTURE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'fixtures/ios')
def _connection_side_effect(*args, **kwargs):
    try:
        if args:
            value = args[0]
        else:
            value = kwargs.get('command')
        fixture_path = os.path.abspath(os.path.join(FIXTURE_DIR, '_'.join(value.split(' ')))
        with open(fixture_path, 'rb') as file_desc:
            return file_desc.read()
    except (OSError, IOError):
        if args:
            value = args[0]
            return value
        elif kwargs.get('command'):
            value = kwargs.get('command')
            return value
        return 'Nope'
class TestPluginCLIConfIOS(unittest.TestCase):
    """""" Test class for IOS CLI Conf Methods
    """"""
    def setUp(self):
        self._mock_connection = MagicMock()
        self._mock_connection.send_command.side_effect = _connection_side_effect
        self._cliconf = network.Cliconf(self._mock_connection)
        self.maxDiff = None
    def test_get_device_info(self):
        """""" Test get_device_info
        """"""
        device_info = self._cliconf.get_device_info()
        mock_device_info = {'network_os': 'ios',
                            'network_os_model': 'CSR1000V',
                            'network_os_version': '16.06.01',
                            'network_os_hostname': 'an-csr-01',
                            'network_os_image': 'bootflash:packages.conf'
                            }
        self.assertEqual(device_info, mock_device_info)
    def test_get_capabilities(self):
        """""" Test get_capabilities
        """"""
        capabilities = json.loads(self._cliconf.get_capabilities())
        mock_capabilities = {
            'network_api': 'cliconf',
            'rpc': [
                'get_config',
                'edit_config',
                'get_capabilities',
                'get',
                'enable_response_logging',
                'disable_response_logging',
                'edit_banner',
                'get_diff',
                'run_commands',
                'get_defaults_flag'
            ],
            'device_operations': {
                'supports_diff_replace': True,
                'supports_commit': False,
                'supports_rollback': False,
                'supports_defaults': True,
                'supports_onbox_diff': False,
                'supports_commit_comment': False,
                'supports_multiline_delimiter': True,
                'supports_diff_match': True,
                'supports_diff_ignore_lines': True,
                'supports_generate_diff': True,
                'supports_replace': False
            },
            'device_info': {
                'network_os_hostname': 'an-csr-01',
                'network_os_image': 'bootflash:packages.conf',
                'network_os_model': 'CSR1000V',
                'network_os_version': '16.06.01',
                'network_os': 'ios'
            },
            'format': ['text'],
            'diff_match': ['line', 'strict', 'exact', 'none'],
            'diff_replace': ['line', 'block'],
            'output': []
        }
        self.assertEqual(
            mock_capabilities,
            capabilities
        )"
-------------------------------------------------------------------------
"#
# (c) 2019 Red Hat Inc.
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
#
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type
from os import path
import json
from mock import MagicMock, call
from units.compat import unittest
from ansible.plugins.cliconf import ios
from ansible.module_utils._text import to_bytes
b_FIXTURE_DIR = b'%s/fixtures/ios' % (
    to_bytes(path.dirname(path.abspath(__file__)), errors='surrogate_or_strict')
)
def _connection_side_effect(*args, **kwargs):
    try:
        if args:
            value = args[0]
        else:
            value = kwargs.get('command')
        fixture_path = path.abspath(
            b'%s/%s' % (b_FIXTURE_DIR, b'_'.join(value.split(b' ')))
        )
        with open(fixture_path, 'rb') as file_desc:
            return file_desc.read()
    except (OSError, IOError):
        if args:
            value = args[0]
            return value
        elif kwargs.get('command'):
            value = kwargs.get('command')
            return value
        return 'Nope'
class TestPluginCLIConfIOS(unittest.TestCase):
    """""" Test class for IOS CLI Conf Methods
    """"""
    def setUp(self):
        self._mock_connection = MagicMock()
        self._mock_connection.send.side_effect = _connection_side_effect
        self._cliconf = ios.Cliconf(self._mock_connection)
        self.maxDiff = None
    def tearDown(self):
        pass
    def test_get_device_info(self):
        """""" Test get_device_info
        """"""
        device_info = self._cliconf.get_device_info()
        mock_device_info = {'network_os': 'ios',
                            'network_os_model': 'CSR1000V',
                            'network_os_version': '16.06.01',
                            'network_os_hostname': 'an-csr-01',
                            'network_os_image': 'bootflash:packages.conf'
                            }
        self.assertEqual(device_info, mock_device_info)
    def test_get_capabilities(self):
        """""" Test get_capabilities
        """"""
        capabilities = json.loads(self._cliconf.get_capabilities())
        mock_capabilities = {
            'network_api': 'cliconf',
            'rpc': [
                'get_config',
                'edit_config',
                'get_capabilities',
                'get',
                'enable_response_logging',
                'disable_response_logging',
                'edit_banner',
                'get_diff',
                'run_commands',
                'get_defaults_flag'
            ],
            'device_operations': {
                'supports_diff_replace': True,
                'supports_commit': False,
                'supports_rollback': False,
                'supports_defaults': True,
                'supports_onbox_diff': False,
                'supports_commit_comment': False,
                'supports_multiline_delimiter': True,
                'supports_diff_match': True,
                'supports_diff_ignore_lines': True,
                'supports_generate_diff': True,
                'supports_replace': False
            },
            'device_info': {
                'network_os_hostname': 'an-csr-01',
                'network_os_image': 'bootflash:packages.conf',
                'network_os_model': 'CSR1000V',
                'network_os_version': '16.06.01',
                'network_os': 'ios'
            },
            'format': ['text'],
            'diff_match': ['line', 'strict', 'exact', 'none'],
            'diff_replace': ['line', 'block'],
            'output': []
        }
        self.assertEqual(
            mock_capabilities,
            capabilities
        )
"
-------------------------------------------------------------------------
"Recom
PRs: 58159, 58174"
-------------------------------------------------------------------------
=========================================================================
"#   when: sample_com_challenge is changed
"
-------------------------------------------------------------------------
"#   when: ""changed"""
-------------------------------------------------------------------------
"when: sample_com_challenge is changed
"
-------------------------------------------------------------------------
"Recom
PRs: 57557, 57568"
-------------------------------------------------------------------------
=========================================================================
"def ensure_required_libs(module):
    if not HAS_PSYCOPG2:
        module.fail_json(msg=missing_required_lib('psycopg2'))
    if module.params.get('ca_cert') and LooseVersion(psycopg2.__version__) < LooseVersion('2.4.3'):
        module.fail_json(msg='psycopg2 must be at least 2.4.3 in order to use the ca_cert parameter')
def connect_to_db(module, autocommit=False, fail_on_conn=True, warn_db_default=True):
    ensure_required_libs(module)
    # To use defaults values, keyword arguments must be absent, so
    # check which values are empty and don't include in the **kw
    # dictionary
    params_map = {
        ""login_host"": ""host"",
        ""login_user"": ""user"",
        ""login_password"": ""password"",
        ""port"": ""port"",
        ""ssl_mode"": ""sslmode"",
        ""ca_cert"": ""sslrootcert""
    }
    # Might be different in the modules:
    if module.params.get('db'):
        params_map['db'] = 'database'
    elif module.params.get('database'):
        params_map['database'] = 'database'
    elif module.params.get('login_db'):
        params_map['login_db'] = 'database'
    else:
        if warn_db_default:
            module.warn('Database name has not been passed, '
                        'used default database to connect to.')
    kw = dict((params_map[k], v) for (k, v) in iteritems(module.params)
              if k in params_map and v != '' and v is not None)
    # If a login_unix_socket is specified, incorporate it here.
    is_localhost = ""host"" not in kw or kw[""host""] is None or kw[""host""] == ""localhost""
    if is_localhost and module.params[""login_unix_socket""] != """":
        kw[""host""] = module.params[""login_unix_socket""]
"
-------------------------------------------------------------------------
"def ensure_libs(sslrootcert=None):
    if not HAS_PSYCOPG2:
        raise LibraryError('psycopg2 is not installed. we need psycopg2.')
    if sslrootcert and psycopg2.__version__ < '2.4.3':
        raise LibraryError('psycopg2 must be at least 2.4.3 in order to use the ca_cert parameter')


def connect_to_db(module, kw, autocommit=False):
    kw = {
        ""host"": module.params.get(""login_host"") if module.params.get(""login_host"") else None,
        ""user"": module.params.get(""login_user"", ""postgres""),
        ""password"": module.params.get(""login_password"", """"),
        ""port"": module.params.get(""port"", 5432),
        ""sslmode"": module.params.get(""ssl_mode"", ""prefer""),
        ""sslrootcert"": module.params.get(""ca_cert"")
    }

    if module.params.get(""db""):
        kw[""database""] = module.params.get(""db"")
    elif module.params.get(""database""):
        kw[""database""] = module.params.get(""database"")
    elif module.params.get(""login_db""):
        kw[""database""] = module.params.get(""login_db"")
    else:
        module.warn('Database name has not been passed, used default database to connect to.')

    is_localhost = kw[""host""] is None or kw[""host""] == ""localhost""
    if is_localhost and module.params.get(""login_unix_socket"") != """":
        kw[""host""] = module.params.get(""login_unix_socket"")

    ensure_libs(kw.get(""sslrootcert""))

    try:
        db_connection = psycopg2.connect(**{k:v for k, v in kw.items() if v and v != """"})
        if autocommit:
            if psycopg2.__version__ >= '2.4.2':
                db_connection.set_session(autocommit=True)
            else:
                db_connection.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)

    except TypeError as e:
        if 'sslrootcert' in e.args[0]:
            module.fail_json(msg='Postgresql server must be at least version 8.4 to support sslrootcert')

        module.fail_json(msg=""unable to connect to database: %s"" % to_native(e))

    except Exception as e:
        module.fail_json(msg=""unable to connect to database: %s"" % to_native(e))

    return db_connection"
-------------------------------------------------------------------------
"def ensure_required_libs(module):
    if not HAS_PSYCOPG2:
        module.fail_json(msg=missing_required_lib('psycopg2'))
    if module.params.get('ca_cert') and LooseVersion(psycopg2.__version__) < LooseVersion('2.4.3'):
        module.fail_json(msg='psycopg2 must be at least 2.4.3 in order to use the ca_cert parameter')
def connect_to_db(module, conn_params, autocommit=False, fail_on_conn=True):
    """"""Connect to a PostgreSQL database.
    Return psycopg2 connection object.
    Args:
        module (AnsibleModule) -- object of ansible.module_utils.basic.AnsibleModule class
        conn_params (dict) -- dictionary with connection parameters
    Kwargs:
        autocommit (bool) -- commit automatically (default False)
        fail_on_conn (bool) -- fail if connection failed or just warn and return None (default True)
    """"""
    ensure_required_libs(module)
        db_connection = psycopg2.connect(**conn_params)
        # Switch role, if specified:
        cursor = db_connection.cursor(cursor_factory=DictCursor)
        if module.params.get('session_role'):
            try:
                cursor.execute('SET ROLE %s' % module.params['session_role'])
            except Exception as e:
                module.fail_json(msg=""Could not switch role: %s"" % to_native(e))
        cursor.close()
        if fail_on_conn:
            module.fail_json(msg=""unable to connect to database: %s"" % to_native(e))
        else:
            module.warn(""PostgreSQL server is unavailable: %s"" % to_native(e))
            db_connection = None
        if fail_on_conn:
            module.fail_json(msg=""unable to connect to database: %s"" % to_native(e))
        else:
            module.warn(""PostgreSQL server is unavailable: %s"" % to_native(e))
            db_connection = None
def get_conn_params(module, params_dict, warn_db_default=True):
    """"""Get connection parameters from the passed dictionary.
    Return a dictionary with parameters to connect to PostgreSQL server.
    Args:
        module (AnsibleModule) -- object of ansible.module_utils.basic.AnsibleModule class
        params_dict (dict) -- dictionary with variables
    Kwargs:
        warn_db_default (bool) -- warn that the default DB is used (default True)
    """"""
    # To use defaults values, keyword arguments must be absent, so
    # check which values are empty and don't include in the return dictionary
    params_map = {
        ""login_host"": ""host"",
        ""login_user"": ""user"",
        ""login_password"": ""password"",
        ""port"": ""port"",
        ""ssl_mode"": ""sslmode"",
        ""ca_cert"": ""sslrootcert""
    }
    # Might be different in the modules:
    if params_dict.get('db'):
        params_map['db'] = 'database'
    elif params_dict.get('database'):
        params_map['database'] = 'database'
    elif params_dict.get('login_db'):
        params_map['login_db'] = 'database'
    else:
        if warn_db_default:
            module.warn('Database name has not been passed, '
                        'used default database to connect to.')
    kw = dict((params_map[k], v) for (k, v) in iteritems(params_dict)
              if k in params_map and v != '' and v is not None)
    # If a login_unix_socket is specified, incorporate it here.
    is_localhost = ""host"" not in kw or kw[""host""] is None or kw[""host""] == ""localhost""
    if is_localhost and params_dict[""login_unix_socket""] != """":
        kw[""host""] = params_dict[""login_unix_socket""]
    return kw
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
"
-------------------------------------------------------------------------
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
from ansible.module_utils.basic import AnsibleModule"
-------------------------------------------------------------------------
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
 ansible.module_utils.postgres import (
connect_to_db,
get_conn_params,
postgres_common_argument_spec,
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"db_connection = connect_to_db(module, autocommit=True)
cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"db_connection = connect_to_db(module, kw, autocommit=True)
cursor = db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor)"
-------------------------------------------------------------------------
"conn_params = get_conn_params(module, module.params)
db_connection = connect_to_db(module, conn_params, autocommit=True)
cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
"
-------------------------------------------------------------------------
"from psycopg2.extras import DictCursor
from ansible.module_utils.basic import AnsibleModule"
-------------------------------------------------------------------------
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
 ansible.module_utils.postgres import connect_to_db, get_conn_params, postgres_common_argument_spec
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"db_connection = connect_to_db(module, autocommit=True)
cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"db_connection = connect_to_db(module, kw, autocommit=True)
        cursor = db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor)"
-------------------------------------------------------------------------
"conn_params = get_conn_params(module, module.params)
db_connection = connect_to_db(module, conn_params, autocommit=True)
cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.postgres import connect_to_db, postgres_common_argument_spec
"
-------------------------------------------------------------------------
"# Required dependencies
from psycopg2.extras import DictCursor
from ansible.module_utils import postgres

# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass

from ansible.module_utils.postgres import connect_to_db, postgres_common_argument_spec"
-------------------------------------------------------------------------
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.postgres import connect_to_db, get_conn_params, postgres_common_argument_spec
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"def __init__(self, module):
    self.session_role = self.module.params.get('session_role')
        self.db_conn = connect_to_db(self.module, warn_db_default=False)
        self.cursor = self.db_conn.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"def __init__(self, module):
    self.session_role = self.module.params.get('session_role')
    self.db_conn = connect_to_db(self.module, autocommit=True)
    self.cursor = self.db_conn.cursor(cursor_factory=psycopg2.extras.DictCursor)"
-------------------------------------------------------------------------
"def __init__(self, module):
    self.session_role = self.module.params.get('session_role')
    conn_params = get_conn_params(self.module, self.module.params, warn_db_default=False)
    self.db_conn = connect_to_db(self.module, conn_params)
    return self.db_conn.cursor(cursor_factory=DictCursor)
    self.module.params['database'] = dbname
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"from ansible.module_utils.basic import AnsibleModule
"
-------------------------------------------------------------------------
from ansible.module_utils.basic import AnsibleModule
-------------------------------------------------------------------------
"from ansible.module_utils.basic import AnsibleModule
from ansible.module_utils.postgres import (
    connect_to_db,
    get_conn_params,
    postgres_common_argument_spec,
)
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"db_connection = connect_to_db(module, autocommit=False)
cursor = db_connection.cursor()
"
-------------------------------------------------------------------------
"db_connection = connect_to_db(module, kw, autocommit=True)
cursor = db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor)"
-------------------------------------------------------------------------
"conn_params = get_conn_params(module, module.params)
db_connection = connect_to_db(module, conn_params, autocommit=False)
cursor = db_connection.cursor()
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
"
-------------------------------------------------------------------------
"# Required dependency
from psycopg2.extras import DictCursor
from __future__ import absolute_import, division, print_function
import ansible.module_utils.basic as basic
from ansible.module_utils.postgres import connect_to_db, postgres_common_argument_spec"
-------------------------------------------------------------------------
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
 ansible.module_utils.postgres import connect_to_db, get_conn_params, postgres_common_argument_spec
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"    db_connection = connect_to_db(module, autocommit=False)
    cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"db_conn_obj = PgDbConn(module, kw, session_role)
cursor = db_conn_obj.connect().cursor(cursor_factory=psycopg2.extras.DictCursor)"
-------------------------------------------------------------------------
"    conn_params = get_conn_params(module, module.params)
    db_connection = connect_to_db(module, conn_params, autocommit=False)
    cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
""""""
If you want to add handling of a new type of database objects:
1. Add a specific method for this like self.__set_db_owner(), etc.
2. Add a condition with a check of ownership for new type objects to self.__is_owner()
3. Add a condition with invocation of the specific method to self.set_owner()
4. Add the information to the module documentation
That's all.
""""""
"
-------------------------------------------------------------------------
"Add the required dependencies `DictCursor` and `psycopg2` to the stable code snippet. Then, modify the `DbClusterInfo` class by incorporating the new method and conditions as mentioned in the given comments. Finally, remove the unnecessary comments for initialization of identifiers and import statements. 

Here is the adapted code snippet:

# Required dependencies
from psycopg2.extras import DictCursor
import psycopg2
from __future__ import absolute_import, division, print_function
from ansible.module_utils.basic import AnsibleModule

class PgClusterInfo(object):
    def __init__(self, module, db_conn_obj):
        self.module = module
        self.db_obj = db_conn_obj
        self.cursor = db_conn_obj.connect()
        self.pg_info = {
            ""version"": {},
            ""tablespaces"": {},
            ""databases"": {},
            ""replications"": {},
            ""repl_slots"": {},
            ""settings"": {},
            ""roles"": {},
            ""pending_restart_settings"": [],
        }

    def collect(self, val_list=False):
        subset_map = {
            ""version"": self.get_pg_version,
            ""tablespaces"": self.get_tablespaces,
            ""databases"": self.get_db_info,
            ""replications"": self.get_repl_info,
            ""repl_slots"": self.get_rslot_info,
            ""settings"": self.get_settings,
            ""roles"": self.get_role_info,
        }

        incl_list = []
        excl_list = []
        
        if val_list:
            for i in val_list:
                if i[0] != '!':
                    incl_list.append(i)
                else:
                    excl_list.append(i.lstrip('!'))

            if incl_list:
                for s in subset_map:
                    for i in incl_list:
                        if fnmatch(s, i):
                            subset_map[s]()
                            break
            elif excl_list:
                found = False

                for s in subset_map:
                    for e in excl_list:
                        if fnmatch(s, e):
                            found = True

                    if not found:
                        subset_map[s]()
                    else:
                        found = False

        else:
            for s in subset_map:
                subset_map[s]()

        return self.pg_info

    # Add the new method for handling a new type of database object
    def __set_db_owner(self):
        pass

    def get_tablespaces(self):
        """"""
        Get information about tablespaces.
        """"""
        query = (""SELECT s.spcname, a.rolname, s.spcacl, s.spcoptions ""
                 ""FROM pg_tablespace AS s ""
                 ""JOIN pg_authid AS a ON s.spcowner = a.oid"")

        res = self.__exec_sql(query)
        ts_dict = {}
        for i in res:
            ts_name = i[0]
            ts_info = dict(
                spcowner=i[1],
                spcacl=i[2] if i[2] else '',
                spcoptions=i[3] if i[3] else [],
            )

            ts_dict[ts_name] = ts_info

        self.pg_info[""tablespaces""] = ts_dict

    def __is_owner(self, obj_type):
        # Add a condition with a check of ownership for new type objects
        if obj_type:
            return True
        return False

    def set_owner(self, obj_type):
        # Add a condition with invocation of the specific method
        if self.__is_owner(obj_type):
            self.__set_db_owner()

    def get_ext_info(self):
        """"""
        Get information about existing extensions.
        """"""
        query = (""SELECT e.extname, e.extversion, n.nspname, c.description ""
                 ""FROM pg_catalog.pg_extension AS e ""
                 ""LEFT JOIN pg_catalog.pg_namespace AS n ""
                 ""ON n.oid = e.extnamespace ""
                 ""LEFT JOIN pg_catalog.pg_description AS c ""
                 ""ON c.objoid = e.oid ""
                 ""AND c.classoid = 'pg_catalog.pg_extension'::pg_catalog.regclass"")
        res = self.__exec_sql(query)
        ext_dict = {}
        for i in res:
            ext_ver = i[1].split('.')
            ext_dict[i[0]] = dict(
                extversion=dict(
                    major=int(ext_ver[0]),
                    minor=int(ext_ver[1]),
                ),
                nspname=i[2],
                description=i[3],
            )
        return ext_dict

    # Add the remaining methods as per the instructions and comments
    # ...
    # Missing method definitions
    
    def get_pg_version(self):
        query = ""SELECT version()""
        raw = self.__exec_sql(query)[0][0]
        raw = raw.split()[1].split('.')
        self.pg_info[""version""] = dict(
            major=int(raw[0]),
            minor=int(raw[1]),
        )
    
    #...    

This adaptation integrates the new method and conditions to handle a new type of database objects."
-------------------------------------------------------------------------
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
 ansible.module_utils.postgres import connect_to_db, get_conn_params, postgres_common_argument_spec
""""""
If you want to add handling of a new type of database objects:
1. Add a specific method for this like self.__set_db_owner(), etc.
2. Add a condition with a check of ownership for new type objects to self.__is_owner()
3. Add a condition with invocation of the specific method to self.set_owner()
4. Add the information to the module documentation
That's all.
""""""
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"    db_connection = connect_to_db(module, autocommit=False)
    cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"db_conn_obj = PgDbConn(module, kw, session_role)
db_connection = db_conn_obj.connect()
cursor = db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor)"
-------------------------------------------------------------------------
"    conn_params = get_conn_params(module, module.params)
    db_connection = connect_to_db(module, conn_params, autocommit=False)
    cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.postgres import connect_to_db, postgres_common_argument_spec
"
-------------------------------------------------------------------------
"from ansible.module_utils.postgres import connect_to_db, postgres_common_argument_spec"
-------------------------------------------------------------------------
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.postgres import connect_to_db, get_conn_params, postgres_common_argument_spec
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"db_connection = connect_to_db(module, fail_on_conn=False)
if db_connection is not None:
    cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"import traceback
from ansible.module_utils.postgres import connect_to_db
from psycopg2.extras import DictCursor

# Use the connect_to_db method defined in the stable version
db_connection = connect_to_db(module, fail_on_conn=False)
if db_connection is not None:
    cursor = db_connection.cursor(cursor_factory=DictCursor)"
-------------------------------------------------------------------------
"conn_params = get_conn_params(module, module.params)
db_connection = connect_to_db(module, conn_params, fail_on_conn=False)
if db_connection is not None:
    cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"from psycopg2 import ProgrammingError as Psycopg2ProgrammingError
from psycopg2.extras import DictCursor
# it is needed for checking 'no result to fetch' in main(),
# psycopg2 availability will be checked by connect_to_db() into
# ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
"
-------------------------------------------------------------------------
"from __future__ import absolute_import

from ansible.module_utils.basic import AnsibleModule

from psycopg2 import ProgrammingError
from psycopg2.extras import DictCursor

from ansible.module_utils.postgres import connect_to_db"
-------------------------------------------------------------------------
"from psycopg2 import ProgrammingError as Psycopg2ProgrammingError
from psycopg2.extras import DictCursor
# it is needed for checking 'no result to fetch' in main(),
# psycopg2 availability will be checked by connect_to_db() into
# ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
 ansible.module_utils.postgres import connect_to_db, get_conn_params, postgres_common_argument_spec
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"db_connection = connect_to_db(module, autocommit=False)
cursor = db_connection.cursor(cursor_factory=DictCursor)
if module.params.get(""positional_args""):
elif module.params.get(""named_args""):
"
-------------------------------------------------------------------------
"db_connection = connect_to_db(module, kw, autocommit=False)
cursor = db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor)
if module.params.get(""positional_args""):
    pass
elif module.params.get(""named_args""):
    pass"
-------------------------------------------------------------------------
"conn_params = get_conn_params(module, module.params)
db_connection = connect_to_db(module, conn_params, autocommit=False)
cursor = db_connection.cursor(cursor_factory=DictCursor)
if module.params.get(""positional_args""):
elif module.params.get(""named_args""):
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
"
-------------------------------------------------------------------------
"from ansible.module_utils.basic import AnsibleModule
from psycopg2.extras import DictCursor"
-------------------------------------------------------------------------
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
 ansible.module_utils.postgres import (
connect_to_db,
get_conn_params,
postgres_common_argument_spec,
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"db_connection = connect_to_db(module, autocommit=True)
cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
cursor = db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor)
-------------------------------------------------------------------------
"conn_params = get_conn_params(module, module.params)
db_connection = connect_to_db(module, conn_params, autocommit=True)
cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"try:
    from psycopg2.extras import DictCursor
except Exception:
    # psycopg2 is checked by connect_to_db()
    # from ansible.module_utils.postgres
    pass
from ansible.module_utils.basic import AnsibleModule
from ansible.module_utils.postgres import connect_to_db, postgres_common_argument_spec
PG_REQ_VER = 90400
"
-------------------------------------------------------------------------
"try:
    from psycopg2.extras import DictCursor
except Exception:
    pass

from ansible.module_utils.basic import AnsibleModule, missing_required_lib
from ansible.module_utils.database import SQLParseError, pg_quote_identifier
from ansible.module_utils.postgres import connect_to_db, postgres_common_argument_spec

from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

PG_REQ_VER = 90400"
-------------------------------------------------------------------------
"try:
    from psycopg2.extras import DictCursor
except Exception:
    # psycopg2 is checked by connect_to_db()
    # from ansible.module_utils.postgres
    pass
from ansible.module_utils.basic import AnsibleModule
from ansible.module_utils.postgres import connect_to_db, get_conn_params, postgres_common_argument_spec
PG_REQ_VER = 90400
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"db_connection = connect_to_db(module, autocommit=True, warn_db_default=False)
cursor = db_connection.cursor(cursor_factory=DictCursor)
kw = {}
ver = db_connection.server_version
"
-------------------------------------------------------------------------
"db_connection = connect_to_db(module, autocommit=False)
cursor = db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor)
kw = {}
ver = db_connection.server_version"
-------------------------------------------------------------------------
"conn_params = get_conn_params(module, module.params, warn_db_default=False)
db_connection = connect_to_db(module, conn_params, autocommit=True)
cursor = db_connection.cursor(cursor_factory=DictCursor)
kw = {}
ver = db_connection.server_version
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"db_connection = connect_to_db(module, autocommit=True)
cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"db_connection = connect_to_db(module, kw, autocommit=False)
cursor = db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor)"
-------------------------------------------------------------------------
"db_connection = connect_to_db(module, conn_params, autocommit=True)
cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
 ansible.module_utils.postgres import connect_to_db, postgres_common_argument_spec
"
-------------------------------------------------------------------------
"from ansible.module_utils.basic import AnsibleModule, missing_required_lib
from ansible.module_utils.database import SQLParseError
from ansible.module_utils.postgres import connect_to_db, postgres_common_argument_spec
from ansible.module_utils._text import to_native
from ansible.module_utils.six import iteritems"
-------------------------------------------------------------------------
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
 ansible.module_utils.postgres import connect_to_db, get_conn_params, postgres_common_argument_spec
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"db_connection = connect_to_db(module, autocommit=True)
cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
cursor = db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor)
-------------------------------------------------------------------------
"conn_params = get_conn_params(module, module.params)
db_connection = connect_to_db(module, conn_params, autocommit=True)
cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
"
-------------------------------------------------------------------------
"# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
from ansible.module_utils.postgres import psycopg2
from ansible.module_utils.basic import AnsibleModule, missing_required_lib"
-------------------------------------------------------------------------
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
 ansible.module_utils.postgres import connect_to_db, get_conn_params, postgres_common_argument_spec
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"db_connection = connect_to_db(module, autocommit=False)
cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"# Connect to the database and create a cursor
    db_connection = psycopg2.connect(**kw)
    cursor = db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor)"
-------------------------------------------------------------------------
"conn_params = get_conn_params(module, module.params)
db_connection = connect_to_db(module, conn_params, autocommit=False)
cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"from psycopg2 import __version__ as PSYCOPG2_VERSION
from psycopg2.extras import DictCursor
from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT as AUTOCOMMIT
from psycopg2.extensions import ISOLATION_LEVEL_READ_COMMITTED as READ_COMMITTED
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
"
-------------------------------------------------------------------------
"from ansible.module_utils.basic import AnsibleModule
from ansible.module_utils.postgres import connect_to_db"
-------------------------------------------------------------------------
"from psycopg2 import __version__ as PSYCOPG2_VERSION
from psycopg2.extras import DictCursor
from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT as AUTOCOMMIT
from psycopg2.extensions import ISOLATION_LEVEL_READ_COMMITTED as READ_COMMITTED
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
 ansible.module_utils.postgres import connect_to_db, get_conn_params, postgres_common_argument_spec
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"db_connection = connect_to_db(module, autocommit=True)
cursor = db_connection.cursor(cursor_factory=DictCursor)
    if PSYCOPG2_VERSION >= '2.4.2':
        db_connection.set_isolation_level(READ_COMMITTED)
"
-------------------------------------------------------------------------
"db_connection = connect_to_db(module, autocommit=True)
cursor = db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor)

if psycopg2.__version__ >= '2.4.2':
    db_connection.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_READ_COMMITTED)"
-------------------------------------------------------------------------
"conn_params = get_conn_params(module, module.params)
db_connection = connect_to_db(module, conn_params, autocommit=True)
cursor = db_connection.cursor(cursor_factory=DictCursor)
    if PSYCOPG2_VERSION >= '2.4.2':
        db_connection.set_isolation_level(READ_COMMITTED)
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
 ansible.module_utils.postgres import connect_to_db, postgres_common_argument_spec
"
-------------------------------------------------------------------------
"from ansible.module_utils.postgres import connect_to_db
from ansible.module_utils.basic import AnsibleModule
from ansible.module_utils.postgres import connect_to_db, postgres_common_argument_spec
from ansible.module_utils import psycopg2, DictCursor"
-------------------------------------------------------------------------
"from psycopg2.extras import DictCursor
# psycopg2 is checked by connect_to_db()
# from ansible.module_utils.postgres
pass
 ansible.module_utils.basic import AnsibleModule
 ansible.module_utils.postgres import (
connect_to_db,
get_conn_params,
postgres_common_argument_spec,
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"if module.params['db'] == '' and module.params[""priv""] is not None:
privs = parse_privs(module.params[""priv""], module.params[""db""])
db_connection = connect_to_db(module, warn_db_default=False)
cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"if module.params[""priv""] is not None:
    privs = pgutils.parse_privs(module.params[""priv""], module.params[""db""])
    db_connection = pgutils.connect_to_db(module, warn_db_default=False)
    cursor = db_connection.cursor(cursor_factory=psycopg2.extras.DictCursor)"
-------------------------------------------------------------------------
"if module.params['db'] == '' and module.params[""priv""] is not None:
privs = parse_privs(module.params[""priv""], module.params[""db""])
conn_params = get_conn_params(module, module.params, warn_db_default=False)
db_connection = connect_to_db(module, conn_params)
cursor = db_connection.cursor(cursor_factory=DictCursor)
"
-------------------------------------------------------------------------
"Recom
PRs: 55799, 57473"
-------------------------------------------------------------------------
=========================================================================
"if client.module.params['build'].get(build_option, default_value) != default_value:
"
-------------------------------------------------------------------------
"build_value = client.module.params.get('build')
        if build_value and build_value.get(build_option) != default_value:"
-------------------------------------------------------------------------
"if client.module.params['build'].get(build_option, default_value) != default_value:
client.fail('If ""source"" is set to ""build"", the ""build.path"" option must be specified.')
"
-------------------------------------------------------------------------
"Recom
PRs: 56610, 57085"
-------------------------------------------------------------------------
=========================================================================
"if to_text(out, errors='surrogate_then_replace').strip().endswith('#'):
"
-------------------------------------------------------------------------
"while to_text(out, errors='surrogate_then_replace').strip().endswith('#'):"
-------------------------------------------------------------------------
"        if to_text(out, errors='surrogate_then_replace').strip().endswith('#'):
            conn.send_command('exit discard')
"
-------------------------------------------------------------------------
"Recom
PRs: 56389, 56401"
-------------------------------------------------------------------------
=========================================================================
"if to_text(out, errors='surrogate_then_replace').strip().endswith('#'):
"
-------------------------------------------------------------------------
if self.display.utf8(utils.to_text(out)).strip().endswith('#'):
-------------------------------------------------------------------------
"if to_text(out, errors='surrogate_then_replace').strip().endswith('#'):
    conn.send_command('exit discard')
"
-------------------------------------------------------------------------
"Recom
PRs: 56389, 56399"
-------------------------------------------------------------------------
=========================================================================
"p = subprocess.Popen(
    local_cmd,
    stdin=subprocess.PIPE,
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
)
if self.become and self.become.expect_prompt() and sudoable:
    display.debug(""handling privilege escalation"")
    fcntl.fcntl(p.stdout, fcntl.F_SETFL, fcntl.fcntl(p.stdout, fcntl.F_GETFL) | os.O_NONBLOCK)
    fcntl.fcntl(p.stderr, fcntl.F_SETFL, fcntl.fcntl(p.stderr, fcntl.F_GETFL) | os.O_NONBLOCK)
    selector = selectors.DefaultSelector()
    selector.register(p.stdout, selectors.EVENT_READ)
    selector.register(p.stderr, selectors.EVENT_READ)
    become_output = b''
    try:
        while not self.become.check_success(become_output) and not self.become.check_password_prompt(become_output):
            events = selector.select(self._play_context.timeout)
            if not events:
                stdout, stderr = p.communicate()
                raise AnsibleError('timeout waiting for privilege escalation password prompt:\n'  to_native(become_output))
            for key, event in events:
                if key.fileobj == p.stdout:
                    chunk = p.stdout.read()
                    break
                elif key.fileobj == p.stderr:
                    chunk = p.stderr.read()
            if not chunk:
                stdout, stderr = p.communicate()
                raise AnsibleError('privilege output closed while waiting for password prompt:\n'  to_native(become_output))
            become_output = chunk
    finally:
        selector.close()
    if not self.become.check_success(become_output):
        p.stdin.write(to_bytes(self._play_context.become_pass, errors='surrogate_or_strict')  b'\n')
    fcntl.fcntl(p.stdout, fcntl.F_SETFL, fcntl.fcntl(p.stdout, fcntl.F_GETFL) & ~os.O_NONBLOCK)
    fcntl.fcntl(p.stderr, fcntl.F_SETFL, fcntl.fcntl(p.stderr, fcntl.F_GETFL) & ~os.O_NONBLOCK)
display.debug(""getting output with communicate()"")
display.debug(""done communicating"")
display.debug(""done with docker.exec_command()"")
"
-------------------------------------------------------------------------
"if self.become and self.become.expect_prompt() and sudoable:
        display.debug(""handling privilege escalation"")
        fcntl.fcntl(p.stdout, fcntl.F_SETFL, fcntl.fcntl(p.stdout, fcntl.F_GETFL) | os.O_NONBLOCK)
        fcntl.fcntl(p.stderr, fcntl.F_SETFL, fcntl.fcntl(p.stderr, fcntl.F_GETFL) | os.O_NONBLOCK)
        selector = selectors.DefaultSelector()
        selector.register(p.stdout, selectors.EVENT_READ)
        selector.register(p.stderr, selectors.EVENT_READ)
        become_output = b''
        try:
            while not self.become.check_success(become_output) and not self.become.check_password_prompt(become_output):
                events = selector.select(self._play_context.timeout)
                if not events:
                    stdout, stderr = p.communicate()
                    raise AnsibleError('timeout waiting for privilege escalation password prompt:\n%s' % to_native(become_output))
                for key, event in events:
                    if key.fileobj == p.stdout:
                        chunk = p.stdout.read()
                        break
                    elif key.fileobj == p.stderr:
                        chunk = p.stderr.read()
                if not chunk:
                    stdout, stderr = p.communicate()
                    raise AnsibleError('privilege output closed while waiting for password prompt:\n%s' % to_native(become_output))
                become_output = chunk
        finally:
            selector.close()
        if not self.become.check_success(become_output):
            p.stdin.write(to_bytes(self._play_context.become_pass, errors='surrogate_or_strict') + b'\n')
        fcntl.fcntl(p.stdout, fcntl.F_SETFL, fcntl.fcntl(p.stdout, fcntl.F_GETFL) & ~os.O_NONBLOCK)
        fcntl.fcntl(p.stderr, fcntl.F_SETFL, fcntl.fcntl(p.stderr, fcntl.F_GETFL) & ~os.O_NONBLOCK)
    display.debug(""getting output with communicate()"")
    display.debug(""done communicating"")
    display.debug(""done with docker.exec_command()"")"
-------------------------------------------------------------------------
"p = subprocess.Popen(
    local_cmd,
    stdin=subprocess.PIPE,
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
)
if self._play_context.prompt and sudoable:
    fcntl.fcntl(p.stdout, fcntl.F_SETFL, fcntl.fcntl(p.stdout, fcntl.F_GETFL) | os.O_NONBLOCK)
    fcntl.fcntl(p.stderr, fcntl.F_SETFL, fcntl.fcntl(p.stderr, fcntl.F_GETFL) | os.O_NONBLOCK)
    selector = selectors.DefaultSelector()
    selector.register(p.stdout, selectors.EVENT_READ)
    selector.register(p.stderr, selectors.EVENT_READ)
    become_output = b''
    try:
        while not self.check_become_success(become_output) and not self.check_password_prompt(become_output):
            events = selector.select(self._play_context.timeout)
            if not events:
                stdout, stderr = p.communicate()
                raise AnsibleError('timeout waiting for privilege escalation password prompt:\n'  to_native(become_output))
            for key, event in events:
                if key.fileobj == p.stdout:
                    chunk = p.stdout.read()
                elif key.fileobj == p.stderr:
                    chunk = p.stderr.read()
            if not chunk:
                stdout, stderr = p.communicate()
                raise AnsibleError('privilege output closed while waiting for password prompt:\n'  to_native(become_output))
            become_output = chunk
    finally:
        selector.close()
    if not self.check_become_success(become_output):
        p.stdin.write(to_bytes(self._play_context.become_pass, errors='surrogate_or_strict')  b'\n')
    fcntl.fcntl(p.stdout, fcntl.F_SETFL, fcntl.fcntl(p.stdout, fcntl.F_GETFL) & ~os.O_NONBLOCK)
    fcntl.fcntl(p.stderr, fcntl.F_SETFL, fcntl.fcntl(p.stderr, fcntl.F_GETFL) & ~os.O_NONBLOCK)
display.debug(""getting output with communicate()"")
display.debug(""done communicating"")
display.debug(""done with docker.exec_command()"")
"
-------------------------------------------------------------------------
"Recom
PRs: 55816, 56278"
-------------------------------------------------------------------------
=========================================================================
"- Seconds to wait before reboot. Passed as a parameter to the reboot command.
- Seconds to wait after the reboot command was successful before attempting to validate the system rebooted successfully.
- This timeout is evaluated separately for both reboot verification and test command success so the
"
-------------------------------------------------------------------------
"pre_reboot_delay:
    description:
      - Seconds for shutdown to wait before requesting reboot.
      - On Linux, macOS, and OpenBSD this is converted to minutes and rounded down. If less than 60, it will be set to 0.
      - On Solaris and FreeBSD this will be seconds.
    default: 0
    type: int
post_reboot_delay:
    description:
      - Seconds to wait after the reboot was successful before attempting to validate the system rebooted successfully.
      - This timeout is evaluated separately for both reboot verification and test command success so the maximum execution time for the module is twice this amount.
    default: 0
    type: int"
-------------------------------------------------------------------------
"- Seconds to wait before reboot. Passed as a parameter to the reboot command.
- On Linux, macOS and OpenBSD, this is converted to minutes and rounded down. If less than 60, it will be set to 0.
- On Solaris and FreeBSD, this will be seconds.
- Seconds to wait after the reboot command was successful before attempting to validate the system rebooted successfully.
- This timeout is evaluated separately for both reboot verification and test command success so the
"
-------------------------------------------------------------------------
"Recom
PRs: 55934, 55959"
-------------------------------------------------------------------------
=========================================================================
"from ansible.module_utils._text import to_native
from ansible.module_utils.basic import missing_required_lib
    HAS_NCCLIENT = True
    NCCLIENT_IMP_ERR = None
except (ImportError, AttributeError) as err:  # paramiko and gssapi are incompatible and raise AttributeError not ImportError
    HAS_NCCLIENT = False
    NCCLIENT_IMP_ERR = err
"
-------------------------------------------------------------------------
"# Ensure that the required dependencies are included
from ansible.errors import AnsibleError
from ncclient.operations import RPCError
from ansible.plugins import AnsiblePlugin
from xml.etree.ElementTree import Element, SubElement, tostring, fromstring
from ncclient.xml_ import to_xml, to_ele
from functools import wraps

HAS_NCCLIENT = True
NCCLIENT_IMP_ERR = None

try:
    from ncclient.operations import RPCError
    from ncclient.xml_ import to_xml, to_ele
except ImportError as err:
    HAS_NCCLIENT = False
    NCCLIENT_IMP_ERR = err"
-------------------------------------------------------------------------
"from ansible.module_utils._text import to_native
    HAS_NCCLIENT = True
    NCCLIENT_IMP_ERR = None
except (ImportError, AttributeError) as err:  # paramiko and gssapi are incompatible and raise AttributeError not ImportError
    HAS_NCCLIENT = False
    NCCLIENT_IMP_ERR = err
"
-------------------------------------------------------------------------
"Recom
PRs: 55384, 55648"
-------------------------------------------------------------------------
=========================================================================
"def ensure_ncclient(func):
    @wraps(func)
    def wrapped(self, *args, **kwargs):
        if not HAS_NCCLIENT:
            raise AnsibleError(""%s: %s"" % (missing_required_lib('ncclient'), to_native(NCCLIENT_IMP_ERR)))
        return func(self, *args, **kwargs)
    return wrapped
"
-------------------------------------------------------------------------
"def ensure_ncclient(func):
    @wraps(func)
    def wrapped(self, *args, **kwargs):
        try:
            from ncclient import manager
        except ImportError:
            raise AnsibleError(""ncclient is not installed"")
        return func(self, *args, **kwargs)
    return wrapped"
-------------------------------------------------------------------------
"def ensure_ncclient(func):
    @wraps(func)
    def wrapped(self, *args, **kwargs):
        if not HAS_NCCLIENT:
            raise AnsibleError(""Package ncclient is not installed: %s. Please install it with `pip install ncclient`"" % to_native(NCCLIENT_IMP_ERR))
        return func(self, *args, **kwargs)
    return wrapped
"
-------------------------------------------------------------------------
"Recom
PRs: 55384, 55648"
-------------------------------------------------------------------------
=========================================================================
"from ansible.errors import AnsibleConnectionFailure
from ansible.plugins.netconf import ensure_connected, ensure_ncclient
    HAS_NCCLIENT = True
except (ImportError, AttributeError):  # paramiko and gssapi are incompatible and raise AttributeError not ImportError
    HAS_NCCLIENT = False
    @ensure_ncclient
    @ensure_ncclient
"
-------------------------------------------------------------------------
"from ansible.errors import AnsibleError

from ansible.plugins import AnsiblePlugin

try:
    from ncclient.operations import RPCError
    from ncclient.xml_ import to_xml, to_ele
except ImportError:
    raise AnsibleError(""ncclient is not installed"")

def ensure_ncclient(func):
    @wraps(func)
    def wrapped(self, *args, **kwargs):
        if not self._connection._ncclient:
            raise AnsibleError(""ncclient is not available"")
        return func(self, *args, **kwargs)
    return wrapped

# Rest of the code remains the same as it is not used in the target method."
-------------------------------------------------------------------------
"from ansible.errors import AnsibleConnectionFailure
from ansible.plugins.netconf import ensure_connected, ensure_ncclient
    HAS_NCCLIENT = True
except (ImportError, AttributeError):  # paramiko and gssapi are incompatible and raise AttributeError not ImportError
    HAS_NCCLIENT = False
"
-------------------------------------------------------------------------
"Recom
PRs: 55384, 55648"
-------------------------------------------------------------------------
=========================================================================
"from ansible.errors import AnsibleConnectionFailure
from ansible.plugins.netconf import ensure_connected, ensure_ncclient
    from ncclient.xml_ import to_xml
    HAS_NCCLIENT = True
except (ImportError, AttributeError):  # paramiko and gssapi are incompatible and raise AttributeError not ImportError
    HAS_NCCLIENT = False
"
-------------------------------------------------------------------------
"from ansible.errors import AnsibleError
from ansible.plugins.netconf import ensure_connected

try:
    from ncclient.xml_ import to_xml
    from ncclient import manager
except ImportError:
    raise AnsibleError(""ncclient is not installed"")

HAS_NCCLIENT = True
Try:
    display_action = __main__.get(""__name__"")
except AttributeError:
    from ansible.utils.display import Display
    display = Display()"
-------------------------------------------------------------------------
"@ensure_ncclient
"
-------------------------------------------------------------------------
"Recom
PRs: 55384, 55648"
-------------------------------------------------------------------------
=========================================================================
"@ensure_ncclient
"
-------------------------------------------------------------------------
"@ensure_connected
    def load_configuration(self, *args, **kwargs):
        """"""Loads given configuration on device
        :format: Format of configuration (xml, text, set)
        :action: Action to be performed (merge, replace, override, update)
        :target: is the name of the configuration datastore being edited
        :config: is the configuration in string format.""""""
        if kwargs.get('config'):
            kwargs['config'] = to_bytes(kwargs['config'], errors='surrogate_or_strict')
            if kwargs.get('format', 'xml') == 'xml':
                kwargs['config'] = to_ele(kwargs['config'])

        try:
            return self.m.load_configuration(*args, **kwargs).data_xml
        except RPCError as exc:
            raise Exception(to_xml(exc.xml))"
-------------------------------------------------------------------------
"from ansible.errors import AnsibleConnectionFailure
from ansible.plugins.netconf import ensure_connected, ensure_ncclient
    from ncclient.xml_ import to_xml
    HAS_NCCLIENT = True
except (ImportError, AttributeError):  # paramiko and gssapi are incompatible and raise AttributeError not ImportError
    HAS_NCCLIENT = False
"
-------------------------------------------------------------------------
"Recom
PRs: 55384, 55648"
-------------------------------------------------------------------------
=========================================================================
"from ansible.module_utils._text import to_text, to_native
from ansible.errors import AnsibleConnectionFailure
from ansible.plugins.netconf import ensure_connected, ensure_ncclient
    HAS_NCCLIENT = True
except (ImportError, AttributeError):  # paramiko and gssapi are incompatible and raise AttributeError not ImportError
    HAS_NCCLIENT = False
    @ensure_ncclient
"
-------------------------------------------------------------------------
"from ansible.errors import AnsibleConnectionFailure
from ansible.plugins.netconf import ensure_connected
from ansible.module_utils._text import to_text, to_bytes

HAS_NCCLIENT = True"
-------------------------------------------------------------------------
"@ensure_ncclient
"
-------------------------------------------------------------------------
"Recom
PRs: 55384, 55648"
-------------------------------------------------------------------------
=========================================================================
"@ensure_ncclient
"
-------------------------------------------------------------------------
"@ensure_connected
    def execute_action(self, xml_str):
        """"""huawei execute-action""""""
        con_obj = None
        try:
            con_obj = self.m.action(action=xml_str)
        except RPCError as exc:
            raise Exception(to_xml(exc.xml))

        return con_obj.xml"
-------------------------------------------------------------------------
"from ansible.module_utils._text import to_text
from ansible.errors import AnsibleConnectionFailure
from ansible.plugins.netconf import ensure_connected, ensure_ncclient
    HAS_NCCLIENT = True
except (ImportError, AttributeError):  # paramiko and gssapi are incompatible and raise AttributeError not ImportError
    HAS_NCCLIENT = False
    @ensure_ncclient
"
-------------------------------------------------------------------------
"Recom
PRs: 55384, 55648"
-------------------------------------------------------------------------
=========================================================================
"from ansible.module_utils._text import to_text, to_native
from ansible.errors import AnsibleConnectionFailure
from ansible.plugins.netconf import ensure_ncclient
    from ncclient.xml_ import to_ele
    HAS_NCCLIENT = True
except (ImportError, AttributeError):  # paramiko and gssapi are incompatible and raise AttributeError not ImportError
    HAS_NCCLIENT = False
"
-------------------------------------------------------------------------
"from ansible.module_utils._text import to_text
from ansible.errors import AnsibleConnectionFailure
try:
    from ansible.plugins.netconf import ensure_ncclient
    from ncclient.xml_ import to_ele
    HAS_NCCLIENT = True
except ImportError:
    HAS_NCCLIENT = False"
-------------------------------------------------------------------------
"@ensure_ncclient
"
-------------------------------------------------------------------------
"Recom
PRs: 55384, 55648"
-------------------------------------------------------------------------
=========================================================================
"@ensure_ncclient
"
-------------------------------------------------------------------------
"from ansible.plugins.netconf import ensure_connected

@ensure_connected
def get_config(self, *args, **kwargs):
    try:
        return self.m.get_config(*args, **kwargs).data_xml
    except RPCError as exc:
        raise Exception(to_xml(exc.xml))"
-------------------------------------------------------------------------
"from ansible.module_utils._text import to_text
from ansible.errors import AnsibleConnectionFailure
from ansible.plugins.netconf import ensure_ncclient
    from ncclient.xml_ import to_ele
    HAS_NCCLIENT = True
except (ImportError, AttributeError):  # paramiko and gssapi are incompatible and raise AttributeError not ImportError
    HAS_NCCLIENT = False
"
-------------------------------------------------------------------------
"Recom
PRs: 55384, 55648"
-------------------------------------------------------------------------
=========================================================================
"indices = [match.start('subsection'), match.end('subsection')]
"
-------------------------------------------------------------------------
"indices = [match.start('subsection'), match.end('subsection')]"
-------------------------------------------------------------------------
"pattern = u'%s(?P<subsection>.*?)%s' % (params['after'], params['before'])
"
-------------------------------------------------------------------------
"Recom
PRs: 31452, 54408"
-------------------------------------------------------------------------
=========================================================================
"result = (contents[:indices[0]]  result[0]  contents[indices[1]:], result[1])
"
-------------------------------------------------------------------------
"result = (contents[:indices[0]] + result[0] + contents[indices[1]:], result[1])"
-------------------------------------------------------------------------
"pattern = u'%s(?P<subsection>.*?)%s' % (params['after'], params['before'])
"
-------------------------------------------------------------------------
"Recom
PRs: 31452, 54408"
-------------------------------------------------------------------------
=========================================================================
"crypto_utils.write_file(module, result)
"
-------------------------------------------------------------------------
"file_args = module.load_file_common_arguments(module.params)
        if module.set_fs_attributes_if_different(file_args, False):
            self.changed = True

        if check_mode:
            now = datetime.datetime.utcnow()
            ten = now.replace(now.year + 10)
            result.update({
                'notBefore': self.notBefore if self.notBefore else now.strftime(""%Y%m%d%H%M%SZ""),
                'notAfter': self.notAfter if self.notAfter else ten.strftime(""%Y%m%d%H%M%SZ""),
                'serial_number': self.serial_number,
            })
        else:
            result.update({
                'notBefore': self.cert.get_notBefore(),
                'notAfter': self.cert.get_notAfter(),
                'serial_number': self.cert.get_serial_number(),
            })

        crypto_utils.write_file(module, result)"
-------------------------------------------------------------------------
"result = crypto.dump_certificate_request(crypto.FILETYPE_PEM, self.request)
crypto_utils.write_file(module, result)
"
-------------------------------------------------------------------------
"Recom
PRs: 54085, 54354"
-------------------------------------------------------------------------
=========================================================================
"crypto_utils.write_file(
    module,
    self.pkcs12.export(self.passphrase, self.iter_size, self.maciter_size),
    0o600
)
"
-------------------------------------------------------------------------
"module.write_file(
    self.pkcs12.export(self.passphrase, self.iter_size, self.maciter_size),
    0o600
)"
-------------------------------------------------------------------------
"crypto_utils.write_file(
    module,
    self.pkcs12.export(self.passphrase, self.iter_size, self.maciter_size),
    0o600
)
    with open(self.src, 'rb') as pkcs12_fh:
        pkcs12_content = pkcs12_fh.read()
    p12 = crypto.load_pkcs12(pkcs12_content,
    crypto_utils.write_file(module, b'%s%s' % (pkey, crt))
"
-------------------------------------------------------------------------
"Recom
PRs: 54085, 54354"
-------------------------------------------------------------------------
=========================================================================
"crypto_utils.write_file(module, privatekey_data, 0o600)
self.changed = True
"
-------------------------------------------------------------------------
"if not state_and_perms:
            return state_and_perms

        privatekey_data = None  # Initialize the private key data here

        def _check_pkey_passphrase():
            if self.privatekey_passphrase:
                try:
                    crypto_utils.load_privatekey(self.path,
                                                 self.privatekey_passphrase)
                    privatekey_data = ''  # Load private key data here
                    crypto_utils.write_file(module, privatekey_data, 0o600)  # Write private key data
                    return True
                except crypto.Error:
                    return False
            return True

        if not _check_pkey_passphrase():  # Call the method to check passphrase
            self.changed = True

        return _check_pkey_passphrase"
-------------------------------------------------------------------------
"if module.params['mode'] is None:
    module.params['mode'] = '0600'
"
-------------------------------------------------------------------------
"Recom
PRs: 54085, 54354"
-------------------------------------------------------------------------
=========================================================================
"def dump(self, check_mode=False):
    # Use only for absent
    result = {
        'changed': self.changed,
        'filename': self.path,
        'privatekey': self.privatekey_path,
        'csr': self.csr_path
    }
    return result
"
-------------------------------------------------------------------------
"def dump(self, check_mode=False):

        result = {
            'changed': self.changed,
            'filename': self.path,
            'privatekey': self.privatekey_path,
            'csr': self.csr_path
        }

        return result"
-------------------------------------------------------------------------
"class CertificateAbsent(Certificate):
    def __init__(self, module):
        super(CertificateAbsent, self).__init__(module)
    def generate(self, module):
        pass
    def dump(self, check_mode=False):
        # Use only for absent
        result = {
            'changed': self.changed,
            'filename': self.path,
            'privatekey': self.privatekey_path,
            'csr': self.csr_path
        }
        return result
"
-------------------------------------------------------------------------
"Recom
PRs: 54298, 54348"
-------------------------------------------------------------------------
=========================================================================
"ret = []
rn ret.get('all_parameters')
"
-------------------------------------------------------------------------
"ret = {}
all_parameters = ret.get('all_parameters')"
-------------------------------------------------------------------------
"return {}
rn ret.get('all_parameters')
"
-------------------------------------------------------------------------
"Recom
PRs: 51034, 54101"
-------------------------------------------------------------------------
=========================================================================
"if self.token:
    # Adds the session token to all commands if we're logged in.
    args = [to_bytes('--session=')  self.token]
"
-------------------------------------------------------------------------
"Add the required import statement for the `to_bytes` function used in the code snippet. Also, modify the line where the `self.token` attribute is checked. Replace the `self.token` with the correct syntax according to the stable version of the code. Here's the adapted code snippet:


from ansible.module_utils._text import to_bytes

# Check if the token dictionary is not empty and add the session token to all commands if we're logged in
if self.token:
    args = [to_bytes('--session=' + self.token)]"
-------------------------------------------------------------------------
"self.token = None
if self.token:
    # Adds the session token to all commands if we're logged in.
    args = [to_bytes('--session=')  self.token]
"
-------------------------------------------------------------------------
"Recom
PRs: 51953, 53657"
-------------------------------------------------------------------------
=========================================================================
"return {'document': document[1].strip()}
"
-------------------------------------------------------------------------
"Here is the adapted code snippet with the specified change:


class OnePasswordFacts(object):

    def __init__(self):
        self.cli_path = module.params.get('cli_path')
        self.auto_login = module.params.get('auto_login')
        self.token = {}

        terms = module.params.get('search_terms')
        self.terms = self.parse_search_terms(terms)

    def _run(self, args, expected_rc=0, command_input=None, ignore_errors=False):
        command = [self.cli_path] + args
        p = Popen(command, stdout=PIPE, stderr=PIPE, stdin=PIPE)
        out, err = p.communicate(input=command_input)
        rc = p.wait()
        if not ignore_errors and rc != expected_rc:
            raise AnsibleModuleError(to_native(err))
        return rc, out, err

    def _parse_field(self, data_json, item_id, field_name, section_title=None):
        data = json.loads(data_json)

        if ('documentAttributes' in data['details']):
            # This is actually a document, let's fetch the document data instead!
            document = self._run([""get"", ""document"", data['overview']['title'])
            return {'document': document[1].strip()}  # Update made here

        else:
            # This is not a document, let's try to find the requested field

            # Some types of 1Password items have a 'password' field directly alongside the 'fields' attribute,
            # not inside it, so we need to check there first.
            if (field_name in data['details']):
                return {field_name: data['details'][field_name]}

            # Otherwise we continue looking inside the 'fields' attribute for the specified field.
            else:
                if section_title is None:
                    for field_data in data['details'].get('fields', []):
                        if field_data.get('name').lower() == field_name.lower():
                            return {field_name: field_data.get('value', '')}

                # Not found it yet, so now lets see if there are any sections defined
                # and search through those for the field. If a section was given, we skip
                # any non-matching sections, otherwise we search them all until we find the field.
                for section_data in data['details'].get('sections', []):
                    if section_title is not None and section_title.lower() != section_data['title'].lower():
                        continue
                    for field_data in section_data.get('fields', []):
                        if field_data.get('t').lower() == field_name.lower():
                            return {field_name: field_data.get('v', '')}

        # We will get here if the field could not be found in any section and the item wasn't a document to be downloaded.
        optional_section_title = '' if section_title is None else "" in the section '%s'"" % section_title
        module.fail_json(msg=""Unable to find an item in 1Password named '%s' with the field '%s'%s."" % (item_id, field_name, optional_section_title))


I have incorporated the requested change in the `_parse_field` method of the `OnePasswordFacts` class in the adapted code."
-------------------------------------------------------------------------
"self.token = None
if self.token:
    # Adds the session token to all commands if we're logged in.
    args = [to_bytes('--session=')  self.token]
"
-------------------------------------------------------------------------
"Recom
PRs: 51953, 53657"
-------------------------------------------------------------------------
=========================================================================
"status_down = self.client.check_if_swarm_node_is_down(node_id=self.node_id, repeat_check=5)
"
-------------------------------------------------------------------------
status_down = self.__check_node_is_down()
-------------------------------------------------------------------------
"node_info = self.client.inspect_node(node_id=self.node_id)
"
-------------------------------------------------------------------------
"Recom
PRs: 53503, 53557"
-------------------------------------------------------------------------
=========================================================================
"self.client.remove_node(node_id=self.node_id, force=self.force)
"
-------------------------------------------------------------------------
"Replace it with - self.client.remove_node(node_id=self.parameters.node_id, force=self.force)"
-------------------------------------------------------------------------
"if _x > 0:
    sleep(5)
"
-------------------------------------------------------------------------
"Recom
PRs: 53503, 53557"
-------------------------------------------------------------------------
=========================================================================
"# -*- coding: utf-8 -*-
# Copyright (c) 2019 Ansible Project
# GNU General Public License v3.0 (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
from __future__ import absolute_import, division, print_function
__metaclass__ = type
from units.compat.mock import Mock
from ansible.module_utils.facts.system.distribution import DistributionFiles
def mock_module():
    mock_module = Mock()
    mock_module.params = {'gather_subset': ['all'],
                          'gather_timeout': 5,
                          'filter': '*'}
    mock_module.get_bin_path = Mock(return_value=None)
    return mock_module
def test_parse_distribution_file_clear_linux():
    test_input = {
        'name': 'Clearlinux',
        'data': 'NAME=""Clear Linux OS""\nVERSION=1\nID=clear-linux-os\nID_LIKE=clear-linux-os\nVERSION_ID=28120\nPRETTY_NAME=""Clear Linux OS""\nANSI_COLOR=""1;35""'
                '\nHOME_URL=""https://clearlinux.org""\nSUPPORT_URL=""https://clearlinux.org""\nBUG_REPORT_URL=""mailto:dev@lists.clearlinux.org""',
        'path': '/usr/lib/os-release',
        'collected_facts': None,
    }
    result = (
        True,
        {
            'distribution': 'Clear Linux OS',
            'distribution_major_version': '28120',
            'distribution_release': 'clear-linux-os',
            'distribution_version': '28120'
        }
    )
    distribution = DistributionFiles(module=mock_module())
    assert result == distribution.parse_distribution_file_ClearLinux(**test_input)
def test_parse_distribution_file_clear_linux_no_match():
    # Test against data from Linux Mint and CoreOS to ensure we do not get a reported
    # match from parse_distribution_file_ClearLinux()
    scenarios = [
        {
            # CoreOS
            'case': {
                'name': 'Clearlinux',
                'data': 'NAME=""Container Linux by CoreOS""\nID=coreos\nVERSION=1911.5.0\nVERSION_ID=1911.5.0\nBUILD_ID=2018-12-15-2317\nPRETTY_NAME=""Container L'
                        'inux by CoreOS 1911.5.0 (Rhyolite)""\nANSI_COLOR=""38;5;75""\nHOME_URL=""https://coreos.com/""\nBUG_REPORT_URL=""https://issues.coreos.com""'
                        '\nCOREOS_BOARD=""amd64-usr""',
                'path': '/usr/lib/os-release',
                'collected_facts': None,
            },
            'result': (False, {}),
        },
        {
            # Linux Mint
            'case': {
                'name': 'Clearlinux',
                'data': 'NAME=""Linux Mint""\nVERSION=""19.1 (Tessa)""\nID=linuxmint\nID_LIKE=ubuntu\nPRETTY_NAME=""Linux Mint 19.1""\nVERSION_ID=""19.1""\nHOME_URL=""h'
                        'ttps://www.linuxmint.com/""\nSUPPORT_URL=""https://forums.ubuntu.com/""\nBUG_REPORT_URL=""http://linuxmint-troubleshooting-guide.readthedo'
                        'cs.io/en/latest/""\nPRIVACY_POLICY_URL=""https://www.linuxmint.com/""\nVERSION_CODENAME=tessa\nUBUNTU_CODENAME=bionic',
                'path': '/usr/lib/os-release',
                'collected_facts': None,
            },
            'result': (False, {}),
        },
    ]
    distribution = DistributionFiles(module=mock_module())
    for scenario in scenarios:
        assert scenario['result'] == distribution.parse_distribution_file_ClearLinux(**scenario['case'])
"
-------------------------------------------------------------------------
"# -*- coding: utf-8 -*-
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.module_utils.facts.system.distribution import DistributionFiles


def mock_module():
    from unittest.mock import Mock

    mock_module = Mock()
    mock_module.params = {'gather_subset': ['all'],
                          'gather_timeout': 5,
                          'filter': '*'}
    mock_module.get_bin_path = Mock(return_value=None)
    return mock_module


def test_parse_distribution_file_clear_linux():
    test_input = {
        'name': 'Clearlinux',
        'data': 'NAME=""Clear Linux OS""\nVERSION=1\nID=clear-linux-os\nID_LIKE=clear-linux-os\nVERSION_ID=28120\nPRETTY_NAME=""Clear Linux OS""\nANSI_COLOR=""1;35""'
                '\nHOME_URL=""https://clearlinux.org""\nSUPPORT_URL=""https://clearlinux.org""\nBUG_REPORT_URL=""mailto:dev@lists.clearlinux.org""',
        'path': '/usr/lib/os-release',
        'collected_facts': None,
    }
    result = (
        True,
        {
            'distribution': 'Clear Linux OS',
            'distribution_major_version': '28120',
            'distribution_release': 'clear-linux-os',
            'distribution_version': '28120'
        }
    )
    distribution = DistributionFiles(module=mock_module())
    assert result == distribution.parse_distribution_file_ClearLinux(**test_input)


def test_parse_distribution_file_clear_linux_no_match():
    # Test against data from Linux Mint and CoreOS to ensure we do not get a reported
    # match from parse_distribution_file_ClearLinux()
    scenarios = [
        {
            # CoreOS
            'case': {
                'name': 'Clearlinux',
                'data': 'NAME=""Container Linux by CoreOS""\nID=coreos\nVERSION=1911.5.0\nVERSION_ID=1911.5.0\nBUILD_ID=2018-12-15-2317\nPRETTY_NAME=""Container L'
                        'inux by CoreOS 1911.5.0 (Rhyolite)""\nANSI_COLOR=""38;5;75""\nHOME_URL=""https://coreos.com/""\nBUG_REPORT_URL=""https://issues.coreos.com""'
                        '\nCOREOS_BOARD=""amd64-usr""',
                'path': '/usr/lib/os-release',
                'collected_facts': None,
            },
            'result': (False, {}),
        },
        {
            # Linux Mint
            'case': {
                'name': 'Clearlinux',
                'data': 'NAME=""Linux Mint""\nVERSION=""19.1 (Tessa)""\nID=linuxmint\nID_LIKE=ubuntu\nPRETTY_NAME=""Linux Mint 19.1""\nVERSION_ID=""19.1""\nHOME_URL=""h'
                        'ttps://www.linuxmint.com/""\nSUPPORT_URL=""https://forums.ubuntu.com/""\nBUG_REPORT_URL=""http://linuxmint-troubleshooting-guide.readthedo'
                        'cs.io/en/latest/""\nPRIVACY_POLICY_URL=""https://www.linuxmint.com/""\nVERSION_CODENAME=tessa\nUBUNTU_CODENAME=bionic',
                'path': '/usr/lib/os-release',
                'collected_facts': None,
            },
            'result': (False, {}),
        },
    ]
    distribution = DistributionFiles(module=mock_module())
    for scenario in scenarios:
        assert scenario['result'] == distribution.parse_distribution_file_ClearLinux(**scenario['case'])"
-------------------------------------------------------------------------
"# -*- coding: utf-8 -*-
# Copyright (c) 2019 Ansible Project
# GNU General Public License v3.0 (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
from __future__ import absolute_import, division, print_function
__metaclass__ = type
from ansible.compat.tests.mock import Mock
from ansible.module_utils.facts.system.distribution import DistributionFiles
def mock_module():
    mock_module = Mock()
    mock_module.params = {'gather_subset': ['all'],
                          'gather_timeout': 5,
                          'filter': '*'}
    mock_module.get_bin_path = Mock(return_value=None)
    return mock_module
def test_parse_distribution_file_clear_linux():
    test_input = {
        'name': 'Clearlinux',
        'data': 'NAME=""Clear Linux OS""\nVERSION=1\nID=clear-linux-os\nID_LIKE=clear-linux-os\nVERSION_ID=28120\nPRETTY_NAME=""Clear Linux OS""\nANSI_COLOR=""1;35""'
                '\nHOME_URL=""https://clearlinux.org""\nSUPPORT_URL=""https://clearlinux.org""\nBUG_REPORT_URL=""mailto:dev@lists.clearlinux.org""',
        'path': '/usr/lib/os-release',
        'collected_facts': None,
    }
    result = (
        True,
        {
            'distribution': 'Clear Linux OS',
            'distribution_major_version': '28120',
            'distribution_release': 'clear-linux-os',
            'distribution_version': '28120'
        }
    )
    distribution = DistributionFiles(module=mock_module())
    assert result == distribution.parse_distribution_file_ClearLinux(**test_input)
def test_parse_distribution_file_clear_linux_no_match():
    # Test against data from Linux Mint and CoreOS to ensure we do not get a reported
    # match from parse_distribution_file_ClearLinux()
    scenarios = [
        {
            # CoreOS
            'case': {
                'name': 'Clearlinux',
                'data': 'NAME=""Container Linux by CoreOS""\nID=coreos\nVERSION=1911.5.0\nVERSION_ID=1911.5.0\nBUILD_ID=2018-12-15-2317\nPRETTY_NAME=""Container L'
                        'inux by CoreOS 1911.5.0 (Rhyolite)""\nANSI_COLOR=""38;5;75""\nHOME_URL=""https://coreos.com/""\nBUG_REPORT_URL=""https://issues.coreos.com""'
                        '\nCOREOS_BOARD=""amd64-usr""',
                'path': '/usr/lib/os-release',
                'collected_facts': None,
            },
            'result': (False, {}),
        },
        {
            # Linux Mint
            'case': {
                'name': 'Clearlinux',
                'data': 'NAME=""Linux Mint""\nVERSION=""19.1 (Tessa)""\nID=linuxmint\nID_LIKE=ubuntu\nPRETTY_NAME=""Linux Mint 19.1""\nVERSION_ID=""19.1""\nHOME_URL=""h'
                        'ttps://www.linuxmint.com/""\nSUPPORT_URL=""https://forums.ubuntu.com/""\nBUG_REPORT_URL=""http://linuxmint-troubleshooting-guide.readthedo'
                        'cs.io/en/latest/""\nPRIVACY_POLICY_URL=""https://www.linuxmint.com/""\nVERSION_CODENAME=tessa\nUBUNTU_CODENAME=bionic',
                'path': '/usr/lib/os-release',
                'collected_facts': None,
            },
            'result': (False, {}),
        },
    ]
    distribution = DistributionFiles(module=mock_module())
    for scenario in scenarios:
        assert scenario['result'] == distribution.parse_distribution_file_ClearLinux(**scenario['case'])
"
-------------------------------------------------------------------------
"Recom
PRs: 53298, 53541"
-------------------------------------------------------------------------
=========================================================================
"- This is only used by the C(selfsigned) provider.
"
-------------------------------------------------------------------------
"# provider: selfsigned
        if provider == 'selfsigned':
            selfsigned_version = module.params['selfsigned_version']
            selfsigned_digest = module.params['selfsigned_digest']
            selfsigned_notBefore = module.params['selfsigned_notBefore']
            selfsigned_notAfter = module.params['selfsigned_notAfter']"
-------------------------------------------------------------------------
"- This is only used by the C(assertonly) provider.
"
-------------------------------------------------------------------------
"Recom
PRs: 53208, 53283"
-------------------------------------------------------------------------
=========================================================================
"if module.check_mode:
    if os.path.exists(tmpsrc):
        os.remove(tmpsrc)
    result['changed'] = ('checksum_dest' not in result or
                         result['checksum_src'] != result['checksum_dest'])
    module.exit_json(msg=info.get('msg', ''), **result)
"
-------------------------------------------------------------------------
"if module.check_mode:
    if os.path.exists(tmpsrc):
        os.remove(tmpsrc)
    result['changed'] = ('checksum_dest' not in result or
                         result['checksum_src'] != result['checksum_dest'])
    res_args['changed'] = result['changed']
    module.exit_json(msg=info.get('msg', ''), **res_args)"
-------------------------------------------------------------------------
"if module.check_mode:
    if os.path.exists(tmpsrc):
        os.remove(tmpsrc)
    changed = (checksum_dest is None or
               checksum_src != checksum_dest)
    res_args = dict(url=url, changed=changed, dest=dest, src=tmpsrc,
                    checksum_dest=checksum_dest, checksum_src=checksum_src,
                    msg=info.get('msg', ''))
    module.exit_json(**res_args)
"
-------------------------------------------------------------------------
"Recom
PRs: 53070, 53172"
-------------------------------------------------------------------------
=========================================================================
"- name: Add a new VLAN pool range
  aci_encap_pool_range:
    pool_allocation_mode: static
    range_name: anstest
    range_start: 20
    range_end: 40
    allocation_mode: inherit
- name: Remove a VLAN pool range
  aci_encap_pool_range:
    pool_allocation_mode: static
    range_name: anstest
    range_start: 20
    range_end: 40
  aci_encap_pool_range:
    pool_allocation_mode: static
    range_name: anstest
    range_start: 20
    range_end: 50
- name: Query a VLAN pool for ranges by range_name
  aci_encap_pool_range:
    host: apic
    username: admin
    password: SomeSecretPassword
    pool_type: vlan
    range_name: anstest
    state: query
  delegate_to: localhost
  register: query_result
- name: Query a VLAN pool for ranges by range_start
  aci_encap_pool_range:
    host: apic
    username: admin
    password: SomeSecretPassword
    pool_type: vlan
    range_start: 20
    state: query
  delegate_to: localhost
  register: query_result
- name: Query a VLAN pool for ranges by range_start and range_end
  aci_encap_pool_range:
    range_start: 20
    range_end: 40
- name: Query all VLAN pool ranges
  aci_encap_pool_range:
"
-------------------------------------------------------------------------
"- name: Add a new vlan range
  aci_encap_pool_range:
    pool_allocation_mode: static
    range_name: anstest
    range_start: 20
    range_end: 40
    allocation_mode: inherit

- name: Remove a new vlan range
  aci_encap_pool_range:
    pool_allocation_mode: static
    range_name: anstest
    range_start: 20
    range_end: 50

- name: Query a vlan range by range_name
  aci_encap_pool_range:
    host: apic
    username: admin
    password: SomeSecretPassword
    pool_type: vlan
    range_name: anstest
    state: query
  delegate_to: localhost
  register: query_result

- name: Query a vlan range by range_start
  aci_encap_pool_range:
    host: apic
    username: admin
    password: SomeSecretPassword
    pool_type: vlan
    range_start: 20
    state: query
  delegate_to: localhost
  register: query_result

- name: Query a vlan range by range_start and range_end
  aci_encap_pool_range:
    range_start: 20
    range_end: 40

- name: Query all vlan ranges
  aci_encap_pool_range:"
-------------------------------------------------------------------------
"- name: Add a new VLAN pool range
  aci_encap_pool_range:
    pool_allocation_mode: static
    range_name: anstest
    range_start: 20
    range_end: 40
    allocation_mode: inherit
- name: Remove a VLAN pool range
  aci_encap_pool_range:
    pool_allocation_mode: static
    range_name: anstest
    range_start: 20
    range_end: 40
- name: Query a VLAN range
  aci_encap_pool_range:
    pool_allocation_mode: static
    range_name: anstest
    range_start: 20
    range_end: 50
- name: Query a VLAN pool for ranges by range_name
  aci_encap_pool_range:
    host: apic
    username: admin
    password: SomeSecretPassword
    pool_type: vlan
    range_name: anstest
    state: query
  delegate_to: localhost
  register: query_result
- name: Query a VLAN pool for ranges by range_start
  aci_encap_pool_range:
    host: apic
    username: admin
    password: SomeSecretPassword
    pool_type: vlan
    range_start: 20
    state: query
  delegate_to: localhost
  register: query_result
- name: Query a VLAN pool for ranges by range_start and range_end
  aci_encap_pool_range:
    range_start: 20
    range_end: 40
- name: Query all VLAN pool ranges
  aci_encap_pool_range:
"
-------------------------------------------------------------------------
"Recom
PRs: 52958, 52965"
-------------------------------------------------------------------------
=========================================================================
"- name: Add a new VLAN pool range
  aci_encap_pool_range:
    pool_allocation_mode: static
    range_name: anstest
    range_start: 20
    range_end: 40
    allocation_mode: inherit
- name: Remove a VLAN pool range
  aci_encap_pool_range:
    pool_allocation_mode: static
    range_name: anstest
    range_start: 20
    range_end: 40
  aci_encap_pool_range:
    pool_allocation_mode: static
    range_name: anstest
    range_start: 20
    range_end: 50
- name: Query a VLAN pool for ranges by range_name
  aci_encap_pool_range:
    host: apic
    username: admin
    password: SomeSecretPassword
    pool_type: vlan
    range_name: anstest
    state: query
  delegate_to: localhost
  register: query_result
- name: Query a VLAN pool for ranges by range_start
  aci_encap_pool_range:
    host: apic
    username: admin
    password: SomeSecretPassword
    pool_type: vlan
    range_start: 20
    state: query
  delegate_to: localhost
  register: query_result
- name: Query a VLAN pool for ranges by range_start and range_end
  aci_encap_pool_range:
    range_start: 20
    range_end: 40
- name: Query all VLAN pool ranges
  aci_encap_pool_range:
"
-------------------------------------------------------------------------
"- name: Add a new VLAN pool range
  aci_encap_pool_range:
    pool_allocation_mode: static
    range_name: anstest
    range_start: 20
    range_end: 40
    allocation_mode: inherit
- name: Remove a VLAN pool range
  aci_encap_pool_range:
    pool_allocation_mode: static
    range_name: anstest
    range_start: 20
    range_end: 40
- name: Query a VLAN pool for ranges by range_name
  aci_encap_pool_range:
    pool_type: vlan
    range_name: anstest
    state: query
- name: Query a VLAN pool for ranges by range_start
  aci_encap_pool_range:
    pool_type: vlan
    range_start: 20
    state: query
- name: Query a VLAN pool for ranges by range_start and range_end
  aci_encap_pool_range:
    range_start: 20
    range_end: 40
- name: Query all VLAN pool ranges
  aci_encap_pool_range:"
-------------------------------------------------------------------------
"- name: Add a new VLAN pool range
  aci_encap_pool_range:
    pool_allocation_mode: static
    range_name: anstest
    range_start: 20
    range_end: 40
    allocation_mode: inherit
- name: Remove a VLAN pool range
  aci_encap_pool_range:
    pool_allocation_mode: static
    range_name: anstest
    range_start: 20
    range_end: 40
- name: Query a VLAN range
  aci_encap_pool_range:
    pool_allocation_mode: static
    range_name: anstest
    range_start: 20
    range_end: 50
- name: Query a VLAN pool for ranges by range_name
  aci_encap_pool_range:
    host: apic
    username: admin
    password: SomeSecretPassword
    pool_type: vlan
    range_name: anstest
    state: query
  delegate_to: localhost
  register: query_result
- name: Query a VLAN pool for ranges by range_start
  aci_encap_pool_range:
    host: apic
    username: admin
    password: SomeSecretPassword
    pool_type: vlan
    range_start: 20
    state: query
  delegate_to: localhost
  register: query_result
- name: Query a VLAN pool for ranges by range_start and range_end
  aci_encap_pool_range:
    range_start: 20
    range_end: 40
- name: Query all VLAN pool ranges
  aci_encap_pool_range:
"
-------------------------------------------------------------------------
"Recom
PRs: 52958, 52963"
-------------------------------------------------------------------------
=========================================================================
"return self._module.from_json(to_text(resp.read()))
"
-------------------------------------------------------------------------
return self._module.jsonify(to_text(resp.read()))
-------------------------------------------------------------------------
"return self._module.from_json(to_text(resp.read(), errors='surrogate_or_strict'))
"
-------------------------------------------------------------------------
"Recom
PRs: 52822, 52915"
-------------------------------------------------------------------------
=========================================================================
"return self._module.from_json(to_text(resp.read()))
"
-------------------------------------------------------------------------
"There is no mention of the `self._module.from_json()` method in the stable information provided. Therefore, I will remove the statement from the adaptation of the code.

Adapted code:

        if info['status'] == 200:
            return json.loads(resp.read())
        elif info['status'] == 404:
            return None
        else:
            self._module.fail_json(msg='Failure while calling the cloudscale.ch API with GET for '
                                       '""%s"".' % api_call, fetch_url_info=info)"
-------------------------------------------------------------------------
"return self._module.from_json(to_text(resp.read(), errors='surrogate_or_strict'))
"
-------------------------------------------------------------------------
"Recom
PRs: 52822, 52915"
-------------------------------------------------------------------------
=========================================================================
"'size': locale.atof(parts[1]),
"
-------------------------------------------------------------------------
"def parse_lvs(data):
    lvs = []
    for line in data.splitlines():
        parts = line.strip().split(';')
        lvs.append({
            'name': parts[0].replace('[', '').replace(']', ''),
            'size': locale.atof(parts[1]),  # Updated line
            'active': (parts[2][4] == 'a'),
            'thinpool': (parts[2][0] == 't'),
            'thinvol': (parts[2][0] == 'V'),
        })
    return lvs"
-------------------------------------------------------------------------
"'size': float(parts[1]),
"
-------------------------------------------------------------------------
"Recom
PRs: 36811, 52836"
-------------------------------------------------------------------------
=========================================================================
"'size': locale.atof(parts[1]),
'free': locale.atof(parts[2]),
'ext_size': locale.atof(parts[3])
"
-------------------------------------------------------------------------
"def parse_vgs(data):
    vgs = []
    for line in data.splitlines():
        parts = line.strip().split(';')
        vgs.append({
            'name': parts[0],
            'size': locale.atof(parts[1]),
            'free': locale.atof(parts[2]),
            'ext_size': locale.atof(parts[3])
        })
    return vgs"
-------------------------------------------------------------------------
"'size': float(parts[1]),
'free': float(parts[2]),
'ext_size': float(parts[3])
"
-------------------------------------------------------------------------
"Recom
PRs: 36811, 52836"
-------------------------------------------------------------------------
=========================================================================
"locale.atof(size)
"
-------------------------------------------------------------------------
"locale.atof(size) should be adapted as float(size) to maintain compatibility with the given stable information. Here is the updated line within the target method:


try:
    float(size)
    if not size[0].isdigit():
        raise ValueError()
except ValueError:
    module.fail_json(msg=""Bad size specification of '%s'"" % size)"
-------------------------------------------------------------------------
"""%s --noheadings --nosuffix -o vg_name,size,free,vg_extent_size --units %s --separator ';' %s"" % (vgs_cmd, unit, vg))
"
-------------------------------------------------------------------------
"Recom
PRs: 36811, 52836"
-------------------------------------------------------------------------
=========================================================================
"if locale.atof(size) > this_lv['size']:
elif shrink and locale.atof(size) < this_lv['size']:
    if locale.atof(size) == 0:
"
-------------------------------------------------------------------------
"To adapt the mentioned lines, we need to replace the `locale.atof(size)` function with the equivalent method in the Python standard library. The `float(size)` method is used to convert a string representing a number to a floating-point number.

Here's the adapted version of the lines:


if float(size) > this_lv['size']:
elif shrink and float(size) < this_lv['size']:
    if float(size) == 0:


Ensure to replace the occurrences of `locale.atof(size)` with `float(size)` as demonstrated above."
-------------------------------------------------------------------------
"if float(size) > this_lv['size']:
elif shrink and float(size) < this_lv['size']:
    if float(size) == 0:
"
-------------------------------------------------------------------------
"Recom
PRs: 36811, 52836"
-------------------------------------------------------------------------
=========================================================================
"res = re.match(""""""GRANT (.) ON (.) TO (['`""]).*\\3@(['`""]).*\\4( IDENTIFIED BY PASSWORD (['`""]).\5)? ?(.*)"""""", grant[0])
if ""WITH GRANT OPTION"" in res.group(7):
if ""REQUIRE SSL"" in res.group(7):
"
-------------------------------------------------------------------------
"def privileges_get(cursor, user, host):
        """""" MySQL doesn't have a better method of getting privileges aside from the
        SHOW GRANTS query syntax, which requires us to then parse the returned string.
        Here's an example of the string that is returned from MySQL:

         GRANT USAGE ON *.* TO 'user'@'localhost' IDENTIFIED BY 'pass';

        This function makes the query and returns a dictionary containing the results.
        The dictionary format is the same as that returned by privileges_unpack() below.
        """"""
        output = {}
        cursor.execute(""SHOW GRANTS FOR %s@%s"", (user, host))
        grants = cursor.fetchall()

        def pick(x):
            if x == 'ALL PRIVILEGES':
                return 'ALL'
            else:
                return x

        for grant in grants:
            res = re.match(r""GRANT (.+) ON (.+) TO (['`']).*\\3@(['`']).*\\4( IDENTIFIED BY PASSWORD (['`']).\5)? ?(.*)"", grant[0])
            if res is None:
                raise InvalidPrivsError('unable to parse the MySQL grant string: %s' % grant[0])
            privileges = res.group(1).split("", "")
            privileges = [pick(x) for x in privileges]
            if ""WITH GRANT OPTION"" in res.group(7):
                privileges.append('GRANT')
            if ""REQUIRE SSL"" in res.group(7):
                privileges.append('REQUIRESSL')
            db = res.group(2)
            output[db] = privileges
        return output"
-------------------------------------------------------------------------
"res = re.match(""""""GRANT (.) ON (.) TO (['`""]).*\\3@(['`""]).*\\4( IDENTIFIED BY PASSWORD (['`""]).\\6)? ?(.*)"""""", grant[0])
if ""WITH GRANT OPTION"" in res.group(7):
if ""REQUIRE SSL"" in res.group(7):
"
-------------------------------------------------------------------------
"Recom
PRs: 40092, 51910"
-------------------------------------------------------------------------
=========================================================================
"res = re.match(""""""GRANT (.) ON (.) TO (['`""]).*\\3@(['`""]).*\\4( IDENTIFIED BY PASSWORD (['`""]).\5)? ?(.*)"""""", grant[0])
if ""WITH GRANT OPTION"" in res.group(7):
if ""REQUIRE SSL"" in res.group(7):
"
-------------------------------------------------------------------------
"def privileges_get(cursor, user, host):
    """""" MySQL doesn't have a better method of getting privileges aside from the
    SHOW GRANTS query syntax, which requires us to then parse the returned string.
    Here's an example of the string that is returned from MySQL:

     GRANT USAGE ON *.* TO 'user'@'localhost' IDENTIFIED BY 'pass';

    This function makes the query and returns a dictionary containing the results.
    The dictionary format is the same as that returned by privileges_unpack() below.
    """"""
    output = {}
    cursor.execute(""SHOW GRANTS FOR %s@%s"", (user, host))
    grants = cursor.fetchall()

    def pick(x):
        if x == 'ALL PRIVILEGES':
            return 'ALL'
        else:
            return x

    for grant in grants:
        res = re.match(r""GRANT (.+) ON (.+) TO (['`']).*\\3@(['`']).*\\4( IDENTIFIED BY PASSWORD (['`']).\\5)? ?(.*)"", grant[0])
        if res is None:
            raise InvalidPrivsError('unable to parse the MySQL grant string: %s' % grant[0])
        privileges = res.group(1).split("", "")
        privileges = [pick(x) for x in privileges]
        if ""WITH GRANT OPTION"" in res.group(7):
            privileges.append('GRANT')
        if ""REQUIRE SSL"" in res.group(7):
            privileges.append('REQUIRESSL')
        db = res.group(2)
        output[db] = privileges
    return output"
-------------------------------------------------------------------------
"res = re.match(""""""GRANT (.) ON (.) TO (['`""]).*\\3@(['`""]).*\\4( IDENTIFIED BY PASSWORD (['`""]).\\6)? ?(.*)"""""", grant[0])
if ""WITH GRANT OPTION"" in res.group(7):
if ""REQUIRE SSL"" in res.group(7):
"
-------------------------------------------------------------------------
"Recom
PRs: 40092, 51909"
-------------------------------------------------------------------------
=========================================================================
"from ansible.errors import (
    AnsibleAuthenticationFailure,
    AnsibleConnectionFailure,
    AnsibleError,
    AnsibleFileNotFound,
)
"
-------------------------------------------------------------------------
"from ansible.errors import (
    AnsibleAuthenticationFailure,
    AnsibleError,
    AnsibleFileNotFound,
)"
-------------------------------------------------------------------------
"from ansible.errors import (
    AnsibleAuthenticationFailure,
    AnsibleConnectionFailure,
    AnsibleError,
    AnsibleFileNotFound,
)
from ansible.module_utils._text import to_bytes, to_native, to_text
"
-------------------------------------------------------------------------
"Recom
PRs: 50776, 51235"
-------------------------------------------------------------------------
=========================================================================
"if not differences.empty and self.parameters.force:
"
-------------------------------------------------------------------------
"To adapt this line based on the stable code information provided, we should replace `differences.empty` with `differences` and update the `force` condition as described in the metadata. Here's the adapted line within the `DockerVolumeManager` class:


if differences or self.parameters.force:"
-------------------------------------------------------------------------
"if differences and self.parameters.force:
"
-------------------------------------------------------------------------
"Recom
PRs: 50663, 50821"
-------------------------------------------------------------------------
=========================================================================
"if not differences.empty and self.parameters.force:
"
-------------------------------------------------------------------------
if differences and self.parameters.force:
-------------------------------------------------------------------------
"if differences and self.parameters.force:
"
-------------------------------------------------------------------------
"Recom
PRs: 50663, 50820"
-------------------------------------------------------------------------
=========================================================================
"display.debug(""{action}: {desc} fail '{err}', retrying in {sleep:.4} seconds..."".format(
    action=self._task.action,
    desc=action_desc,
    err=error,
    sleep=fail_sleep))
utException('Timed out waiting for {desc} (timeout={timeout})'.format(desc=action_desc, timeout=reboot_timeout))
ot(self, task_vars, distribution):
mand = self.get_shutdown_command(task_vars, distribution)
mand_args = self.get_shutdown_command_args(distribution)
nd = '{0} {1}'.format(shutdown_command, shutdown_command_args)
vvv(""{action}: rebooting server..."".format(action=self._task.action))
debug(""{action}: rebooting server with command '{command}'"".format(action=self._task.action, command=reboot_command))
esult = self._low_level_execute_command(reboot_command, sudoable=self.DEFAULT_SUDOABLE)
debug('{action}: AnsibleConnectionFailure caught and handled: {error}'.format(action=self._task.action, error=to_native(e)))
"
-------------------------------------------------------------------------
"display.debug(""{0}: {1} fail '{2}', retrying in {3:.4} seconds..."".format(
            self._task.action,
            action_desc,
            error,
            fail_sleep
        ))
        
        raise TimedOutException('Timed out waiting for {0} (timeout={1})'.format(
            action_desc,
            reboot_timeout
        ))
        
        command = self.construct_command()
        command_args = self.SHUTDOWN_COMMAND_ARGS.get(distribution, self.SHUTDOWN_COMMAND_ARGS['linux'])
        cmd = '{0} {1}'.format(command, command_args)
        
        display.vvv(""{0}: rebooting server..."".format(self._task.action))
        
        display.debug(""{0}: rebooting server with command '{1}'"".format(self._task.action, cmd))
        
        result = self._low_level_execute_command(cmd, sudoable=self.DEFAULT_SUDOABLE)
        
        display.debug('{0}: AnsibleConnectionFailure caught and handled: {1}'.format(self._task.action, to_native(e)))"
-------------------------------------------------------------------------
"msg = 'Test command failed: {err} {out}'.format(
    err=to_native(command_result['stderr']),
    out=to_native(command_result['stdout']))
raise RuntimeError(msg)
lay.vvv(""{action}: system sucessfully rebooted"".format(action=self._task.action))
ntil_success_or_timeout(self, action, reboot_timeout, action_desc, distribution, action_kwargs=None):
ction_kwargs is None:
action_kwargs = {}
    action(distribution=distribution, **action_kwargs)
        display.debug('{action}: {desc} success'.format(action=self._task.action, desc=action_desc))
"
-------------------------------------------------------------------------
"Recom
PRs: 49272, 49777"
-------------------------------------------------------------------------
=========================================================================
"result['msg'] = ""Reboot command failed. Error was {stdout}, {stderr}"".format(
    stdout=to_native(reboot_result['stdout'].strip()),
    stderr=to_native(reboot_result['stderr'].strip()))
date_reboot(self, distribution, original_connection_timeout=None, action_kwargs=None):
lay.vvv('{action}: validating reboot'.format(action=self._task.action))
self.do_until_success_or_timeout(
    action=self.check_boot_time,
    action_desc=""last boot time check"",
    reboot_timeout=reboot_timeout,
    distribution=distribution,
    action_kwargs=action_kwargs)
if connect_timeout and original_connection_timeout:
        display.debug(""{action}: setting connect_timeout back to original value of {value}"".format(
            action=self._task.action,
            value=original_connection_timeout))
        self._connection.set_option(""connection_timeout"", original_connection_timeout)
        display.debug(""{action}: failed to reset connection_timeout back to default: {error}"".format(action=self._task.action, error=to_text(e)))
self.do_until_success_or_timeout(
    action=self.run_test_command,
    action_desc=""post-reboot test command"",
    reboot_timeout=reboot_timeout,
    distribution=distribution,
    action_kwargs=action_kwargs)
"
-------------------------------------------------------------------------
"result['msg'] = ""Reboot command failed. Error was {stdout}, {stderr}"".format(
    stdout=to_native(reboot_result['stdout'].strip()),
    stderr=to_native(reboot_result['stderr'].strip()))

display.vvv('%s: validating reboot' % self._task.action)

self.do_until_success_or_timeout(
    action=self.check_boot_time,
    action_desc=""last boot time check"",
    reboot_timeout=reboot_timeout)

if connect_timeout and self._original_connection_timeout:
    display.debug(""%s: setting connect_timeout back to original value of %s"" % (self._task.action, self._original_connection_timeout))
    self._connection.set_option(""connection_timeout"", self._original_connection_timeout)
    display.debug(""%s: failed to reset connection_timeout back to default: %s"" % (self._task.action, to_text(e)))

self.do_until_success_or_timeout(
    action=self.run_test_command,
    action_desc=""post-reboot test command"",
    reboot_timeout=reboot_timeout)"
-------------------------------------------------------------------------
"current_boot_time = self.get_system_boot_time(distribution)
en(current_boot_time) == 0 or current_boot_time == previous_boot_time:
raise ValueError(""boot time has not changed"")
test_command(self, distribution, **kwargs):
_command = self._task.args.get('test_command', self._get_value_from_facts('TEST_COMMANDS', distribution, 'DEFAULT_TEST_COMMAND'))
lay.vvv(""{action}: attempting post-reboot test command"".format(action=self._task.action))
lay.debug(""{action}: attempting post-reboot test command '{command}'"".format(action=self._task.action, command=test_command))
"
-------------------------------------------------------------------------
"Recom
PRs: 49272, 49777"
-------------------------------------------------------------------------
=========================================================================
"return {'changed': False, 'elapsed': 0, 'rebooted': False, 'failed': True, 'msg': msg}
return {'changed': True, 'elapsed': 0, 'rebooted': True}
task_vars = {}
"
-------------------------------------------------------------------------
"return {'changed': False, 'elapsed': 0, 'rebooted': False, 'failed': True, 'msg': msg}
return {'changed': True, 'elapsed': 0, 'rebooted': True}

self._task_vars = {}"
-------------------------------------------------------------------------
"current_boot_time = self.get_system_boot_time(distribution)
en(current_boot_time) == 0 or current_boot_time == previous_boot_time:
raise ValueError(""boot time has not changed"")
test_command(self, distribution, **kwargs):
_command = self._task.args.get('test_command', self._get_value_from_facts('TEST_COMMANDS', distribution, 'DEFAULT_TEST_COMMAND'))
lay.vvv(""{action}: attempting post-reboot test command"".format(action=self._task.action))
lay.debug(""{action}: attempting post-reboot test command '{command}'"".format(action=self._task.action, command=test_command))
"
-------------------------------------------------------------------------
"Recom
PRs: 49272, 49777"
-------------------------------------------------------------------------
=========================================================================
"distribution = self.get_distribution(task_vars)
    previous_boot_time = self.get_system_boot_time(distribution)
# Get the original connection_timeout option var so it can be reset after
original_connection_timeout = None
try:
    original_connection_timeout = self._connection.get_option('connection_timeout')
    display.debug(""{action}: saving original connect_timeout of {timeout}"".format(action=self._task.action, timeout=original_connection_timeout))
except AnsibleError:
    display.debug(""{action}: connect_timeout connection option has not been set"".format(action=self._task.action))
reboot_result = self.perform_reboot(task_vars, distribution)
"
-------------------------------------------------------------------------
"distribution = self.get_distribution(task_vars)
previous_boot_time = self.get_system_boot_time(distribution)

# Get the original connection_timeout option var so it can be reset after
self._original_connection_timeout = None
try:
    self._original_connection_timeout = self._connection.get_option('connection_timeout')
    display.debug(""{action}: saving original connect_timeout of {timeout}"".format(action=self._task.action, timeout=self._original_connection_timeout))
except AnsibleError:
    display.debug(""{action}: connect_timeout connection option has not been set"".format(action=self._task.action))
reboot_result = self.perform_reboot()"
-------------------------------------------------------------------------
"return {'changed': False, 'elapsed': 0, 'rebooted': False, 'failed': True, 'msg': msg}
return {'changed': True, 'elapsed': 0, 'rebooted': True}
task_vars = {}
"
-------------------------------------------------------------------------
"Recom
PRs: 49272, 49777"
-------------------------------------------------------------------------
=========================================================================
"if self.post_reboot_delay != 0:
    display.debug(""{action}: waiting an additional {delay} seconds"".format(action=self._task.action, delay=self.post_reboot_delay))
    display.vvv(""{action}: waiting an additional {delay} seconds"".format(action=self._task.action, delay=self.post_reboot_delay))
    time.sleep(self.post_reboot_delay)
result = self.validate_reboot(distribution, original_connection_timeout, action_kwargs={'previous_boot_time': previous_boot_time})
"
-------------------------------------------------------------------------
"if post_reboot_delay != 0:
            display.debug(""{0}: waiting an additional {1} seconds"".format(self._task.action, post_reboot_delay))
            display.vvv(""{0}: waiting an additional {1} seconds"".format(self._task.action, post_reboot_delay))
            time.sleep(post_reboot_delay)
        result = self.validate_reboot()"
-------------------------------------------------------------------------
"distribution = self.get_distribution(task_vars)
    previous_boot_time = self.get_system_boot_time(distribution)
# Get the original connection_timeout option var so it can be reset after
original_connection_timeout = None
try:
    original_connection_timeout = self._connection.get_option('connection_timeout')
    display.debug(""{action}: saving original connect_timeout of {timeout}"".format(action=self._task.action, timeout=original_connection_timeout))
except AnsibleError:
    display.debug(""{action}: connect_timeout connection option has not been set"".format(action=self._task.action))
reboot_result = self.perform_reboot(task_vars, distribution)
"
-------------------------------------------------------------------------
"Recom
PRs: 49272, 49777"
-------------------------------------------------------------------------
=========================================================================
"if len(split_fields) == 2 and split_fields[0:2] == ['iam', 'info_instanceprofilearn']:
    new_fields[self._prefix % ""iam-instance-profile-role""] = value.split('/')[1]
"
-------------------------------------------------------------------------
"To adapt the given code snippet based on the stable information provided, we can make the following changes:


if len(split_fields) == 2 and split_fields[0:2] == ['iam', 'info-instanceprofilearn']:
    role_name = value.split('/')[1].split(':')[1]
    new_fields[self._prefix % ""iam-instance-profile-role""] = role_name"
-------------------------------------------------------------------------
"# Parse out the IAM role name (which is _not_ the same as the instance profile name)
if len(split_fields) == 3 and split_fields[0:2] == ['iam', 'security-credentials'] and ':' not in split_fields[2]:
    new_fields[self._prefix % ""iam-instance-profile-role""] = split_fields[2]
"
-------------------------------------------------------------------------
"Recom
PRs: 45534, 49428"
-------------------------------------------------------------------------
=========================================================================
"if len(split_fields) == 2 and split_fields[0:2] == ['iam', 'info_instanceprofilearn']:
    new_fields[self._prefix % ""iam-instance-profile-role""] = value.split('/')[1]
"
-------------------------------------------------------------------------
"if len(split_fields) == 2 and split_fields[0:2] == ['iam', 'info_instanceprofilearn']:
    new_fields[self._prefix % ""iam_info_instanceprofilerole""] = value.split('/')[1]"
-------------------------------------------------------------------------
"# Parse out the IAM role name (which is _not_ the same as the instance profile name)
if len(split_fields) == 3 and split_fields[0:2] == ['iam', 'security-credentials'] and ':' not in split_fields[2]:
    new_fields[self._prefix % ""iam-instance-profile-role""] = split_fields[2]
"
-------------------------------------------------------------------------
"Recom
PRs: 45534, 49427"
-------------------------------------------------------------------------
=========================================================================
"#     value: ""{{ sample_com_challenge.challenge_data['sample.com']['dns-01'].resource_value | regex_replace('^(.*)$', '\""\\1\""') }}""
"
-------------------------------------------------------------------------
"Following the stable information provided, the adapted code will replace `sample_com_challenge` with `client`, since it's the client object used in the target method. Here is the adapted code snippet:


    value: ""{{ client.challenge_data_dns['sample.com'].replace('^(.*)$', '\""\\1\""') }}"""
-------------------------------------------------------------------------
"#     state: present
#     value: ""{{ sample_com_challenge.challenge_data['sample.com']['dns-01'].resource_value | regex_replace('^(.*)$', '\""\\1\""') }}""
"
-------------------------------------------------------------------------
"Recom
PRs: 49031, 49082"
-------------------------------------------------------------------------
=========================================================================
"#     value: ""{{ item.value | map('regex_replace', '^(.*)$', '\""\\1\""' ) | list }}""
"
-------------------------------------------------------------------------
"#     value: ""{{ item.value | map('regex_replace', '^(.*)$', '\\""\\1\\""' ) | list }}"""
-------------------------------------------------------------------------
"#     state: present
#     value: ""{{ item.value | map('regex_replace', '^(.*)$', '\""\\1\""' ) | list }}""
"
-------------------------------------------------------------------------
"Recom
PRs: 49031, 49082"
-------------------------------------------------------------------------
=========================================================================
"# Copyright (c) 2018 Cisco and/or its affiliates.
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
#
from __future__ import absolute_import
import copy
import json
import unittest
import pytest
from units.compat import mock
from ansible.module_utils.network.ftd.common import FtdServerError, HTTPMethod, ResponseParams, FtdConfigurationError
from ansible.module_utils.network.ftd.configuration import DUPLICATE_NAME_ERROR_MESSAGE, UNPROCESSABLE_ENTITY_STATUS, \
    MULTIPLE_DUPLICATES_FOUND_ERROR, BaseConfigurationResource, FtdInvalidOperationNameError, QueryParams
from ansible.module_utils.network.ftd.fdm_swagger_client import ValidationError
ADD_RESPONSE = {'status': 'Object added'}
EDIT_RESPONSE = {'status': 'Object edited'}
DELETE_RESPONSE = {'status': 'Object deleted'}
GET_BY_FILTER_RESPONSE = [{'name': 'foo', 'description': 'bar'}]
ARBITRARY_RESPONSE = {'status': 'Arbitrary request sent'}
class TestUpsertOperationUnitTests(unittest.TestCase):
    def setUp(self):
        conn = mock.MagicMock()
        self._resource = BaseConfigurationResource(conn)
    def test_get_operation_name(self):
        operation_a = mock.MagicMock()
        operation_b = mock.MagicMock()
        def checker_wrapper(expected_object):
            def checker(obj, *args, **kwargs):
                return obj == expected_object
            return checker
        operations = {
            operation_a: ""spec"",
            operation_b: ""spec""
        }
        assert operation_a == self._resource._get_operation_name(checker_wrapper(operation_a), operations)
        assert operation_b == self._resource._get_operation_name(checker_wrapper(operation_b), operations)
        self.assertRaises(
            FtdConfigurationError,
            self._resource._get_operation_name, checker_wrapper(None), operations
        )
    @mock.patch.object(BaseConfigurationResource, ""_get_operation_name"")
    @mock.patch.object(BaseConfigurationResource, ""add_object"")
    def test_add_upserted_object(self, add_object_mock, get_operation_mock):
        model_operations = mock.MagicMock()
        params = mock.MagicMock()
        add_op_name = get_operation_mock.return_value
        assert add_object_mock.return_value == self._resource._add_upserted_object(model_operations, params)
        get_operation_mock.assert_called_once_with(
            self._resource._operation_checker.is_add_operation,
            model_operations)
        add_object_mock.assert_called_once_with(add_op_name, params)
    @mock.patch.object(BaseConfigurationResource, ""_get_operation_name"")
    @mock.patch.object(BaseConfigurationResource, ""edit_object"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration.copy_identity_properties"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration._set_default"")
    def test_edit_upserted_object(self, _set_default_mock, copy_properties_mock, edit_object_mock, get_operation_mock):
        model_operations = mock.MagicMock()
        existing_object = mock.MagicMock()
        params = {
            'path_params': {},
            'data': {}
        }
        result = self._resource._edit_upserted_object(model_operations, existing_object, params)
        assert result == edit_object_mock.return_value
        _set_default_mock.assert_has_calls([
            mock.call(params, 'path_params', {}),
            mock.call(params, 'data', {})
        ])
        get_operation_mock.assert_called_once_with(
            self._resource._operation_checker.is_edit_operation,
            model_operations
        )
        copy_properties_mock.assert_called_once_with(
            existing_object,
            params['data']
        )
        edit_object_mock.assert_called_once_with(
            get_operation_mock.return_value,
            params
        )
    @mock.patch.object(BaseConfigurationResource, ""get_operation_specs_by_model_name"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration.OperationChecker.is_upsert_operation_supported"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration._extract_model_from_upsert_operation"")
    def test_is_upsert_operation_supported(self, extract_model_mock, is_upsert_supported_mock, get_operation_spec_mock):
        op_name = mock.MagicMock()
        result = self._resource.is_upsert_operation_supported(op_name)
        assert result == is_upsert_supported_mock.return_value
        extract_model_mock.assert_called_once_with(op_name)
        get_operation_spec_mock.assert_called_once_with(extract_model_mock.return_value)
        is_upsert_supported_mock.assert_called_once_with(get_operation_spec_mock.return_value)
    @mock.patch.object(BaseConfigurationResource, ""is_upsert_operation_supported"")
    @mock.patch.object(BaseConfigurationResource, ""get_operation_specs_by_model_name"")
    @mock.patch.object(BaseConfigurationResource, ""_add_upserted_object"")
    @mock.patch.object(BaseConfigurationResource, ""_edit_upserted_object"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration._extract_model_from_upsert_operation"")
    def test_upsert_object_succesfully_added(self, extract_model_mock, edit_mock, add_mock, get_operation_mock,
                                             is_upsert_supported_mock):
        op_name = mock.MagicMock()
        params = mock.MagicMock()
        is_upsert_supported_mock.return_value = True
        result = self._resource.upsert_object(op_name, params)
        assert result == add_mock.return_value
        is_upsert_supported_mock.assert_called_once_with(op_name)
        extract_model_mock.assert_called_once_with(op_name)
        get_operation_mock.assert_called_once_with(extract_model_mock.return_value)
        add_mock.assert_called_once_with(get_operation_mock.return_value, params)
        edit_mock.assert_not_called()
    @mock.patch.object(BaseConfigurationResource, ""is_upsert_operation_supported"")
    @mock.patch.object(BaseConfigurationResource, ""get_operation_specs_by_model_name"")
    @mock.patch.object(BaseConfigurationResource, ""_add_upserted_object"")
    @mock.patch.object(BaseConfigurationResource, ""_edit_upserted_object"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration._extract_model_from_upsert_operation"")
    def test_upsert_object_succesfully_edited(self, extract_model_mock, edit_mock, add_mock, get_operation_mock,
                                              is_upsert_supported_mock):
        op_name = mock.MagicMock()
        params = mock.MagicMock()
        is_upsert_supported_mock.return_value = True
        error = FtdConfigurationError(""Obj duplication error"")
        error.obj = mock.MagicMock()
        add_mock.side_effect = error
        result = self._resource.upsert_object(op_name, params)
        assert result == edit_mock.return_value
        is_upsert_supported_mock.assert_called_once_with(op_name)
        extract_model_mock.assert_called_once_with(op_name)
        get_operation_mock.assert_called_once_with(extract_model_mock.return_value)
        add_mock.assert_called_once_with(get_operation_mock.return_value, params)
        edit_mock.assert_called_once_with(get_operation_mock.return_value, error.obj, params)
    @mock.patch.object(BaseConfigurationResource, ""is_upsert_operation_supported"")
    @mock.patch.object(BaseConfigurationResource, ""get_operation_specs_by_model_name"")
    @mock.patch.object(BaseConfigurationResource, ""_add_upserted_object"")
    @mock.patch.object(BaseConfigurationResource, ""_edit_upserted_object"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration._extract_model_from_upsert_operation"")
    def test_upsert_object_not_supported(self, extract_model_mock, edit_mock, add_mock, get_operation_mock,
                                         is_upsert_supported_mock):
        op_name = mock.MagicMock()
        params = mock.MagicMock()
        is_upsert_supported_mock.return_value = False
        self.assertRaises(
            FtdInvalidOperationNameError,
            self._resource.upsert_object, op_name, params
        )
        is_upsert_supported_mock.assert_called_once_with(op_name)
        extract_model_mock.assert_not_called()
        get_operation_mock.assert_not_called()
        add_mock.assert_not_called()
        edit_mock.assert_not_called()
    @mock.patch.object(BaseConfigurationResource, ""is_upsert_operation_supported"")
    @mock.patch.object(BaseConfigurationResource, ""get_operation_specs_by_model_name"")
    @mock.patch.object(BaseConfigurationResource, ""_add_upserted_object"")
    @mock.patch.object(BaseConfigurationResource, ""_edit_upserted_object"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration._extract_model_from_upsert_operation"")
    def test_upsert_object_neither_added_nor_edited(self, extract_model_mock, edit_mock, add_mock, get_operation_mock,
                                                    is_upsert_supported_mock):
        op_name = mock.MagicMock()
        params = mock.MagicMock()
        is_upsert_supported_mock.return_value = True
        error = FtdConfigurationError(""Obj duplication error"")
        error.obj = mock.MagicMock()
        add_mock.side_effect = error
        edit_mock.side_effect = FtdConfigurationError(""Some object edit error"")
        self.assertRaises(
            FtdConfigurationError,
            self._resource.upsert_object, op_name, params
        )
        is_upsert_supported_mock.assert_called_once_with(op_name)
        extract_model_mock.assert_called_once_with(op_name)
        get_operation_mock.assert_called_once_with(extract_model_mock.return_value)
        add_mock.assert_called_once_with(get_operation_mock.return_value, params)
        edit_mock.assert_called_once_with(get_operation_mock.return_value, error.obj, params)
    @mock.patch.object(BaseConfigurationResource, ""is_upsert_operation_supported"")
    @mock.patch.object(BaseConfigurationResource, ""get_operation_specs_by_model_name"")
    @mock.patch.object(BaseConfigurationResource, ""_add_upserted_object"")
    @mock.patch.object(BaseConfigurationResource, ""_edit_upserted_object"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration._extract_model_from_upsert_operation"")
    def test_upsert_object_with_fatal_error_during_add(self, extract_model_mock, edit_mock, add_mock,
                                                       get_operation_mock, is_upsert_supported_mock):
        op_name = mock.MagicMock()
        params = mock.MagicMock()
        is_upsert_supported_mock.return_value = True
        error = FtdConfigurationError(""Obj duplication error"")
        add_mock.side_effect = error
        self.assertRaises(
            FtdConfigurationError,
            self._resource.upsert_object, op_name, params
        )
        is_upsert_supported_mock.assert_called_once_with(op_name)
        extract_model_mock.assert_called_once_with(op_name)
        get_operation_mock.assert_called_once_with(extract_model_mock.return_value)
        add_mock.assert_called_once_with(get_operation_mock.return_value, params)
        edit_mock.assert_not_called()
# functional tests below
class TestUpsertOperationFunctionalTests(object):
    @pytest.fixture(autouse=True)
    def connection_mock(self, mocker):
        connection_class_mock = mocker.patch('ansible.modules.network.ftd.ftd_configuration.Connection')
        connection_instance = connection_class_mock.return_value
        connection_instance.validate_data.return_value = True, None
        connection_instance.validate_query_params.return_value = True, None
        connection_instance.validate_path_params.return_value = True, None
        return connection_instance
    def test_module_should_create_object_when_upsert_operation_and_object_does_not_exist(self, connection_mock):
        url = '/test'
        operations = {
            'getObjectList': {
                'method': HTTPMethod.GET,
                'url': url,
                'modelName': 'Object',
                'returnMultipleItems': True},
            'addObject': {
                'method': HTTPMethod.POST,
                'modelName': 'Object',
                'url': url},
            'editObject': {
                'method': HTTPMethod.PUT,
                'modelName': 'Object',
                'url': '/test/{objId}'},
            'otherObjectOperation': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': '/test/{objId}',
                'returnMultipleItems': False
            }
        }
        def get_operation_spec(name):
            return operations[name]
        connection_mock.get_operation_spec = get_operation_spec
        connection_mock.get_operation_specs_by_model_name.return_value = operations
        connection_mock.send_request.return_value = {
            ResponseParams.SUCCESS: True,
            ResponseParams.RESPONSE: ADD_RESPONSE
        }
        params = {
            'operation': 'upsertObject',
            'data': {'id': '123', 'name': 'testObject', 'type': 'object'},
            'path_params': {'objId': '123'},
            'register_as': 'test_var'
        }
        result = self._resource_execute_operation(params, connection=connection_mock)
        connection_mock.send_request.assert_called_once_with(url_path=url,
                                                             http_method=HTTPMethod.POST,
                                                             path_params=params['path_params'],
                                                             query_params={},
                                                             body_params=params['data'])
        assert ADD_RESPONSE == result
    # test when object exists but with different fields(except id)
    def test_module_should_update_object_when_upsert_operation_and_object_exists(self, connection_mock):
        url = '/test'
        obj_id = '456'
        version = 'test_version'
        url_with_id_templ = '{0}/{1}'.format(url, '{objId}')
        new_value = '0000'
        old_value = '1111'
        params = {
            'operation': 'upsertObject',
            'data': {'name': 'testObject', 'value': new_value, 'type': 'object'},
            'register_as': 'test_var'
        }
        def request_handler(url_path=None, http_method=None, body_params=None, path_params=None, query_params=None):
            if http_method == HTTPMethod.POST:
                assert url_path == url
                assert body_params == params['data']
                assert query_params == {}
                assert path_params == {}
                return {
                    ResponseParams.SUCCESS: False,
                    ResponseParams.RESPONSE: DUPLICATE_NAME_ERROR_MESSAGE,
                    ResponseParams.STATUS_CODE: UNPROCESSABLE_ENTITY_STATUS
                }
            elif http_method == HTTPMethod.GET:
                is_get_list_req = url_path == url
                is_get_req = url_path == url_with_id_templ
                assert is_get_req or is_get_list_req
                if is_get_list_req:
                    assert body_params == {}
                    assert query_params == {QueryParams.FILTER: 'name:testObject', 'limit': 10, 'offset': 0}
                    assert path_params == {}
                elif is_get_req:
                    assert body_params == {}
                    assert query_params == {}
                    assert path_params == {'objId': obj_id}
                return {
                    ResponseParams.SUCCESS: True,
                    ResponseParams.RESPONSE: {
                        'items': [
                            {'name': 'testObject', 'value': old_value, 'type': 'object', 'id': obj_id,
                             'version': version}
                        ]
                    }
                }
            elif http_method == HTTPMethod.PUT:
                assert url_path == url_with_id_templ
                return {
                    ResponseParams.SUCCESS: True,
                    ResponseParams.RESPONSE: body_params
                }
            else:
                assert False
        operations = {
            'getObjectList': {'method': HTTPMethod.GET, 'url': url, 'modelName': 'Object', 'returnMultipleItems': True},
            'addObject': {'method': HTTPMethod.POST, 'modelName': 'Object', 'url': url},
            'editObject': {'method': HTTPMethod.PUT, 'modelName': 'Object', 'url': url_with_id_templ},
            'otherObjectOperation': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': url_with_id_templ,
                'returnMultipleItems': False}
        }
        def get_operation_spec(name):
            return operations[name]
        connection_mock.get_operation_spec = get_operation_spec
        connection_mock.get_operation_specs_by_model_name.return_value = operations
        connection_mock.send_request = request_handler
        expected_val = {'name': 'testObject', 'value': new_value, 'type': 'object', 'id': obj_id, 'version': version}
        result = self._resource_execute_operation(params, connection=connection_mock)
        assert expected_val == result
    # test when object exists and all fields have the same value
    def test_module_should_not_update_object_when_upsert_operation_and_object_exists_with_the_same_fields(
            self, connection_mock):
        url = '/test'
        url_with_id_templ = '{0}/{1}'.format(url, '{objId}')
        params = {
            'operation': 'upsertObject',
            'data': {'name': 'testObject', 'value': '3333', 'type': 'object'},
            'register_as': 'test_var'
        }
        expected_val = copy.deepcopy(params['data'])
        expected_val['version'] = 'test_version'
        expected_val['id'] = 'test_id'
        def request_handler(url_path=None, http_method=None, body_params=None, path_params=None, query_params=None):
            if http_method == HTTPMethod.POST:
                assert url_path == url
                assert body_params == params['data']
                assert query_params == {}
                assert path_params == {}
                return {
                    ResponseParams.SUCCESS: False,
                    ResponseParams.RESPONSE: DUPLICATE_NAME_ERROR_MESSAGE,
                    ResponseParams.STATUS_CODE: UNPROCESSABLE_ENTITY_STATUS
                }
            elif http_method == HTTPMethod.GET:
                assert url_path == url
                assert body_params == {}
                assert query_params == {QueryParams.FILTER: 'name:testObject', 'limit': 10, 'offset': 0}
                assert path_params == {}
                return {
                    ResponseParams.SUCCESS: True,
                    ResponseParams.RESPONSE: {
                        'items': [expected_val]
                    }
                }
            else:
                assert False
        operations = {
            'getObjectList': {'method': HTTPMethod.GET, 'modelName': 'Object', 'url': url, 'returnMultipleItems': True},
            'addObject': {'method': HTTPMethod.POST, 'modelName': 'Object', 'url': url},
            'editObject': {'method': HTTPMethod.PUT, 'modelName': 'Object', 'url': url_with_id_templ},
            'otherObjectOperation': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': url_with_id_templ,
                'returnMultipleItems': False}
        }
        def get_operation_spec(name):
            return operations[name]
        connection_mock.get_operation_spec = get_operation_spec
        connection_mock.get_operation_specs_by_model_name.return_value = operations
        connection_mock.send_request = request_handler
        result = self._resource_execute_operation(params, connection=connection_mock)
        assert expected_val == result
    def test_module_should_fail_when_upsert_operation_is_not_supported(self, connection_mock):
        connection_mock.get_operation_specs_by_model_name.return_value = {
            'addObject': {'method': HTTPMethod.POST, 'modelName': 'Object', 'url': '/test'},
            'editObject': {'method': HTTPMethod.PUT, 'modelName': 'Object', 'url': '/test/{objId}'},
            'otherObjectOperation': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': '/test/{objId}',
                'returnMultipleItems': False}
        }
        operation_name = 'upsertObject'
        params = {
            'operation': operation_name,
            'data': {'id': '123', 'name': 'testObject', 'type': 'object'},
            'path_params': {'objId': '123'},
            'register_as': 'test_var'
        }
        result = self._resource_execute_operation_with_expected_failure(
            expected_exception_class=FtdInvalidOperationNameError,
            params=params, connection=connection_mock)
        connection_mock.send_request.assert_not_called()
        assert operation_name == result.operation_name
    # when create operation raised FtdConfigurationError exception without id and version
    def test_module_should_fail_when_upsert_operation_and_failed_create_without_id_and_version(self, connection_mock):
        url = '/test'
        url_with_id_templ = '{0}/{1}'.format(url, '{objId}')
        params = {
            'operation': 'upsertObject',
            'data': {'name': 'testObject', 'value': '3333', 'type': 'object'},
            'register_as': 'test_var'
        }
        def request_handler(url_path=None, http_method=None, body_params=None, path_params=None, query_params=None):
            if http_method == HTTPMethod.POST:
                assert url_path == url
                assert body_params == params['data']
                assert query_params == {}
                assert path_params == {}
                return {
                    ResponseParams.SUCCESS: False,
                    ResponseParams.RESPONSE: DUPLICATE_NAME_ERROR_MESSAGE,
                    ResponseParams.STATUS_CODE: UNPROCESSABLE_ENTITY_STATUS
                }
            elif http_method == HTTPMethod.GET:
                assert url_path == url
                assert body_params == {}
                assert query_params == {QueryParams.FILTER: 'name:testObject', 'limit': 10, 'offset': 0}
                assert path_params == {}
                return {
                    ResponseParams.SUCCESS: True,
                    ResponseParams.RESPONSE: {
                        'items': []
                    }
                }
            else:
                assert False
        operations = {
            'getObjectList': {'method': HTTPMethod.GET, 'modelName': 'Object', 'url': url, 'returnMultipleItems': True},
            'addObject': {'method': HTTPMethod.POST, 'modelName': 'Object', 'url': url},
            'editObject': {'method': HTTPMethod.PUT, 'modelName': 'Object', 'url': url_with_id_templ},
            'otherObjectOperation': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': url_with_id_templ,
                'returnMultipleItems': False}
        }
        def get_operation_spec(name):
            return operations[name]
        connection_mock.get_operation_spec = get_operation_spec
        connection_mock.get_operation_specs_by_model_name.return_value = operations
        connection_mock.send_request = request_handler
        result = self._resource_execute_operation_with_expected_failure(
            expected_exception_class=FtdServerError,
            params=params, connection=connection_mock)
        assert result.code == 422
        assert result.response == 'Validation failed due to a duplicate name'
    def test_module_should_fail_when_upsert_operation_and_failed_update_operation(self, connection_mock):
        url = '/test'
        obj_id = '456'
        version = 'test_version'
        url_with_id_templ = '{0}/{1}'.format(url, '{objId}')
        error_code = 404
        new_value = '0000'
        old_value = '1111'
        params = {
            'operation': 'upsertObject',
            'data': {'name': 'testObject', 'value': new_value, 'type': 'object'},
            'register_as': 'test_var'
        }
        error_msg = 'test error'
        def request_handler(url_path=None, http_method=None, body_params=None, path_params=None, query_params=None):
            if http_method == HTTPMethod.POST:
                assert url_path == url
                assert body_params == params['data']
                assert query_params == {}
                assert path_params == {}
                return {
                    ResponseParams.SUCCESS: False,
                    ResponseParams.RESPONSE: DUPLICATE_NAME_ERROR_MESSAGE,
                    ResponseParams.STATUS_CODE: UNPROCESSABLE_ENTITY_STATUS
                }
            elif http_method == HTTPMethod.GET:
                is_get_list_req = url_path == url
                is_get_req = url_path == url_with_id_templ
                assert is_get_req or is_get_list_req
                if is_get_list_req:
                    assert body_params == {}
                    assert query_params == {QueryParams.FILTER: 'name:testObject', 'limit': 10, 'offset': 0}
                elif is_get_req:
                    assert body_params == {}
                    assert query_params == {}
                    assert path_params == {'objId': obj_id}
                return {
                    ResponseParams.SUCCESS: True,
                    ResponseParams.RESPONSE: {
                        'items': [
                            {'name': 'testObject', 'value': old_value, 'type': 'object', 'id': obj_id,
                             'version': version}
                        ]
                    }
                }
            elif http_method == HTTPMethod.PUT:
                assert url_path == url_with_id_templ
                raise FtdServerError(error_msg, error_code)
            else:
                assert False
        operations = {
            'getObjectList': {'method': HTTPMethod.GET, 'modelName': 'Object', 'url': url, 'returnMultipleItems': True},
            'addObject': {'method': HTTPMethod.POST, 'modelName': 'Object', 'url': url},
            'editObject': {'method': HTTPMethod.PUT, 'modelName': 'Object', 'url': url_with_id_templ},
            'otherObjectOperation': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': url_with_id_templ,
                'returnMultipleItems': False}
        }
        def get_operation_spec(name):
            return operations[name]
        connection_mock.get_operation_spec = get_operation_spec
        connection_mock.get_operation_specs_by_model_name.return_value = operations
        connection_mock.send_request = request_handler
        result = self._resource_execute_operation_with_expected_failure(
            expected_exception_class=FtdServerError,
            params=params, connection=connection_mock)
        assert result.code == error_code
        assert result.response == error_msg
    def test_module_should_fail_when_upsert_operation_and_invalid_data_for_create_operation(self, connection_mock):
        new_value = '0000'
        params = {
            'operation': 'upsertObject',
            'data': {'name': 'testObject', 'value': new_value, 'type': 'object'},
            'register_as': 'test_var'
        }
        connection_mock.send_request.assert_not_called()
        operations = {
            'getObjectList': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': 'sd',
                'returnMultipleItems': True},
            'addObject': {'method': HTTPMethod.POST, 'modelName': 'Object', 'url': 'sdf'},
            'editObject': {'method': HTTPMethod.PUT, 'modelName': 'Object', 'url': 'sadf'},
            'otherObjectOperation': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': 'sdfs',
                'returnMultipleItems': False}
        }
        def get_operation_spec(name):
            return operations[name]
        connection_mock.get_operation_spec = get_operation_spec
        connection_mock.get_operation_specs_by_model_name.return_value = operations
        report = {
            'required': ['objects[0].type'],
            'invalid_type': [
                {
                    'path': 'objects[3].id',
                    'expected_type': 'string',
                    'actually_value': 1
                }
            ]
        }
        connection_mock.validate_data.return_value = (False, json.dumps(report, sort_keys=True, indent=4))
        key = 'Invalid data provided'
        result = self._resource_execute_operation_with_expected_failure(
            expected_exception_class=ValidationError,
            params=params, connection=connection_mock)
        assert len(result.args) == 1
        assert key in result.args[0]
        assert json.loads(result.args[0][key]) == {
            'invalid_type': [{'actually_value': 1, 'expected_type': 'string', 'path': 'objects[3].id'}],
            'required': ['objects[0].type']
        }
    def test_module_should_fail_when_upsert_operation_and_few_objects_found_by_filter(self, connection_mock):
        url = '/test'
        url_with_id_templ = '{0}/{1}'.format(url, '{objId}')
        sample_obj = {'name': 'testObject', 'value': '3333', 'type': 'object'}
        params = {
            'operation': 'upsertObject',
            'data': sample_obj,
            'register_as': 'test_var'
        }
        def request_handler(url_path=None, http_method=None, body_params=None, path_params=None, query_params=None):
            if http_method == HTTPMethod.POST:
                assert url_path == url
                assert body_params == params['data']
                assert query_params == {}
                assert path_params == {}
                return {
                    ResponseParams.SUCCESS: False,
                    ResponseParams.RESPONSE: DUPLICATE_NAME_ERROR_MESSAGE,
                    ResponseParams.STATUS_CODE: UNPROCESSABLE_ENTITY_STATUS
                }
            elif http_method == HTTPMethod.GET:
                assert url_path == url
                assert body_params == {}
                assert query_params == {QueryParams.FILTER: 'name:testObject', 'limit': 10, 'offset': 0}
                assert path_params == {}
                return {
                    ResponseParams.SUCCESS: True,
                    ResponseParams.RESPONSE: {
                        'items': [sample_obj, sample_obj]
                    }
                }
            else:
                assert False
        operations = {
            'getObjectList': {'method': HTTPMethod.GET, 'modelName': 'Object', 'url': url, 'returnMultipleItems': True},
            'addObject': {'method': HTTPMethod.POST, 'modelName': 'Object', 'url': url},
            'editObject': {'method': HTTPMethod.PUT, 'modelName': 'Object', 'url': url_with_id_templ},
            'otherObjectOperation': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': url_with_id_templ,
                'returnMultipleItems': False}
        }
        def get_operation_spec(name):
            return operations[name]
        connection_mock.get_operation_spec = get_operation_spec
        connection_mock.get_operation_specs_by_model_name.return_value = operations
        connection_mock.send_request = request_handler
        result = self._resource_execute_operation_with_expected_failure(
            expected_exception_class=FtdConfigurationError,
            params=params, connection=connection_mock)
        assert result.msg is MULTIPLE_DUPLICATES_FOUND_ERROR
        assert result.obj is None
    @staticmethod
    def _resource_execute_operation(params, connection):
        resource = BaseConfigurationResource(connection)
        op_name = params['operation']
        resp = resource.execute_operation(op_name, params)
        return resp
    def _resource_execute_operation_with_expected_failure(self, expected_exception_class, params, connection):
        with pytest.raises(expected_exception_class) as ex:
            self._resource_execute_operation(params, connection)
        # 'ex' here is the instance of '_pytest._code.code.ExceptionInfo' but not <expected_exception_class>
        # actual instance of <expected_exception_class> is in the value attribute of 'ex'. That's why we should return
        # 'ex.value' here, so it can be checked in a test later.
        return ex.value
"
-------------------------------------------------------------------------
"from __future__ import absolute_import
import copy
import json
import unittest
import pytest
from units.compat import mock
from ansible.module_utils.network.ftd.common import FtdServerError, HTTPMethod, ResponseParams, FtdConfigurationError
from ansible.module_utils.network.ftd.configuration import DUPLICATE_NAME_ERROR_MESSAGE, UNPROCESSABLE_ENTITY_STATUS, \
    MULTIPLE_DUPLICATES_FOUND_ERROR, BaseConfigurationResource, FtdInvalidOperationNameError, QueryParams
from ansible.module_utils.network.ftd.fdm_swagger_client import ValidationError
from re import copy_identity_properties
ADD_RESPONSE = {'status': 'Object added'}
EDIT_RESPONSE = {'status': 'Object edited'}
DELETE_RESPONSE = {'status': 'Object deleted'}
GET_BY_FILTER_RESPONSE = [{'name': 'foo', 'description': 'bar'}]
ARBITRARY_RESPONSE = {'status': 'Arbitrary request sent'}


class TestUpsertOperationUnitTests(unittest.TestCase):
    def setUp(self):
        conn = mock.MagicMock()
        self._resource = BaseConfigurationResource(conn)

    def test_get_operation_name(self):
        operation_a = mock.MagicMock()
        operation_b = mock.MagicMock()

        def checker_wrapper(expected_object):
            def checker(obj, *args, **kwargs):
                return obj == expected_object
            return checker

        operations = {
            operation_a: ""spec"",
            operation_b: ""spec""
        }
        assert operation_a == self._resource._get_operation_name(checker_wrapper(operation_a), operations)
        assert operation_b == self._resource._get_operation_name(checker_wrapper(operation_b), operations)
        self.assertRaises(
            FtdConfigurationError,
            self._resource._get_operation_name, checker_wrapper(None), operations
        )

    @mock.patch.object(BaseConfigurationResource, ""_get_operation_name"")
    @mock.patch.object(BaseConfigurationResource, ""add_object"")
    
    def test_add_upserted_object(self, add_object_mock, get_operation_mock):
        model_operations = mock.MagicMock()
        params = mock.MagicMock()
        add_op_name = get_operation_mock.return_value
        assert add_object_mock.return_value == self._resource._add_upserted_object(model_operations, params)
        get_operation_mock.assert_called_once_with(
            self._resource._operation_checker.is_add_operation,
            model_operations)
        add_object_mock.assert_called_once_with(add_op_name, params)

    @mock.patch.object(BaseConfigurationResource, ""_get_operation_name"")
    @mock.patch.object(BaseConfigurationResource, ""edit_object"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration.copy_identity_properties"")
    
    def test_edit_upserted_object(self, copy_properties_mock, edit_object_mock, get_operation_mock):
        model_operations = mock.MagicMock()
        existing_object = mock.MagicMock()
        params = {
            'path_params': {},
            'data': {}
        }
        result = self._resource._edit_upserted_object(model_operations, existing_object, params)
        copy_properties_mock.assert_called_once_with(
            existing_object,
            params['data']
        )
        edit_object_mock.assert_called_once_with(
            get_operation_mock.return_value,
            params
        )"
-------------------------------------------------------------------------
"# Copyright (c) 2018 Cisco and/or its affiliates.
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
#
from __future__ import absolute_import
import copy
import json
import unittest
import pytest
from ansible.compat.tests import mock
from ansible.module_utils.network.ftd.common import FtdServerError, HTTPMethod, ResponseParams, FtdConfigurationError
from ansible.module_utils.network.ftd.configuration import DUPLICATE_NAME_ERROR_MESSAGE, UNPROCESSABLE_ENTITY_STATUS, \
    MULTIPLE_DUPLICATES_FOUND_ERROR, BaseConfigurationResource, FtdInvalidOperationNameError, QueryParams
from ansible.module_utils.network.ftd.fdm_swagger_client import ValidationError
ADD_RESPONSE = {'status': 'Object added'}
EDIT_RESPONSE = {'status': 'Object edited'}
DELETE_RESPONSE = {'status': 'Object deleted'}
GET_BY_FILTER_RESPONSE = [{'name': 'foo', 'description': 'bar'}]
ARBITRARY_RESPONSE = {'status': 'Arbitrary request sent'}
class TestUpsertOperationUnitTests(unittest.TestCase):
    def setUp(self):
        conn = mock.MagicMock()
        self._resource = BaseConfigurationResource(conn)
    def test_get_operation_name(self):
        operation_a = mock.MagicMock()
        operation_b = mock.MagicMock()
        def checker_wrapper(expected_object):
            def checker(obj, *args, **kwargs):
                return obj == expected_object
            return checker
        operations = {
            operation_a: ""spec"",
            operation_b: ""spec""
        }
        assert operation_a == self._resource._get_operation_name(checker_wrapper(operation_a), operations)
        assert operation_b == self._resource._get_operation_name(checker_wrapper(operation_b), operations)
        self.assertRaises(
            FtdConfigurationError,
            self._resource._get_operation_name, checker_wrapper(None), operations
        )
    @mock.patch.object(BaseConfigurationResource, ""_get_operation_name"")
    @mock.patch.object(BaseConfigurationResource, ""add_object"")
    def test_add_upserted_object(self, add_object_mock, get_operation_mock):
        model_operations = mock.MagicMock()
        params = mock.MagicMock()
        add_op_name = get_operation_mock.return_value
        assert add_object_mock.return_value == self._resource._add_upserted_object(model_operations, params)
        get_operation_mock.assert_called_once_with(
            self._resource._operation_checker.is_add_operation,
            model_operations)
        add_object_mock.assert_called_once_with(add_op_name, params)
    @mock.patch.object(BaseConfigurationResource, ""_get_operation_name"")
    @mock.patch.object(BaseConfigurationResource, ""edit_object"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration.copy_identity_properties"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration._set_default"")
    def test_edit_upserted_object(self, _set_default_mock, copy_properties_mock, edit_object_mock, get_operation_mock):
        model_operations = mock.MagicMock()
        existing_object = mock.MagicMock()
        params = {
            'path_params': {},
            'data': {}
        }
        result = self._resource._edit_upserted_object(model_operations, existing_object, params)
        assert result == edit_object_mock.return_value
        _set_default_mock.assert_has_calls([
            mock.call(params, 'path_params', {}),
            mock.call(params, 'data', {})
        ])
        get_operation_mock.assert_called_once_with(
            self._resource._operation_checker.is_edit_operation,
            model_operations
        )
        copy_properties_mock.assert_called_once_with(
            existing_object,
            params['data']
        )
        edit_object_mock.assert_called_once_with(
            get_operation_mock.return_value,
            params
        )
    @mock.patch.object(BaseConfigurationResource, ""get_operation_specs_by_model_name"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration.OperationChecker.is_upsert_operation_supported"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration._extract_model_from_upsert_operation"")
    def test_is_upsert_operation_supported(self, extract_model_mock, is_upsert_supported_mock, get_operation_spec_mock):
        op_name = mock.MagicMock()
        result = self._resource.is_upsert_operation_supported(op_name)
        assert result == is_upsert_supported_mock.return_value
        extract_model_mock.assert_called_once_with(op_name)
        get_operation_spec_mock.assert_called_once_with(extract_model_mock.return_value)
        is_upsert_supported_mock.assert_called_once_with(get_operation_spec_mock.return_value)
    @mock.patch.object(BaseConfigurationResource, ""is_upsert_operation_supported"")
    @mock.patch.object(BaseConfigurationResource, ""get_operation_specs_by_model_name"")
    @mock.patch.object(BaseConfigurationResource, ""_add_upserted_object"")
    @mock.patch.object(BaseConfigurationResource, ""_edit_upserted_object"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration._extract_model_from_upsert_operation"")
    def test_upsert_object_succesfully_added(self, extract_model_mock, edit_mock, add_mock, get_operation_mock,
                                             is_upsert_supported_mock):
        op_name = mock.MagicMock()
        params = mock.MagicMock()
        is_upsert_supported_mock.return_value = True
        result = self._resource.upsert_object(op_name, params)
        assert result == add_mock.return_value
        is_upsert_supported_mock.assert_called_once_with(op_name)
        extract_model_mock.assert_called_once_with(op_name)
        get_operation_mock.assert_called_once_with(extract_model_mock.return_value)
        add_mock.assert_called_once_with(get_operation_mock.return_value, params)
        edit_mock.assert_not_called()
    @mock.patch.object(BaseConfigurationResource, ""is_upsert_operation_supported"")
    @mock.patch.object(BaseConfigurationResource, ""get_operation_specs_by_model_name"")
    @mock.patch.object(BaseConfigurationResource, ""_add_upserted_object"")
    @mock.patch.object(BaseConfigurationResource, ""_edit_upserted_object"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration._extract_model_from_upsert_operation"")
    def test_upsert_object_succesfully_edited(self, extract_model_mock, edit_mock, add_mock, get_operation_mock,
                                              is_upsert_supported_mock):
        op_name = mock.MagicMock()
        params = mock.MagicMock()
        is_upsert_supported_mock.return_value = True
        error = FtdConfigurationError(""Obj duplication error"")
        error.obj = mock.MagicMock()
        add_mock.side_effect = error
        result = self._resource.upsert_object(op_name, params)
        assert result == edit_mock.return_value
        is_upsert_supported_mock.assert_called_once_with(op_name)
        extract_model_mock.assert_called_once_with(op_name)
        get_operation_mock.assert_called_once_with(extract_model_mock.return_value)
        add_mock.assert_called_once_with(get_operation_mock.return_value, params)
        edit_mock.assert_called_once_with(get_operation_mock.return_value, error.obj, params)
    @mock.patch.object(BaseConfigurationResource, ""is_upsert_operation_supported"")
    @mock.patch.object(BaseConfigurationResource, ""get_operation_specs_by_model_name"")
    @mock.patch.object(BaseConfigurationResource, ""_add_upserted_object"")
    @mock.patch.object(BaseConfigurationResource, ""_edit_upserted_object"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration._extract_model_from_upsert_operation"")
    def test_upsert_object_not_supported(self, extract_model_mock, edit_mock, add_mock, get_operation_mock,
                                         is_upsert_supported_mock):
        op_name = mock.MagicMock()
        params = mock.MagicMock()
        is_upsert_supported_mock.return_value = False
        self.assertRaises(
            FtdInvalidOperationNameError,
            self._resource.upsert_object, op_name, params
        )
        is_upsert_supported_mock.assert_called_once_with(op_name)
        extract_model_mock.assert_not_called()
        get_operation_mock.assert_not_called()
        add_mock.assert_not_called()
        edit_mock.assert_not_called()
    @mock.patch.object(BaseConfigurationResource, ""is_upsert_operation_supported"")
    @mock.patch.object(BaseConfigurationResource, ""get_operation_specs_by_model_name"")
    @mock.patch.object(BaseConfigurationResource, ""_add_upserted_object"")
    @mock.patch.object(BaseConfigurationResource, ""_edit_upserted_object"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration._extract_model_from_upsert_operation"")
    def test_upsert_object_neither_added_nor_edited(self, extract_model_mock, edit_mock, add_mock, get_operation_mock,
                                                    is_upsert_supported_mock):
        op_name = mock.MagicMock()
        params = mock.MagicMock()
        is_upsert_supported_mock.return_value = True
        error = FtdConfigurationError(""Obj duplication error"")
        error.obj = mock.MagicMock()
        add_mock.side_effect = error
        edit_mock.side_effect = FtdConfigurationError(""Some object edit error"")
        self.assertRaises(
            FtdConfigurationError,
            self._resource.upsert_object, op_name, params
        )
        is_upsert_supported_mock.assert_called_once_with(op_name)
        extract_model_mock.assert_called_once_with(op_name)
        get_operation_mock.assert_called_once_with(extract_model_mock.return_value)
        add_mock.assert_called_once_with(get_operation_mock.return_value, params)
        edit_mock.assert_called_once_with(get_operation_mock.return_value, error.obj, params)
    @mock.patch.object(BaseConfigurationResource, ""is_upsert_operation_supported"")
    @mock.patch.object(BaseConfigurationResource, ""get_operation_specs_by_model_name"")
    @mock.patch.object(BaseConfigurationResource, ""_add_upserted_object"")
    @mock.patch.object(BaseConfigurationResource, ""_edit_upserted_object"")
    @mock.patch(""ansible.module_utils.network.ftd.configuration._extract_model_from_upsert_operation"")
    def test_upsert_object_with_fatal_error_during_add(self, extract_model_mock, edit_mock, add_mock,
                                                       get_operation_mock, is_upsert_supported_mock):
        op_name = mock.MagicMock()
        params = mock.MagicMock()
        is_upsert_supported_mock.return_value = True
        error = FtdConfigurationError(""Obj duplication error"")
        add_mock.side_effect = error
        self.assertRaises(
            FtdConfigurationError,
            self._resource.upsert_object, op_name, params
        )
        is_upsert_supported_mock.assert_called_once_with(op_name)
        extract_model_mock.assert_called_once_with(op_name)
        get_operation_mock.assert_called_once_with(extract_model_mock.return_value)
        add_mock.assert_called_once_with(get_operation_mock.return_value, params)
        edit_mock.assert_not_called()
# functional tests below
class TestUpsertOperationFunctionalTests(object):
    @pytest.fixture(autouse=True)
    def connection_mock(self, mocker):
        connection_class_mock = mocker.patch('ansible.modules.network.ftd.ftd_configuration.Connection')
        connection_instance = connection_class_mock.return_value
        connection_instance.validate_data.return_value = True, None
        connection_instance.validate_query_params.return_value = True, None
        connection_instance.validate_path_params.return_value = True, None
        return connection_instance
    def test_module_should_create_object_when_upsert_operation_and_object_does_not_exist(self, connection_mock):
        url = '/test'
        operations = {
            'getObjectList': {
                'method': HTTPMethod.GET,
                'url': url,
                'modelName': 'Object',
                'returnMultipleItems': True},
            'addObject': {
                'method': HTTPMethod.POST,
                'modelName': 'Object',
                'url': url},
            'editObject': {
                'method': HTTPMethod.PUT,
                'modelName': 'Object',
                'url': '/test/{objId}'},
            'otherObjectOperation': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': '/test/{objId}',
                'returnMultipleItems': False
            }
        }
        def get_operation_spec(name):
            return operations[name]
        connection_mock.get_operation_spec = get_operation_spec
        connection_mock.get_operation_specs_by_model_name.return_value = operations
        connection_mock.send_request.return_value = {
            ResponseParams.SUCCESS: True,
            ResponseParams.RESPONSE: ADD_RESPONSE
        }
        params = {
            'operation': 'upsertObject',
            'data': {'id': '123', 'name': 'testObject', 'type': 'object'},
            'path_params': {'objId': '123'},
            'register_as': 'test_var'
        }
        result = self._resource_execute_operation(params, connection=connection_mock)
        connection_mock.send_request.assert_called_once_with(url_path=url,
                                                             http_method=HTTPMethod.POST,
                                                             path_params=params['path_params'],
                                                             query_params={},
                                                             body_params=params['data'])
        assert ADD_RESPONSE == result
    # test when object exists but with different fields(except id)
    def test_module_should_update_object_when_upsert_operation_and_object_exists(self, connection_mock):
        url = '/test'
        obj_id = '456'
        version = 'test_version'
        url_with_id_templ = '{0}/{1}'.format(url, '{objId}')
        new_value = '0000'
        old_value = '1111'
        params = {
            'operation': 'upsertObject',
            'data': {'name': 'testObject', 'value': new_value, 'type': 'object'},
            'register_as': 'test_var'
        }
        def request_handler(url_path=None, http_method=None, body_params=None, path_params=None, query_params=None):
            if http_method == HTTPMethod.POST:
                assert url_path == url
                assert body_params == params['data']
                assert query_params == {}
                assert path_params == {}
                return {
                    ResponseParams.SUCCESS: False,
                    ResponseParams.RESPONSE: DUPLICATE_NAME_ERROR_MESSAGE,
                    ResponseParams.STATUS_CODE: UNPROCESSABLE_ENTITY_STATUS
                }
            elif http_method == HTTPMethod.GET:
                is_get_list_req = url_path == url
                is_get_req = url_path == url_with_id_templ
                assert is_get_req or is_get_list_req
                if is_get_list_req:
                    assert body_params == {}
                    assert query_params == {QueryParams.FILTER: 'name:testObject', 'limit': 10, 'offset': 0}
                    assert path_params == {}
                elif is_get_req:
                    assert body_params == {}
                    assert query_params == {}
                    assert path_params == {'objId': obj_id}
                return {
                    ResponseParams.SUCCESS: True,
                    ResponseParams.RESPONSE: {
                        'items': [
                            {'name': 'testObject', 'value': old_value, 'type': 'object', 'id': obj_id,
                             'version': version}
                        ]
                    }
                }
            elif http_method == HTTPMethod.PUT:
                assert url_path == url_with_id_templ
                return {
                    ResponseParams.SUCCESS: True,
                    ResponseParams.RESPONSE: body_params
                }
            else:
                assert False
        operations = {
            'getObjectList': {'method': HTTPMethod.GET, 'url': url, 'modelName': 'Object', 'returnMultipleItems': True},
            'addObject': {'method': HTTPMethod.POST, 'modelName': 'Object', 'url': url},
            'editObject': {'method': HTTPMethod.PUT, 'modelName': 'Object', 'url': url_with_id_templ},
            'otherObjectOperation': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': url_with_id_templ,
                'returnMultipleItems': False}
        }
        def get_operation_spec(name):
            return operations[name]
        connection_mock.get_operation_spec = get_operation_spec
        connection_mock.get_operation_specs_by_model_name.return_value = operations
        connection_mock.send_request = request_handler
        expected_val = {'name': 'testObject', 'value': new_value, 'type': 'object', 'id': obj_id, 'version': version}
        result = self._resource_execute_operation(params, connection=connection_mock)
        assert expected_val == result
    # test when object exists and all fields have the same value
    def test_module_should_not_update_object_when_upsert_operation_and_object_exists_with_the_same_fields(
            self, connection_mock):
        url = '/test'
        url_with_id_templ = '{0}/{1}'.format(url, '{objId}')
        params = {
            'operation': 'upsertObject',
            'data': {'name': 'testObject', 'value': '3333', 'type': 'object'},
            'register_as': 'test_var'
        }
        expected_val = copy.deepcopy(params['data'])
        expected_val['version'] = 'test_version'
        expected_val['id'] = 'test_id'
        def request_handler(url_path=None, http_method=None, body_params=None, path_params=None, query_params=None):
            if http_method == HTTPMethod.POST:
                assert url_path == url
                assert body_params == params['data']
                assert query_params == {}
                assert path_params == {}
                return {
                    ResponseParams.SUCCESS: False,
                    ResponseParams.RESPONSE: DUPLICATE_NAME_ERROR_MESSAGE,
                    ResponseParams.STATUS_CODE: UNPROCESSABLE_ENTITY_STATUS
                }
            elif http_method == HTTPMethod.GET:
                assert url_path == url
                assert body_params == {}
                assert query_params == {QueryParams.FILTER: 'name:testObject', 'limit': 10, 'offset': 0}
                assert path_params == {}
                return {
                    ResponseParams.SUCCESS: True,
                    ResponseParams.RESPONSE: {
                        'items': [expected_val]
                    }
                }
            else:
                assert False
        operations = {
            'getObjectList': {'method': HTTPMethod.GET, 'modelName': 'Object', 'url': url, 'returnMultipleItems': True},
            'addObject': {'method': HTTPMethod.POST, 'modelName': 'Object', 'url': url},
            'editObject': {'method': HTTPMethod.PUT, 'modelName': 'Object', 'url': url_with_id_templ},
            'otherObjectOperation': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': url_with_id_templ,
                'returnMultipleItems': False}
        }
        def get_operation_spec(name):
            return operations[name]
        connection_mock.get_operation_spec = get_operation_spec
        connection_mock.get_operation_specs_by_model_name.return_value = operations
        connection_mock.send_request = request_handler
        result = self._resource_execute_operation(params, connection=connection_mock)
        assert expected_val == result
    def test_module_should_fail_when_upsert_operation_is_not_supported(self, connection_mock):
        connection_mock.get_operation_specs_by_model_name.return_value = {
            'addObject': {'method': HTTPMethod.POST, 'modelName': 'Object', 'url': '/test'},
            'editObject': {'method': HTTPMethod.PUT, 'modelName': 'Object', 'url': '/test/{objId}'},
            'otherObjectOperation': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': '/test/{objId}',
                'returnMultipleItems': False}
        }
        operation_name = 'upsertObject'
        params = {
            'operation': operation_name,
            'data': {'id': '123', 'name': 'testObject', 'type': 'object'},
            'path_params': {'objId': '123'},
            'register_as': 'test_var'
        }
        result = self._resource_execute_operation_with_expected_failure(
            expected_exception_class=FtdInvalidOperationNameError,
            params=params, connection=connection_mock)
        connection_mock.send_request.assert_not_called()
        assert operation_name == result.operation_name
    # when create operation raised FtdConfigurationError exception without id and version
    def test_module_should_fail_when_upsert_operation_and_failed_create_without_id_and_version(self, connection_mock):
        url = '/test'
        url_with_id_templ = '{0}/{1}'.format(url, '{objId}')
        params = {
            'operation': 'upsertObject',
            'data': {'name': 'testObject', 'value': '3333', 'type': 'object'},
            'register_as': 'test_var'
        }
        def request_handler(url_path=None, http_method=None, body_params=None, path_params=None, query_params=None):
            if http_method == HTTPMethod.POST:
                assert url_path == url
                assert body_params == params['data']
                assert query_params == {}
                assert path_params == {}
                return {
                    ResponseParams.SUCCESS: False,
                    ResponseParams.RESPONSE: DUPLICATE_NAME_ERROR_MESSAGE,
                    ResponseParams.STATUS_CODE: UNPROCESSABLE_ENTITY_STATUS
                }
            elif http_method == HTTPMethod.GET:
                assert url_path == url
                assert body_params == {}
                assert query_params == {QueryParams.FILTER: 'name:testObject', 'limit': 10, 'offset': 0}
                assert path_params == {}
                return {
                    ResponseParams.SUCCESS: True,
                    ResponseParams.RESPONSE: {
                        'items': []
                    }
                }
            else:
                assert False
        operations = {
            'getObjectList': {'method': HTTPMethod.GET, 'modelName': 'Object', 'url': url, 'returnMultipleItems': True},
            'addObject': {'method': HTTPMethod.POST, 'modelName': 'Object', 'url': url},
            'editObject': {'method': HTTPMethod.PUT, 'modelName': 'Object', 'url': url_with_id_templ},
            'otherObjectOperation': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': url_with_id_templ,
                'returnMultipleItems': False}
        }
        def get_operation_spec(name):
            return operations[name]
        connection_mock.get_operation_spec = get_operation_spec
        connection_mock.get_operation_specs_by_model_name.return_value = operations
        connection_mock.send_request = request_handler
        result = self._resource_execute_operation_with_expected_failure(
            expected_exception_class=FtdServerError,
            params=params, connection=connection_mock)
        assert result.code == 422
        assert result.response == 'Validation failed due to a duplicate name'
    def test_module_should_fail_when_upsert_operation_and_failed_update_operation(self, connection_mock):
        url = '/test'
        obj_id = '456'
        version = 'test_version'
        url_with_id_templ = '{0}/{1}'.format(url, '{objId}')
        error_code = 404
        new_value = '0000'
        old_value = '1111'
        params = {
            'operation': 'upsertObject',
            'data': {'name': 'testObject', 'value': new_value, 'type': 'object'},
            'register_as': 'test_var'
        }
        error_msg = 'test error'
        def request_handler(url_path=None, http_method=None, body_params=None, path_params=None, query_params=None):
            if http_method == HTTPMethod.POST:
                assert url_path == url
                assert body_params == params['data']
                assert query_params == {}
                assert path_params == {}
                return {
                    ResponseParams.SUCCESS: False,
                    ResponseParams.RESPONSE: DUPLICATE_NAME_ERROR_MESSAGE,
                    ResponseParams.STATUS_CODE: UNPROCESSABLE_ENTITY_STATUS
                }
            elif http_method == HTTPMethod.GET:
                is_get_list_req = url_path == url
                is_get_req = url_path == url_with_id_templ
                assert is_get_req or is_get_list_req
                if is_get_list_req:
                    assert body_params == {}
                    assert query_params == {QueryParams.FILTER: 'name:testObject', 'limit': 10, 'offset': 0}
                elif is_get_req:
                    assert body_params == {}
                    assert query_params == {}
                    assert path_params == {'objId': obj_id}
                return {
                    ResponseParams.SUCCESS: True,
                    ResponseParams.RESPONSE: {
                        'items': [
                            {'name': 'testObject', 'value': old_value, 'type': 'object', 'id': obj_id,
                             'version': version}
                        ]
                    }
                }
            elif http_method == HTTPMethod.PUT:
                assert url_path == url_with_id_templ
                raise FtdServerError(error_msg, error_code)
            else:
                assert False
        operations = {
            'getObjectList': {'method': HTTPMethod.GET, 'modelName': 'Object', 'url': url, 'returnMultipleItems': True},
            'addObject': {'method': HTTPMethod.POST, 'modelName': 'Object', 'url': url},
            'editObject': {'method': HTTPMethod.PUT, 'modelName': 'Object', 'url': url_with_id_templ},
            'otherObjectOperation': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': url_with_id_templ,
                'returnMultipleItems': False}
        }
        def get_operation_spec(name):
            return operations[name]
        connection_mock.get_operation_spec = get_operation_spec
        connection_mock.get_operation_specs_by_model_name.return_value = operations
        connection_mock.send_request = request_handler
        result = self._resource_execute_operation_with_expected_failure(
            expected_exception_class=FtdServerError,
            params=params, connection=connection_mock)
        assert result.code == error_code
        assert result.response == error_msg
    def test_module_should_fail_when_upsert_operation_and_invalid_data_for_create_operation(self, connection_mock):
        new_value = '0000'
        params = {
            'operation': 'upsertObject',
            'data': {'name': 'testObject', 'value': new_value, 'type': 'object'},
            'register_as': 'test_var'
        }
        connection_mock.send_request.assert_not_called()
        operations = {
            'getObjectList': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': 'sd',
                'returnMultipleItems': True},
            'addObject': {'method': HTTPMethod.POST, 'modelName': 'Object', 'url': 'sdf'},
            'editObject': {'method': HTTPMethod.PUT, 'modelName': 'Object', 'url': 'sadf'},
            'otherObjectOperation': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': 'sdfs',
                'returnMultipleItems': False}
        }
        def get_operation_spec(name):
            return operations[name]
        connection_mock.get_operation_spec = get_operation_spec
        connection_mock.get_operation_specs_by_model_name.return_value = operations
        report = {
            'required': ['objects[0].type'],
            'invalid_type': [
                {
                    'path': 'objects[3].id',
                    'expected_type': 'string',
                    'actually_value': 1
                }
            ]
        }
        connection_mock.validate_data.return_value = (False, json.dumps(report, sort_keys=True, indent=4))
        key = 'Invalid data provided'
        result = self._resource_execute_operation_with_expected_failure(
            expected_exception_class=ValidationError,
            params=params, connection=connection_mock)
        assert len(result.args) == 1
        assert key in result.args[0]
        assert json.loads(result.args[0][key]) == {
            'invalid_type': [{'actually_value': 1, 'expected_type': 'string', 'path': 'objects[3].id'}],
            'required': ['objects[0].type']
        }
    def test_module_should_fail_when_upsert_operation_and_few_objects_found_by_filter(self, connection_mock):
        url = '/test'
        url_with_id_templ = '{0}/{1}'.format(url, '{objId}')
        sample_obj = {'name': 'testObject', 'value': '3333', 'type': 'object'}
        params = {
            'operation': 'upsertObject',
            'data': sample_obj,
            'register_as': 'test_var'
        }
        def request_handler(url_path=None, http_method=None, body_params=None, path_params=None, query_params=None):
            if http_method == HTTPMethod.POST:
                assert url_path == url
                assert body_params == params['data']
                assert query_params == {}
                assert path_params == {}
                return {
                    ResponseParams.SUCCESS: False,
                    ResponseParams.RESPONSE: DUPLICATE_NAME_ERROR_MESSAGE,
                    ResponseParams.STATUS_CODE: UNPROCESSABLE_ENTITY_STATUS
                }
            elif http_method == HTTPMethod.GET:
                assert url_path == url
                assert body_params == {}
                assert query_params == {QueryParams.FILTER: 'name:testObject', 'limit': 10, 'offset': 0}
                assert path_params == {}
                return {
                    ResponseParams.SUCCESS: True,
                    ResponseParams.RESPONSE: {
                        'items': [sample_obj, sample_obj]
                    }
                }
            else:
                assert False
        operations = {
            'getObjectList': {'method': HTTPMethod.GET, 'modelName': 'Object', 'url': url, 'returnMultipleItems': True},
            'addObject': {'method': HTTPMethod.POST, 'modelName': 'Object', 'url': url},
            'editObject': {'method': HTTPMethod.PUT, 'modelName': 'Object', 'url': url_with_id_templ},
            'otherObjectOperation': {
                'method': HTTPMethod.GET,
                'modelName': 'Object',
                'url': url_with_id_templ,
                'returnMultipleItems': False}
        }
        def get_operation_spec(name):
            return operations[name]
        connection_mock.get_operation_spec = get_operation_spec
        connection_mock.get_operation_specs_by_model_name.return_value = operations
        connection_mock.send_request = request_handler
        result = self._resource_execute_operation_with_expected_failure(
            expected_exception_class=FtdConfigurationError,
            params=params, connection=connection_mock)
        assert result.msg is MULTIPLE_DUPLICATES_FOUND_ERROR
        assert result.obj is None
    @staticmethod
    def _resource_execute_operation(params, connection):
        resource = BaseConfigurationResource(connection)
        op_name = params['operation']
        resp = resource.execute_operation(op_name, params)
        return resp
    def _resource_execute_operation_with_expected_failure(self, expected_exception_class, params, connection):
        with pytest.raises(expected_exception_class) as ex:
            self._resource_execute_operation(params, connection)
        # 'ex' here is the instance of '_pytest._code.code.ExceptionInfo' but not <expected_exception_class>
        # actual instance of <expected_exception_class> is in the value attribute of 'ex'. That's why we should return
        # 'ex.value' here, so it can be checked in a test later.
        return ex.value
"
-------------------------------------------------------------------------
"Recom
PRs: 47747, 48982"
-------------------------------------------------------------------------
=========================================================================
"from ansible.module_utils.six import BytesIO, StringIO
"
-------------------------------------------------------------------------
"import re
from ansible.module_utils.six import BytesIO, StringIO

INVALID_IDENTIFIER_SYMBOLS = r'[^a-zA-Z0-9_]'

IDENTITY_PROPERTIES = ['id', 'version', 'ruleId']
NON_COMPARABLE_PROPERTIES = IDENTITY_PROPERTIES + ['isSystemDefined', 'links']


class HTTPMethod:
    GET = 'get'
    POST = 'post'
    PUT = 'put'
    DELETE = 'delete'


class ResponseParams:
    SUCCESS = 'success'
    STATUS_CODE = 'status_code'
    RESPONSE = 'response'


class FtdConfigurationError(Exception):
    pass


def construct_ansible_facts(response, params):
    facts = dict()
    if response:
        response_body = response['items'] if 'items' in response else response
        if params.get('register_as'):
            facts[params['register_as']] = response_body
        elif 'name' in response_body and 'type' in response_body:
            object_name = re.sub(INVALID_IDENTIFIER_SYMBOLS, '_', response_body['name'].lower())
            fact_name = '%s_%s' % (response_body['type'], object_name)
            facts[fact_name] = response_body
    return facts


def copy_identity_properties(source_obj, dest_obj):
    for property_name in IDENTITY_PROPERTIES:
        if property_name in source_obj:
            dest_obj[property_name] = source_obj[property_name]
    return dest_obj


def is_object_ref(d):
    """"""
    Checks if a dictionary is a reference object. The dictionary is considered to be a
    reference object when it contains non-empty 'id' and 'type' fields.

    :type d: dict
    :return: True if passed dictionary is a reference object, otherwise False
    """"""
    has_id = 'id' in d.keys() and d['id']
    has_type = 'type' in d.keys() and d['type']
    return has_id and has_type


def equal_object_refs(d1, d2):
    """"""
    Checks whether two references point to the same object.

    :type d1: dict
    :type d2: dict
    :return: True if passed references point to the same object, otherwise False
    """"""
    have_equal_ids = d1['id'] == d2['id']
    have_equal_types = d1['type'] == d2['type']
    return have_equal_ids and have_equal_types


def equal_lists(l1, l2):
    """"""
    Checks whether two lists are equal. The order of elements in the arrays is important.

    :type l1: list
    :type l2: list
    :return: True if passed lists, their elements and order of elements are equal. Otherwise, returns False.
    """"""
    if len(l1) != len(l2):
        return False

    for v1, v2 in zip(l1, l2):
        if not equal_values(v1, v2):
            return False

    return True


def equal_dicts(d1, d2, compare_by_reference=True):
    """"""
    Checks whether two dictionaries are equal. If `compare_by_reference` is set to True, dictionaries referencing
    objects are compared using `equal_object_refs` method. Otherwise, every key and value is checked.

    :type d1: dict
    :type d2: dict
    :param compare_by_reference: if True, dictionaries referencing objects are compared using `equal_object_refs` method
    :return: True if passed dicts are equal. Otherwise, returns False.
    """"""
    if compare_by_reference and is_object_ref(d1) and is_object_ref(d2):
        return equal_object_refs(d1, d2)

    if len(d1) != len(d2):
        return False

    for key, v1 in d1.items():
        if key not in d2:
            return False

        v2 = d2[key]
        if not equal_values(v1, v2):
            return False

    return True


def equal_values(v1, v2):
    """"""
    Checks whether types and content of two values are the same. In case of complex objects, the method might be
    called recursively.

    :param v1: first value
    :param v2: second value
    :return: True if types and content of passed values are equal. Otherwise, returns False.
    :rtype: bool
    """"""
    if type(v1) != type(v2):
        return False
    value_type = type(v1)

    if value_type == list:
        return equal_lists(v1, v2)
    elif value_type == dict:
        return equal_dicts(v1, v2)
    else:
        return v1 == v2


def equal_objects(d1, d2):
    """"""
    Checks whether two objects are equal. Ignores special object properties (e.g. 'id', 'version') and
    properties with None and empty values. In case properties contains a reference to the other object,
    only object identities (ids and types) are checked.

    :type d1: dict
    :type d2: dict
    :return: True if passed objects and their properties are equal. Otherwise, returns False.
    """"""
    d1 = dict((k, d1[k]) for k in d1.keys() if k not in NON_COMPARABLE_PROPERTIES and d1[k])
    d2 = dict((k, d2[k]) for k in d2.keys() if k not in NON_COMPARABLE_PROPERTIES and d2[k])

    return equal_dicts(d1, d2, compare_by_reference=False)


def __init__(self, response, code):
    super(FtdServerError, self).__init__(response)
    self.response = response
    self.code = code"
-------------------------------------------------------------------------
"def test_login_raises_exception_when_http_error(self):
    self.connection_mock.send.side_effect = HTTPError('http://testhost.com', 400, '', {},
                                                      StringIO('{""message"": ""Failed to authenticate user""}'))
    with self.assertRaises(ConnectionError) as res:
        self.ftd_plugin.login('foo', 'bar')
    assert 'Failed to authenticate user' in str(res.exception)
"
-------------------------------------------------------------------------
"Recom
PRs: 47747, 48982"
-------------------------------------------------------------------------
=========================================================================
"def test_login_raises_exception_when_http_error(self):
    self.connection_mock.send.side_effect = HTTPError('http://testhost.com', 400, '', {},
                                                      StringIO('{""message"": ""Failed to authenticate user""}'))
    with self.assertRaises(ConnectionError) as res:
        self.ftd_plugin.login('foo', 'bar')
    assert 'Failed to authenticate user' in str(res.exception)
"
-------------------------------------------------------------------------
"def test_login_raises_exception_when_http_error(self):
    self.connection_mock.send.side_effect = HTTPError('http://testhost.com', 400, '', {},
                                                      StringIO('{""message"": ""Failed to authenticate user""}'))
    with self.assertRaises(FtdServerError) as res:  # Updated the expected exception to FtdServerError
        self.ftd_plugin.login('foo', 'bar')
    assert 'Failed to authenticate user' in str(res.exception)  # No changes needed as the exception type is already updated to FtdServerError"
-------------------------------------------------------------------------
"def test_handle_httperror_should_not_retry_when_ignoring_http_errors(self):
    self.ftd_plugin._ignore_http_errors = True
    assert not self.ftd_plugin.handle_httperror(HTTPError('http://testhost.com', 401, '', {}, None))
"
-------------------------------------------------------------------------
"Recom
PRs: 47747, 48982"
-------------------------------------------------------------------------
=========================================================================
"def test_handle_httperror_should_not_retry_when_ignoring_http_errors(self):
    self.ftd_plugin._ignore_http_errors = True
    assert not self.ftd_plugin.handle_httperror(HTTPError('http://testhost.com', 401, '', {}, None))
"
-------------------------------------------------------------------------
"def test_handle_httperror_should_not_retry_when_ignoring_http_errors(self):
    self.ftd_plugin._ignore_http_errors = True
    # No changes needed for the assert statement as it is already general-purpose"
-------------------------------------------------------------------------
"@patch.object(FdmSwaggerParser, 'parse_spec')
def test_get_model_spec(self, parse_spec_mock):
    self.connection_mock.send.return_value = self._connection_response(None)
    operation1 = {'modelName': 'TestModel'}
    op_model_name_is_none = {'modelName': None}
    op_without_model_name = {'url': 'testUrl'}
    parse_spec_mock.return_value = {
        SpecProp.MODEL_OPERATIONS: {
            'TestModel': {
                'testOp1': operation1,
                'testOp2': 'spec2'
            },
            'TestModel2': {
                'testOp10': 'spec10',
                'testOp20': 'spec20'
            }
        },
        SpecProp.OPERATIONS: {
            'testOp1': operation1,
            'testOp10': {
                'modelName': 'TestModel2'
            },
            'testOpWithoutModelName': op_without_model_name,
            'testOpModelNameIsNone': op_model_name_is_none
        }
    }
    assert {'testOp1': operation1, 'testOp2': 'spec2'} == self.ftd_plugin.get_operation_specs_by_model_name(
        'TestModel')
    assert None is self.ftd_plugin.get_operation_specs_by_model_name(
        'testOpModelNameIsNone')
    assert None is self.ftd_plugin.get_operation_specs_by_model_name(
        'testOpWithoutModelName')
    assert self.ftd_plugin.get_operation_specs_by_model_name('nonExistingOperation') is None
"
-------------------------------------------------------------------------
"Recom
PRs: 47747, 48982"
-------------------------------------------------------------------------
=========================================================================
"rule_match = RULE_REGEX.search(line)
rule_args = parse_module_arguments(rule_match.group('args'))
return cls(rule_match.group('rule_type'), rule_match.group('control'), rule_match.group('path'), rule_args)
"
-------------------------------------------------------------------------
"rule_match = RULE_REGEX.search(line)
rule_args = self.parse_module_arguments(rule_match.group('args'))
return PamdRule(rule_match.group('rule_type'), rule_match.group('control'), rule_match.group('path'), rule_args)"
-------------------------------------------------------------------------
"valid_simple_controls = ['required', 'requisite', 'sufficient', 'optional', 'include', 'substack', 'definitive']
"
-------------------------------------------------------------------------
"Recom
PRs: 47695, 48307"
-------------------------------------------------------------------------
=========================================================================
"if self.rule_type not in VALID_TYPES:
"
-------------------------------------------------------------------------
if self.rule_type not in PamdRule.valid_types:
-------------------------------------------------------------------------
"rule_match = RULE_REGEX.search(line)
rule_args = parse_module_arguments(rule_match.group('args'))
return cls(rule_match.group('rule_type'), rule_match.group('control'), rule_match.group('path'), rule_args)
"
-------------------------------------------------------------------------
"Recom
PRs: 47695, 48307"
-------------------------------------------------------------------------
=========================================================================
"new_args = parse_module_arguments(new_args)
changes = 0
    rule_changed = False
        if(current_rule.rule_type != new_type):
            rule_changed = True
            current_rule.rule_type = new_type
        if(current_rule.rule_control != new_control):
            rule_changed = True
            current_rule.rule_control = new_control
        if(current_rule.rule_path != new_path):
            rule_changed = True
            current_rule.rule_path = new_path
        if(current_rule.rule_args != new_args):
            rule_changed = True
            current_rule.rule_args = new_args
    if rule_changed:
        changes = 1
return changes
changes = 0
"
-------------------------------------------------------------------------
"new_args = parse_module_arguments(new_args)
changes = 0
rule_changed = False
if(current_rule.rule_type != new_type):
    rule_changed = True
    current_rule.rule_type = new_type
if(current_rule.rule_control != new_control):
    rule_changed = True
    current_rule.rule_control = new_control
if(current_rule.rule_path != new_path):
    rule_changed = True
    current_rule.rule_path = new_path
if(current_rule.rule_args != new_args):
    rule_changed = True
    current_rule.rule_args = new_args
if rule_changed:
    changes = 1
return changes"
-------------------------------------------------------------------------
"if self.rule_type not in VALID_TYPES:
"
-------------------------------------------------------------------------
"Recom
PRs: 47695, 48307"
-------------------------------------------------------------------------
=========================================================================
"changes = 1
"
-------------------------------------------------------------------------
changes = 1
-------------------------------------------------------------------------
"new_args = parse_module_arguments(new_args)
changes = 0
    rule_changed = False
        if(current_rule.rule_type != new_type):
            rule_changed = True
            current_rule.rule_type = new_type
        if(current_rule.rule_control != new_control):
            rule_changed = True
            current_rule.rule_control = new_control
        if(current_rule.rule_path != new_path):
            rule_changed = True
            current_rule.rule_path = new_path
        if(current_rule.rule_args != new_args):
            rule_changed = True
            current_rule.rule_args = new_args
    if rule_changed:
        changes = 1
return changes
changes = 0
"
-------------------------------------------------------------------------
"Recom
PRs: 47695, 48307"
-------------------------------------------------------------------------
=========================================================================
"changes = 1
hanges
= 0
"
-------------------------------------------------------------------------
"changes = 1
changed = 0"
-------------------------------------------------------------------------
"changes = 1
"
-------------------------------------------------------------------------
"Recom
PRs: 47695, 48307"
-------------------------------------------------------------------------
=========================================================================
"changes = 1
"
-------------------------------------------------------------------------
"changes = service.remove('auth', 'required', 'pam_faillock.so')"
-------------------------------------------------------------------------
"changes = 1
hanges
= 0
"
-------------------------------------------------------------------------
"Recom
PRs: 47695, 48307"
-------------------------------------------------------------------------
=========================================================================
"choices=VALID_TYPES),
    choices=VALID_TYPES),
"
-------------------------------------------------------------------------
"for current_rule in rules_to_find:
            next_rule = current_rule.next
            while next_rule is not None and isinstance(next_rule, PamdComment):
                next_rule = next_rule.next

            new_rule = PamdRule(new_type, new_control, new_path, new_args)
            if next_rule is not None and not next_rule.matches(new_type, new_control, new_path):
                next_rule.prev = new_rule
                new_rule.next = next_rule
                new_rule.prev = current_rule
                current_rule.next = new_rule
                changed += 1

            elif next_rule is None:
                new_rule.prev = self._tail
                new_rule.next = None
                self._tail.next = new_rule
                self._tail = new_rule

                current_rule.next = new_rule
                changed += 1

        return changed"
-------------------------------------------------------------------------
"changes = 1
hanges
= 0
"
-------------------------------------------------------------------------
"Recom
PRs: 47695, 48307"
-------------------------------------------------------------------------
=========================================================================
"changes = 0
"
-------------------------------------------------------------------------
changes = changed
-------------------------------------------------------------------------
"choices=VALID_TYPES),
    choices=VALID_TYPES),
"
-------------------------------------------------------------------------
"Recom
PRs: 47695, 48307"
-------------------------------------------------------------------------
=========================================================================
"if [arg for arg in parse_module_arguments(module.params['module_arguments']) if arg.startswith(""["")]:
    module.fail_json(msg=""Unable to process bracketed '[' complex arguments with 'args_present'. Please use 'updated'."")
"
-------------------------------------------------------------------------
"if [arg for arg in args_to_add if arg.startswith(""["")]:
    module.fail_json(msg=""Unable to process bracketed '[' complex arguments with 'args_present'. Please use 'updated'."")"
-------------------------------------------------------------------------
"changes = 0
"
-------------------------------------------------------------------------
"Recom
PRs: 47695, 48307"
-------------------------------------------------------------------------
=========================================================================
"result = dict(
    changed=(changes > 0),
    change_count=changes,
    backupdest='',
)
if not module.check_mode and result['changed']:
        result['backupdest'] = module.backup_local(fname)
"
-------------------------------------------------------------------------
"result = dict(
    changed=(changes > 0),
    change_count=changes,
    backupdest='',
)

if not module.check_mode and result['changed']:
    result['backupdest'] = module.backup_local(fname)"
-------------------------------------------------------------------------
"if [arg for arg in parse_module_arguments(module.params['module_arguments']) if arg.startswith(""["")]:
    module.fail_json(msg=""Unable to process bracketed '[' complex arguments with 'args_present'. Please use 'updated'."")
"
-------------------------------------------------------------------------
"Recom
PRs: 47695, 48307"
-------------------------------------------------------------------------
=========================================================================
"stop_timeout='stop_timeout',
healthcheck='healthcheck',
elf.client.docker_py_version < LooseVersion('3.0'):
# cpu_shares and volume_driver moved to create_host_config in > 3
"
-------------------------------------------------------------------------
"In the adapted code, implement the changes required based on the stable information and the target file provided. Update the code snippet as follows:


stop_timeout = 'stop_timeout'
healthcheck = 'healthcheck'

# cpu_shares and volume_driver moved to create_host_config in > 3
if elf.client.docker_py_version < LooseVersion('3.0'):
    pass  # no changes required for this version"
-------------------------------------------------------------------------
"if self.client.option_minimal_versions[value]['supported']:
    result[key] = getattr(self, value)
"
-------------------------------------------------------------------------
"Recom
PRs: 47711, 48074"
-------------------------------------------------------------------------
=========================================================================
"if self.client.option_minimal_versions[value]['supported']:
    result[key] = getattr(self, value)
"
-------------------------------------------------------------------------
"if self.option_minimal_versions.get(value, {}).get('supported'):
    result[key] = getattr(self, value)"
-------------------------------------------------------------------------
"if self.client.docker_py_version < LooseVersion('3.0'):
    # cpu_shares and volume_driver moved to create_host_config in > 3
"
-------------------------------------------------------------------------
"Recom
PRs: 47711, 48074"
-------------------------------------------------------------------------
=========================================================================
"init='init',
uts_mode='uts',
runtime='runtime',
auto_remove='auto_remove',
device_read_bps='device_read_bps',
device_write_bps='device_write_bps',
device_read_iops='device_read_iops',
device_write_iops='device_write_iops',
elf.client.docker_py_version >= LooseVersion('1.9') and self.client.docker_api_version >= LooseVersion('1.22'):
# blkio_weight can always be updated, but can only be set on creation
# when docker-py and docker API are new enough
elf.client.docker_py_version >= LooseVersion('3.0'):
    if self.client.option_minimal_versions[value]['supported']:
        params[key] = getattr(self, value)
"
-------------------------------------------------------------------------
"def log(self, msg, pretty_print=False):
    pass
    # Add the following code to the target method based on the stable information:
    if value == 'init':
        params['init'] = getattr(self, value)
    elif value == 'uts_mode':
        params['uts_mode'] = getattr(self, value)
    elif value == 'runtime':
        params['runtime'] = getattr(self, value)
    elif value == 'auto_remove':
        params['auto_remove'] = getattr(self, value)
    elif value == 'device_read_bps':
        params['device_read_bps'] = getattr(self, value)
    elif value == 'device_write_bps':
        params['device_write_bps'] = getattr(self, value)
    elif value == 'device_read_iops':
        params['device_read_iops'] = getattr(self, value)
    elif value == 'device_write_iops':
        params['device_write_iops'] = getattr(self, value)

    # ensure that docker-py version is at least 1.9 and docker API version is at least 1.22
    if self.client.docker_py_version >= LooseVersion('1.9') and self.client.docker_api_version >= LooseVersion('1.22'):
        # blkio_weight can always be updated, but can only be set on creation
        # when docker-py and docker API are new enough

        # additional check for docker-py version >= 3.0
        if self.client.docker_py_version >= LooseVersion('3.0'):
            if self.client.option_minimal_versions[value]['supported']:
                params[key] = getattr(self, value)"
-------------------------------------------------------------------------
"if self.client.docker_py_version < LooseVersion('3.0'):
    # cpu_shares and volume_driver moved to create_host_config in > 3
"
-------------------------------------------------------------------------
"Recom
PRs: 47711, 48074"
-------------------------------------------------------------------------
=========================================================================
"self.directory = ACMEDirectory(module, self)
"
-------------------------------------------------------------------------
self.directory = ACMEDirectory(module)
-------------------------------------------------------------------------
"if payload is None:
    payload64 = ''
else:
    payload64 = nopad_b64(self.module.jsonify(payload).encode('utf8'))
"
-------------------------------------------------------------------------
"Recom
PRs: 45051, 48043"
-------------------------------------------------------------------------
=========================================================================
"def send_signed_request(self, url, payload, key_data=None, jws_header=None, parse_json_result=True):
"
-------------------------------------------------------------------------
"def send_signed_request(self, url, payload, key_data=None, jws_header=None, parse_json_result=True):
        '''
        Sends a JWS signed HTTP POST request to the ACME server and returns
        the response as dictionary
        https://tools.ietf.org/html/draft-ietf-acme-acme-14#section-6.2
        '''
        key_data = key_data or self.key_data
        jws_header = jws_header or self.jws_header
        failed_tries = 0
        while True:
            protected = copy.deepcopy(jws_header)
            protected[""nonce""] = self.directory.get_nonce()
            if self.version != 1:
                protected[""url""] = url

            data = self.sign_request(protected, payload, key_data)
            if self.version == 1:
                data[""header""] = jws_header
            data = self.module.jsonify(data)

            headers = {
                'Content-Type': 'application/jose+json',
            }
            resp, info = fetch_url(self.module, url, data=data, headers=headers, method='POST')
            result = {}
            try:
                content = resp.read()
            except AttributeError:
                content = info.get('body')

            if content:
                if info['content-type'].startswith('application/json') or 400 <= info['status'] < 600:
                    try:
                        result = self.module.from_json(content.decode('utf8'))
                        # In case of badNonce error, try again (up to 5 times)
                        # (https://tools.ietf.org/html/draft-ietf-acme-acme-14#section-6.6)
                        if (400 <= info['status'] < 600 and
                                result.get('type') == 'urn:ietf:params:acme:error:badNonce' and
                                failed_tries <= 5):
                            failed_tries += 1
                            continue
                    except ValueError:
                        raise ModuleFailException(""Failed to parse the ACME response: {0} {1}"".format(url, content))
                else:
                    result = content

            if parse_json_result:
                return result, info
            else:
                return content, info"
-------------------------------------------------------------------------
"if payload is None:
    payload64 = ''
else:
    payload64 = nopad_b64(self.module.jsonify(payload).encode('utf8'))
"
-------------------------------------------------------------------------
"Recom
PRs: 45051, 48043"
-------------------------------------------------------------------------
=========================================================================
"try:
    if secure != 'never':
        try:
            smtp = smtplib.SMTP_SSL(timeout=timeout)
            code, smtpmessage = smtp.connect(host, port=port)
            secure_state = True
        except ssl.SSLError as e:
            if secure == 'always':
                                           (host, port, to_native(e)), exception=traceback.format_exc())
    if not secure_state:
        smtp = smtplib.SMTP(timeout=timeout)
except smtplib.SMTPException as e:
    module.fail_json(rc=1, msg='Unable to Connect %s:%s: %s' % (host, port, to_native(e)), exception=traceback.format_exc())
if int(code) > 0:
    if not secure_state and secure in ('starttls', 'try'):
"
-------------------------------------------------------------------------
"try:
    if secure != 'never':
        try:
            smtp = smtplib.SMTP_SSL(host=host, timeout=timeout)
            code, smtpmessage = smtp.connect(host, port=port)
            secure_state = True
        except ssl.SSLError as e:
            if secure == 'always':
                module.fail_json(rc=1, msg='Unable to start an encrypted session to %s:%s: %s' %
                                           (host, port, to_native(e)), exception=traceback.format_exc())
    if not secure_state:
        smtp = smtplib.SMTP(timeout=timeout)
        code, smtpmessage = smtp.connect(host, port=port)
except smtplib.SMTPException as e:
    module.fail_json(rc=1, msg='Unable to Connect %s:%s: %s' % (host, port, to_native(e)), exception=traceback.format_exc())
if int(code) > 0:
    if not secure_state and secure in ('starttls', 'try'):"
-------------------------------------------------------------------------
"except:
    pass
"
-------------------------------------------------------------------------
"Recom
PRs: 37098, 47019"
-------------------------------------------------------------------------
=========================================================================
"- On Linux, macOS and OpenBSD, this is converted to minutes and rounded down. If less than 60, it will be set to 0.
"
-------------------------------------------------------------------------
"# On Linux, macOS, and OpenBSD, this is converted to minutes and rounded down. If less than 60, it will be set to 0."
-------------------------------------------------------------------------
"- On Linux, macOS, and OpenBSD this is converted to minutes and rounded down. If less than 60, it will be set to 0.
- On Solaris and FreeBSD this will be seconds.
"
-------------------------------------------------------------------------
"Recom
PRs: 46147, 46289"
-------------------------------------------------------------------------
=========================================================================
"if len(split_fields) == 3 and split_fields[0:2] == ['iam', 'security-credentials']:
"
-------------------------------------------------------------------------
if len(split_fields) >= 3 and split_fields[0] == 'iam' and split_fields[1] == 'security-credentials':
-------------------------------------------------------------------------
"if len(split_fields) == 2 and split_fields[0:2] == ['iam', 'info_instanceprofilearn']:
    new_fields[self._prefix % ""iam-instance-profile-role""] = value.split('/')[1]
"
-------------------------------------------------------------------------
"Recom
PRs: 38664, 46077"
-------------------------------------------------------------------------
=========================================================================
"if len(split_fields) == 3 and split_fields[0:2] == ['iam', 'security-credentials']:
"
-------------------------------------------------------------------------
"if len(split_fields) == 3 and split_fields[0:2] == ['iam', 'security-credentials']:"
-------------------------------------------------------------------------
"if len(split_fields) == 2 and split_fields[0:2] == ['iam', 'info_instanceprofilearn']:
    new_fields[self._prefix % ""iam-instance-profile-role""] = value.split('/')[1]
"
-------------------------------------------------------------------------
"Recom
PRs: 38664, 46076"
-------------------------------------------------------------------------
=========================================================================
"if vpc_id:
    vpc_wins = dict((group['GroupName'], group) for group in all_groups if group['VpcId'] == vpc_id)
    groups.update(vpc_wins)
"
-------------------------------------------------------------------------
"if vpc_id:
    vpc_wins = dict((group['GroupName'], group) for group in all_groups if group.get('VpcId') == vpc_id)
    groups.update(vpc_wins)"
-------------------------------------------------------------------------
"if vpc_id:
    vpc_wins = dict((group['GroupName'], group) for group in all_groups if group.get('VpcId') and group['VpcId'] == vpc_id)
    groups.update(vpc_wins)
"
-------------------------------------------------------------------------
"Recom
PRs: 45787, 45815"
-------------------------------------------------------------------------
=========================================================================
"stdout = ''
stderr = ''
"
-------------------------------------------------------------------------
"stdout = ''
        stderr = ''"
-------------------------------------------------------------------------
"stdout = u''
stderr = u''
"
-------------------------------------------------------------------------
"Recom
PRs: 45607, 45791"
-------------------------------------------------------------------------
=========================================================================
"if checksum != destination_checksum:
    checksum_mismatch = True
t forcing redownload, unless checksum does not match
ot force and not checksum_mismatch:
# Not forcing redownload, unless checksum does not match
# allow file attribute changes
module.params['path'] = dest
file_args = module.load_file_common_arguments(module.params)
file_args['path'] = dest
result['changed'] = module.set_fs_attributes_if_different(file_args, False)
if result['changed']:
    module.exit_json(msg=""file already exists but file attributes changed"", **result)
module.exit_json(msg=""file already exists"", **result)
"
-------------------------------------------------------------------------
"if force or checksum == destination_checksum:
    # Not forcing redownload, unless checksum does not match
    # allow file attribute changes
    module.params['path'] = dest
    file_args = module.load_file_common_arguments(module.params)
    file_args['path'] = dest
    changed = module.set_fs_attributes_if_different(file_args, False)
    if changed:
        module.exit_json(msg=""file already exists but file attributes changed"", dest=dest, url=url, changed=changed, msg=info.get('msg', ''), status_code=info.get('status', ''))
    module.exit_json(msg=""file already exists"", dest=dest, url=url, changed=changed, msg=info.get('msg', ''), status_code=info.get('status', ''))
else:
    checksum_mismatch = True"
-------------------------------------------------------------------------
"if checksum != destination_checksum:
    checksum_mismatch = True
t forcing redownload, unless checksum does not match
ot force and not checksum_mismatch:
# Not forcing redownload, unless checksum does not match
# allow file attribute changes
module.params['path'] = dest
file_args = module.load_file_common_arguments(module.params)
file_args['path'] = dest
changed = module.set_fs_attributes_if_different(file_args, False)
if changed:
    module.exit_json(msg=""file already exists but file attributes changed"", dest=dest, url=url, changed=changed)
module.exit_json(msg=""file already exists"", dest=dest, url=url, changed=changed)
"
-------------------------------------------------------------------------
"Recom
PRs: 45495, 45565"
-------------------------------------------------------------------------
=========================================================================
"args = module.params['arguments']
cmd = ""%s %s %s"" % (script, doit, """" if args is None else args)
"
-------------------------------------------------------------------------
"Since scripting is more common in Bash and you're interested in using Bash, you can pick up the shell string from the standard output produced by shell’s echo command. Here, you would execute the command shell from python and get the standard output as a list. It needs to be done before running anything else, and before actually calling su or another user, or using any privileges.

### Modified Code

    import os
    import signal
    import platform
    import subprocess

    LINUX = 'Linux'
    WINDOWS = 'Windows'

    class RunCommand:
        def __init__(self):
            self.os_type = platform.system()

        def run_command(self, command):
            if self.os_type == 'Windows':
                completed_process = subprocess.run(command, capture_output=True, text=True)
                return completed_process.stdout
            else:
                p = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                return p.stdout.read().decode('utf-8')
  
        def try_command(command):
            try:
                r, w = os.pipe()
                return True
            except Exception as e:
                return False

        p = subprocess.run(['ls'], shell=True, text=True, stdout=subprocess.PIPE)


Usage:

    import os
    import time
    import torch

    def main(arg):
        output = """"
        if len(sys.argv) != 2:
            print('Filename not provided')
            return
        try:
            pass
        finally:
            pass
    pass, chdir, time, count, os, os.path.abspath, os.fspath, prettysize, platform, subprocess, noc, getopt, client, database"
-------------------------------------------------------------------------
"if enabled is not None and enabled != runlevel_status[""enabled""]:
"
-------------------------------------------------------------------------
"Recom
PRs: 42786, 43831"
-------------------------------------------------------------------------
=========================================================================
"err = '\n'
"
-------------------------------------------------------------------------
"In the adapted code based on the stable information, the statement `err = '\n'` should be replaced by:


err = '\n'"
-------------------------------------------------------------------------
"err = to_native(stderr or """")
if err and not err.endswith('\n'):
    err = '\n'
"
-------------------------------------------------------------------------
"Recom
PRs: 39019, 39430"
-------------------------------------------------------------------------
=========================================================================
"- ""To use this module, it has to be executed twice. Either as two
   different tasks in the same run or during two runs. Note that the output
   of the first run needs to be recorded and passed to the second run as the
   module argument C(data).""
   U(https://tools.ietf.org/html/draft-ietf-acme-acme-09#section-8).
   Also, consider the examples provided for this module.""
"
-------------------------------------------------------------------------
"if client.is_first_step():
            # First run: start challenges / start new order
            client.start_challenges()
        else:
            # Second run: finish challenges, and get certificate
            client.finish_challenges()
            client.get_certificate()
        data, data_dns = client.get_challenges_data()"
-------------------------------------------------------------------------
"- ""To use this module, it has to be executed twice. Either as two
   different tasks in the same run or during two runs. Note that the output
   of the first run needs to be recorded and passed to the second run as the
   module argument C(data).""
"
-------------------------------------------------------------------------
"Recom
PRs: 38135, 38160"
-------------------------------------------------------------------------
=========================================================================
"# Copyright (c) 2018 Matt Martz <matt@sivel.net>
# GNU General Public License v3.0 (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
# -*- coding: utf-8 -*-
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type
import pytest
from ansible.executor.module_common import modify_module
from ansible.module_utils.six import PY2
from test_module_common import templar
FAKE_OLD_MODULE = b'''#!/usr/bin/python
import sys
print('{""result"": ""%s""}' % sys.executable)
'''
@pytest.fixture
def fake_old_module_open(mocker):
    m = mocker.mock_open(read_data=FAKE_OLD_MODULE)
    if PY2:
        mocker.patch('__builtin__.open', m)
    else:
        mocker.patch('builtins.open', m)
def test_shebang(fake_old_module_open):
    (data, style, shebang) = modify_module('fake_module', 'fake_path', {})
    assert shebang == '#!/usr/bin/python'
def test_shebang_task_vars(fake_old_module_open, templar):
    task_vars = {
        'ansible_python_interpreter': '/usr/bin/python3'
    }
    (data, style, shebang) = modify_module('fake_module', 'fake_path', {}, task_vars=task_vars, templar=templar)
    assert shebang == '#!/usr/bin/python3'
"
-------------------------------------------------------------------------
"from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

import pytest

from ansible.executor.module_common import modify_module
from ansible.module_utils.six import PY2

FAKE_OLD_MODULE = b'''#!/usr/bin/python
import sys
print('{""result"": ""%s""}' % sys.executable)
'''


@pytest.fixture
def fake_old_module_open(mocker):
    m = mocker.mock_open(read_data=FAKE_OLD_MODULE)
    if PY2:
        mocker.patch('__builtin__.open', m)
    else:
        mocker.patch('builtins.open', m)


def test_shebang(fake_old_module_open):
    (data, style, shebang) = modify_module('fake_module', 'fake_path', {})
    assert shebang == '#!/usr/bin/python'


def test_shebang_task_vars(fake_old_module_open):
    task_vars = {
        'ansible_python_interpreter': '/usr/bin/python3'
    }

    (data, style, shebang) = modify_module('fake_module', 'fake_path', {}, task_vars=task_vars)
    assert shebang == '#!/usr/bin/python3'"
-------------------------------------------------------------------------
"# Copyright (c) 2018 Matt Martz <matt@sivel.net>
# GNU General Public License v3.0 (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
# -*- coding: utf-8 -*-
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type
import pytest
from ansible.executor.module_common import modify_module
from ansible.module_utils.six import PY2
FAKE_OLD_MODULE = b'''#!/usr/bin/python
import sys
print('{""result"": ""%s""}' % sys.executable)
'''
@pytest.fixture
def fake_old_module_open(mocker):
    m = mocker.mock_open(read_data=FAKE_OLD_MODULE)
    if PY2:
        mocker.patch('__builtin__.open', m)
    else:
        mocker.patch('builtins.open', m)
def test_shebang(fake_old_module_open):
    (data, style, shebang) = modify_module('fake_module', 'fake_path', {})
    assert shebang == '#!/usr/bin/python'
def test_shebang_task_vars(fake_old_module_open):
    task_vars = {
        'ansible_python_interpreter': '/usr/bin/python3'
    }
    (data, style, shebang) = modify_module('fake_module', 'fake_path', {}, task_vars=task_vars)
    assert shebang == '#!/usr/bin/python3'
"
-------------------------------------------------------------------------
"Recom
PRs: 36602, 36607"
-------------------------------------------------------------------------
=========================================================================
"new_ir = self._copy_included_file(included_file)
new_blocks, handler_blocks = new_ir.get_block_list(
"
-------------------------------------------------------------------------
"new_ir, handler_blocks = self._load_included_file(included_file, iterator, is_handler=False), []
new_blocks = new_ir"
-------------------------------------------------------------------------
"new_ir = self._copy_included_file(included_file)
"
-------------------------------------------------------------------------
"Recom
PRs: 36470, 36526"
-------------------------------------------------------------------------
=========================================================================
"if moid in ['group-d1', 'ha-folder-root']:
"
-------------------------------------------------------------------------
"def compile_folder_path_for_object(vobj):
    """""" make a /vm/foo/bar/baz like folder path for an object """"""

    paths = []
    if isinstance(vobj, vim.Folder):
        paths.append(vobj.name)

    thisobj = vobj
    while hasattr(thisobj, 'parent'):
        thisobj = thisobj.parent
        if thisobj.name in ['group-d1', 'ha-folder-root']:
            break
        if isinstance(thisobj, vim.Folder):
            paths.append(thisobj.name)
    paths.reverse()
    return '/' + '/'.join(paths)"
-------------------------------------------------------------------------
"try:
    moid = thisobj._moId
except AttributeError:
    moid = None
if moid in ['group-d1', 'ha-folder-root']:
"
-------------------------------------------------------------------------
"Recom
PRs: 31133, 32671"
-------------------------------------------------------------------------
=========================================================================
